[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Jerwell Escusa",
    "section": "",
    "text": "I graduated with a Bachelor of Science in Economics from the University of the Philippines - Diliman. In my studies, I find my strengths lie in econometrics, data analysis, and empirical research. At present, I am looking for jobs related to data analysis and research. I have experience using Stata doing multiple linear regression, panel regression (first difference and fixed effects), and logistic regression in my econometrics classes and Cox regression for my undergraduate thesis. I also used R to produce and evaluate forecasts from naive to ARIMA forecasting. I am currently expanding my skill set learning the different softwares and programming languages used in data analysis.",
    "crumbs": [
      "About Me"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "index.html#first-steps",
    "href": "index.html#first-steps",
    "title": "1 Data Visualization",
    "section": "1.2 First steps",
    "text": "1.2 First steps\n\npenguins\n\n# A tibble: 344 × 8\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.5          17.4               186        3800\n 3 Adelie  Torgersen           40.3          18                 195        3250\n 4 Adelie  Torgersen           NA            NA                  NA          NA\n 5 Adelie  Torgersen           36.7          19.3               193        3450\n 6 Adelie  Torgersen           39.3          20.6               190        3650\n 7 Adelie  Torgersen           38.9          17.8               181        3625\n 8 Adelie  Torgersen           39.2          19.6               195        4675\n 9 Adelie  Torgersen           34.1          18.1               193        3475\n10 Adelie  Torgersen           42            20.2               190        4250\n# ℹ 334 more rows\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\n\nglimpse(penguins)\n\nRows: 344\nColumns: 8\n$ species           &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel…\n$ island            &lt;fct&gt; Torgersen, Torgersen, Torgersen, Torgersen, Torgerse…\n$ bill_length_mm    &lt;dbl&gt; 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, …\n$ bill_depth_mm     &lt;dbl&gt; 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1, …\n$ flipper_length_mm &lt;int&gt; 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 186…\n$ body_mass_g       &lt;int&gt; 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, …\n$ sex               &lt;fct&gt; male, female, female, NA, female, male, female, male…\n$ year              &lt;int&gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007…\n\n\n\npenguins %&gt;% ggplot(aes(x = flipper_length_mm, y = body_mass_g)) +\n  geom_point(aes(color = species, shape = species)) +\n  geom_smooth(method = \"lm\") +\n  labs(\n    title = \"Body mass and flipper length\",\n    subtitle = \"Dimensions for Adelie, Chinstrap, and Gentoo Penguins\",\n    x = \"Flipper length (mm)\", y = \"Body mass (g)\",\n    color = \"Species\", shape = \"Species\"\n  ) +\n  scale_color_colorblind()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\n\n1.2.5 Exercises\n\nHow many rows are in penguins? How many columns?\nThere are 344 rows and 8 columns.\nWhat does the bill_depth_mm variable in the penguins data frame describe? Read the help for ?penguins to find out.\nThe bill_depth_mm variable describes the bill depth of penguins in millimeters.\nMake a scatterplot of bill_depth_mm vs. bill_length_mm. That is, make a scatterplot with bill_depth_mm on the y-axis and bill_length_mm on the x-axis. Describe the relationship between these two variables.\n\npenguins %&gt;% ggplot(aes(y = bill_depth_mm, x = bill_length_mm)) +\n  geom_point() +\n  geom_smooth(method = \"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nThe bill depth and the bill length of a penguin are inversely related with each other. The strength of this relationship, however, appears to be weak given the nearly flat slope of the regression line.\nWhat happens if you make a scatterplot of species vs. bill_depth_mm? What might be a better choice of geom?\n\npenguins %&gt;% ggplot(aes(x = species, y = bill_depth_mm)) +\n  geom_point()\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nThe species variable is a categorical variable, so it does not make sense to create a scatterplot with it. It should be a continuous variable.\n\npenguins %&gt;% ggplot(aes(x = species, y = bill_depth_mm, color = species)) +\n  geom_boxplot() +\n  scale_color_colorblind()\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n\n\n\n\n\nA boxplot is a more helpful visualization to see the characteristics of the bill depth values for each penguin species.\nWhy does the following give an error and how would you fix it?\n\nggplot(data = penguins) + \n  geom_point()\n\nError in `geom_point()`:\n! Problem while setting up geom.\nℹ Error occurred in the 1st layer.\nCaused by error in `compute_geom_1()`:\n! `geom_point()` requires the following missing aesthetics: x and y.\n\n\nAs shown in the error message, it requires an aesthetic mapping for x and y.\nWhat does the na.rm argument do in geom_point()? What is the default value of the argument? Create a scatterplot where you successfully use this argument set to TRUE.\nWhen you run ?geom_point(), the documentation indicates that this argument controls how missing values are handle. If TRUE, missing values are removed without warning, whereas if FALSE, the missing values are removed but you will receive a warning. The documentation also show that the default value is FALSE.\n\npenguins %&gt;% ggplot(aes(y = bill_depth_mm, x = bill_length_mm)) +\n  geom_point(na.rm = TRUE)\n\n\n\n\n\n\n\n\nAdd the following caption to the plot you made in the previous exercise: “Data come from the palmerpenguins package.” Hint: Take a look at the documentation for labs().\n\npenguins %&gt;% ggplot(aes(y = bill_depth_mm, x = bill_length_mm)) +\n  geom_point(na.rm = TRUE) + \n  labs(caption = \"Data come from the palmerpenguins package.\")\n\n\n\n\n\n\n\n\nRecreate the following visualization. What aesthetic should bill_depth_mm be mapped to? And should it be mapped at the global level or at the geom level?\n\npenguins %&gt;% ggplot(aes(y = body_mass_g, x = flipper_length_mm, color = bill_depth_mm)) +\n  geom_point() +\n  geom_smooth()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: The following aesthetics were dropped during statistical transformation:\ncolour.\nℹ This can happen when ggplot fails to infer the correct grouping structure in\n  the data.\nℹ Did you forget to specify a `group` aesthetic or to convert a numerical\n  variable into a factor?\n\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\n\npenguins %&gt;% ggplot(aes(y = body_mass_g, x = flipper_length_mm)) +\n  geom_point(aes(color = bill_depth_mm)) +\n  geom_smooth()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nThe bill_depth_mm variable clearly should be mapped to the color aesthetic. Either global or local declaration of color aesthetic mapping works.\nRun this code in your head and predict what the output will look like. Then, run the code in R and check your predictions.\n\nggplot(\n  data = penguins,\n  mapping = aes(x = flipper_length_mm, y = body_mass_g, color = island)\n) +\n  geom_point() +\n  geom_smooth(se = FALSE)\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nThis would create a scatterplot with a LOESS line (without a confidence interval) showing the relationship between flipper_length_mm and body_mass_g. The data points have colors based on the island variable.\nI missed the issue in the global and local declaration of aesthetic mapping argument for the color.\nWill these two graphs look different? Why/why not?\n\nggplot(\n  data = penguins,\n  mapping = aes(x = flipper_length_mm, y = body_mass_g)\n) +\n  geom_point() +\n  geom_smooth()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\n\nggplot() +\n  geom_point(\n    data = penguins,\n    mapping = aes(x = flipper_length_mm, y = body_mass_g)\n  ) +\n  geom_smooth(\n    data = penguins,\n    mapping = aes(x = flipper_length_mm, y = body_mass_g)\n  )\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nThere should not be a difference, but the latter is a more verbose implementation of the visualization.",
    "crumbs": [
      "Whole Game",
      "1 Data Visualization"
    ]
  },
  {
    "objectID": "index.html#ggplot-2-calls",
    "href": "index.html#ggplot-2-calls",
    "title": "1 Data Visualization",
    "section": "1.3 ggplot 2 calls",
    "text": "1.3 ggplot 2 calls",
    "crumbs": [
      "Whole Game",
      "1 Data Visualization"
    ]
  },
  {
    "objectID": "chapter_1.html",
    "href": "chapter_1.html",
    "title": "1 Data Visualization",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\nlibrary(palmerpenguins)\nlibrary(ggthemes)",
    "crumbs": [
      "R for Data Science: Outputs",
      "Whole Game",
      "1 Data Visualization"
    ]
  },
  {
    "objectID": "chapter_1.html#first-steps",
    "href": "chapter_1.html#first-steps",
    "title": "1 Data Visualization",
    "section": "1.2 First steps",
    "text": "1.2 First steps\n\npenguins\n\n# A tibble: 344 × 8\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.5          17.4               186        3800\n 3 Adelie  Torgersen           40.3          18                 195        3250\n 4 Adelie  Torgersen           NA            NA                  NA          NA\n 5 Adelie  Torgersen           36.7          19.3               193        3450\n 6 Adelie  Torgersen           39.3          20.6               190        3650\n 7 Adelie  Torgersen           38.9          17.8               181        3625\n 8 Adelie  Torgersen           39.2          19.6               195        4675\n 9 Adelie  Torgersen           34.1          18.1               193        3475\n10 Adelie  Torgersen           42            20.2               190        4250\n# ℹ 334 more rows\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\n\nglimpse(penguins)\n\nRows: 344\nColumns: 8\n$ species           &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel…\n$ island            &lt;fct&gt; Torgersen, Torgersen, Torgersen, Torgersen, Torgerse…\n$ bill_length_mm    &lt;dbl&gt; 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, …\n$ bill_depth_mm     &lt;dbl&gt; 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1, …\n$ flipper_length_mm &lt;int&gt; 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 186…\n$ body_mass_g       &lt;int&gt; 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, …\n$ sex               &lt;fct&gt; male, female, female, NA, female, male, female, male…\n$ year              &lt;int&gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007…\n\n\n\npenguins %&gt;% ggplot(aes(x = flipper_length_mm, y = body_mass_g)) +\n  geom_point(aes(color = species, shape = species)) +\n  geom_smooth(method = \"lm\") +\n  labs(\n    title = \"Body mass and flipper length\",\n    subtitle = \"Dimensions for Adelie, Chinstrap, and Gentoo Penguins\",\n    x = \"Flipper length (mm)\", y = \"Body mass (g)\",\n    color = \"Species\", shape = \"Species\"\n  ) +\n  scale_color_colorblind()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\n\n1.2.5 Exercises\n\nHow many rows are in penguins? How many columns?\nThere are 344 rows and 8 columns.\nWhat does the bill_depth_mm variable in the penguins data frame describe? Read the help for ?penguins to find out.\nThe bill_depth_mm variable describes the bill depth of penguins in millimeters.\nMake a scatterplot of bill_depth_mm vs. bill_length_mm. That is, make a scatterplot with bill_depth_mm on the y-axis and bill_length_mm on the x-axis. Describe the relationship between these two variables.\n\npenguins %&gt;% ggplot(aes(y = bill_depth_mm, x = bill_length_mm)) +\n  geom_point() +\n  geom_smooth(method = \"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nThe bill depth and the bill length of a penguin are inversely related with each other. The strength of this relationship, however, appears to be weak given the nearly flat slope of the regression line.\nWhat happens if you make a scatterplot of species vs. bill_depth_mm? What might be a better choice of geom?\n\npenguins %&gt;% ggplot(aes(x = species, y = bill_depth_mm)) +\n  geom_point()\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nThe species variable is a categorical variable, so it does not make sense to create a scatterplot with it. It should be a continuous variable.\n\npenguins %&gt;% ggplot(aes(x = species, y = bill_depth_mm, color = species)) +\n  geom_boxplot() +\n  scale_color_colorblind()\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n\n\n\n\n\nA boxplot is a more helpful visualization to see the characteristics of the bill depth values for each penguin species.\nWhy does the following give an error and how would you fix it?\n\nggplot(data = penguins) + \n  geom_point()\n\nError in `geom_point()`:\n! Problem while setting up geom.\nℹ Error occurred in the 1st layer.\nCaused by error in `compute_geom_1()`:\n! `geom_point()` requires the following missing aesthetics: x and y.\n\n\nAs shown in the error message, it requires an aesthetic mapping for x and y.\nWhat does the na.rm argument do in geom_point()? What is the default value of the argument? Create a scatterplot where you successfully use this argument set to TRUE.\nWhen you run ?geom_point(), the documentation indicates that this argument controls how missing values are handle. If TRUE, missing values are removed without warning, whereas if FALSE, the missing values are removed but you will receive a warning. The documentation also show that the default value is FALSE.\n\npenguins %&gt;% ggplot(aes(y = bill_depth_mm, x = bill_length_mm)) +\n  geom_point(na.rm = TRUE)\n\n\n\n\n\n\n\n\nAdd the following caption to the plot you made in the previous exercise: “Data come from the palmerpenguins package.” Hint: Take a look at the documentation for labs().\n\npenguins %&gt;% ggplot(aes(y = bill_depth_mm, x = bill_length_mm)) +\n  geom_point(na.rm = TRUE) + \n  labs(caption = \"Data come from the palmerpenguins package.\")\n\n\n\n\n\n\n\n\nRecreate the following visualization. What aesthetic should bill_depth_mm be mapped to? And should it be mapped at the global level or at the geom level?\n\npenguins %&gt;% ggplot(aes(y = body_mass_g, x = flipper_length_mm, color = bill_depth_mm)) +\n  geom_point() +\n  geom_smooth()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: The following aesthetics were dropped during statistical transformation:\ncolour.\nℹ This can happen when ggplot fails to infer the correct grouping structure in\n  the data.\nℹ Did you forget to specify a `group` aesthetic or to convert a numerical\n  variable into a factor?\n\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\n\npenguins %&gt;% ggplot(aes(y = body_mass_g, x = flipper_length_mm)) +\n  geom_point(aes(color = bill_depth_mm)) +\n  geom_smooth()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nThe bill_depth_mm variable clearly should be mapped to the color aesthetic. Either global or local declaration of color aesthetic mapping works.\nRun this code in your head and predict what the output will look like. Then, run the code in R and check your predictions.\n\nggplot(\n  data = penguins,\n  mapping = aes(x = flipper_length_mm, y = body_mass_g, color = island)\n) +\n  geom_point() +\n  geom_smooth(se = FALSE)\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nThis would create a scatterplot with a LOESS line (without a confidence interval) showing the relationship between flipper_length_mm and body_mass_g. The data points have colors based on the island variable.\nI missed the issue in the global and local declaration of aesthetic mapping argument for the color.\nWill these two graphs look different? Why/why not?\n\nggplot(\n  data = penguins,\n  mapping = aes(x = flipper_length_mm, y = body_mass_g)\n) +\n  geom_point() +\n  geom_smooth()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\n\nggplot() +\n  geom_point(\n    data = penguins,\n    mapping = aes(x = flipper_length_mm, y = body_mass_g)\n  ) +\n  geom_smooth(\n    data = penguins,\n    mapping = aes(x = flipper_length_mm, y = body_mass_g)\n  )\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nThere should not be a difference, but the latter is a more verbose implementation of the visualization.",
    "crumbs": [
      "R for Data Science: Outputs",
      "Whole Game",
      "1 Data Visualization"
    ]
  },
  {
    "objectID": "chapter_1.html#ggplot-2-calls",
    "href": "chapter_1.html#ggplot-2-calls",
    "title": "1 Data Visualization",
    "section": "1.3 ggplot 2 calls",
    "text": "1.3 ggplot 2 calls",
    "crumbs": [
      "R for Data Science: Outputs",
      "Whole Game",
      "1 Data Visualization"
    ]
  },
  {
    "objectID": "r4ds_outputs/chapter_1.html",
    "href": "r4ds_outputs/chapter_1.html",
    "title": "1 Data Visualization",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\nlibrary(palmerpenguins)\nlibrary(ggthemes)",
    "crumbs": [
      "R for Data Science: Outputs",
      "Whole Game",
      "1 Data Visualization"
    ]
  },
  {
    "objectID": "r4ds_outputs/chapter_1.html#first-steps",
    "href": "r4ds_outputs/chapter_1.html#first-steps",
    "title": "1 Data Visualization",
    "section": "1.2 First steps",
    "text": "1.2 First steps\n\npenguins\n\n# A tibble: 344 × 8\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.5          17.4               186        3800\n 3 Adelie  Torgersen           40.3          18                 195        3250\n 4 Adelie  Torgersen           NA            NA                  NA          NA\n 5 Adelie  Torgersen           36.7          19.3               193        3450\n 6 Adelie  Torgersen           39.3          20.6               190        3650\n 7 Adelie  Torgersen           38.9          17.8               181        3625\n 8 Adelie  Torgersen           39.2          19.6               195        4675\n 9 Adelie  Torgersen           34.1          18.1               193        3475\n10 Adelie  Torgersen           42            20.2               190        4250\n# ℹ 334 more rows\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\n\nglimpse(penguins)\n\nRows: 344\nColumns: 8\n$ species           &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel…\n$ island            &lt;fct&gt; Torgersen, Torgersen, Torgersen, Torgersen, Torgerse…\n$ bill_length_mm    &lt;dbl&gt; 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, …\n$ bill_depth_mm     &lt;dbl&gt; 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1, …\n$ flipper_length_mm &lt;int&gt; 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 186…\n$ body_mass_g       &lt;int&gt; 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, …\n$ sex               &lt;fct&gt; male, female, female, NA, female, male, female, male…\n$ year              &lt;int&gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007…\n\n\n\npenguins %&gt;% ggplot(aes(x = flipper_length_mm, y = body_mass_g)) +\n  geom_point(aes(color = species, shape = species)) +\n  geom_smooth(method = \"lm\") +\n  labs(\n    title = \"Body mass and flipper length\",\n    subtitle = \"Dimensions for Adelie, Chinstrap, and Gentoo Penguins\",\n    x = \"Flipper length (mm)\", y = \"Body mass (g)\",\n    color = \"Species\", shape = \"Species\"\n  ) +\n  scale_color_colorblind()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\n\n1.2.5 Exercises\n\nHow many rows are in penguins? How many columns?\nThere are 344 rows and 8 columns.\nWhat does the bill_depth_mm variable in the penguins data frame describe? Read the help for ?penguins to find out.\nThe bill_depth_mm variable describes the bill depth of penguins in millimeters.\nMake a scatterplot of bill_depth_mm vs. bill_length_mm. That is, make a scatterplot with bill_depth_mm on the y-axis and bill_length_mm on the x-axis. Describe the relationship between these two variables.\n\npenguins %&gt;% ggplot(aes(y = bill_depth_mm, x = bill_length_mm)) +\n  geom_point() +\n  geom_smooth(method = \"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nThe bill depth and the bill length of a penguin are inversely related with each other. The strength of this relationship, however, appears to be weak given the nearly flat slope of the regression line.\nWhat happens if you make a scatterplot of species vs. bill_depth_mm? What might be a better choice of geom?\n\npenguins %&gt;% ggplot(aes(x = species, y = bill_depth_mm)) +\n  geom_point()\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nThe species variable is a categorical variable, so it does not make sense to create a scatterplot with it. It should be a continuous variable.\n\npenguins %&gt;% ggplot(aes(x = species, y = bill_depth_mm, color = species)) +\n  geom_boxplot() +\n  scale_color_colorblind()\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n\n\n\n\n\nA boxplot is a more helpful visualization to see the characteristics of the bill depth values for each penguin species.\nWhy does the following give an error and how would you fix it?\n\nggplot(data = penguins) + \n  geom_point()\n\nError in `geom_point()`:\n! Problem while setting up geom.\nℹ Error occurred in the 1st layer.\nCaused by error in `compute_geom_1()`:\n! `geom_point()` requires the following missing aesthetics: x and y.\n\n\nAs shown in the error message, it requires an aesthetic mapping for x and y.\nWhat does the na.rm argument do in geom_point()? What is the default value of the argument? Create a scatterplot where you successfully use this argument set to TRUE.\nWhen you run ?geom_point(), the documentation indicates that this argument controls how missing values are handle. If TRUE, missing values are removed without warning, whereas if FALSE, the missing values are removed but you will receive a warning. The documentation also show that the default value is FALSE.\n\npenguins %&gt;% ggplot(aes(y = bill_depth_mm, x = bill_length_mm)) +\n  geom_point(na.rm = TRUE)\n\n\n\n\n\n\n\n\nAdd the following caption to the plot you made in the previous exercise: “Data come from the palmerpenguins package.” Hint: Take a look at the documentation for labs().\n\npenguins %&gt;% ggplot(aes(y = bill_depth_mm, x = bill_length_mm)) +\n  geom_point(na.rm = TRUE) + \n  labs(caption = \"Data come from the palmerpenguins package.\")\n\n\n\n\n\n\n\n\nRecreate the following visualization. What aesthetic should bill_depth_mm be mapped to? And should it be mapped at the global level or at the geom level?\n\nGlobalLocal\n\n\n\npenguins %&gt;% ggplot(aes(y = body_mass_g, x = flipper_length_mm, color = bill_depth_mm)) +\n  geom_point() +\n  geom_smooth()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: The following aesthetics were dropped during statistical transformation:\ncolour.\nℹ This can happen when ggplot fails to infer the correct grouping structure in\n  the data.\nℹ Did you forget to specify a `group` aesthetic or to convert a numerical\n  variable into a factor?\n\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\n\n\n\npenguins %&gt;% ggplot(aes(y = body_mass_g, x = flipper_length_mm)) +\n  geom_point(aes(color = bill_depth_mm)) +\n  geom_smooth()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\n\n\n\nThe bill_depth_mm variable clearly should be mapped to the color aesthetic. Either global or local declaration of color aesthetic mapping works.1\nRun this code in your head and predict what the output will look like. Then, run the code in R and check your predictions.\n\nggplot(\n  data = penguins,\n  mapping = aes(x = flipper_length_mm, y = body_mass_g, color = island)\n) +\n  geom_point() +\n  geom_smooth(se = FALSE)\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nThis would create a scatterplot with a LOESS line (without a confidence interval) showing the relationship between flipper_length_mm and body_mass_g. The data points have colors based on the island variable.\nI missed the issue in the global and local declaration of aesthetic mapping argument for the color.\nWill these two graphs look different? Why/why not?\n\nggplot(\n  data = penguins,\n  mapping = aes(x = flipper_length_mm, y = body_mass_g)\n) +\n  geom_point() +\n  geom_smooth()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\n\nggplot() +\n  geom_point(\n    data = penguins,\n    mapping = aes(x = flipper_length_mm, y = body_mass_g)\n  ) +\n  geom_smooth(\n    data = penguins,\n    mapping = aes(x = flipper_length_mm, y = body_mass_g)\n  )\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nThere should not be a difference, but the latter is a more verbose implementation of the visualization.",
    "crumbs": [
      "R for Data Science: Outputs",
      "Whole Game",
      "1 Data Visualization"
    ]
  },
  {
    "objectID": "r4ds_outputs/chapter_1.html#ggplot-2-calls",
    "href": "r4ds_outputs/chapter_1.html#ggplot-2-calls",
    "title": "1 Data Visualization",
    "section": "1.3 ggplot 2 calls",
    "text": "1.3 ggplot 2 calls",
    "crumbs": [
      "R for Data Science: Outputs",
      "Whole Game",
      "1 Data Visualization"
    ]
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Jerwell Escusa",
    "section": "Education",
    "text": "Education\nUniversity of the Philippines - Diliman\nBachelor of Science in Economics | 2018 - 2024",
    "crumbs": [
      "About Me"
    ]
  },
  {
    "objectID": "r4ds_outputs/chapter_1.html#footnotes",
    "href": "r4ds_outputs/chapter_1.html#footnotes",
    "title": "1 Data Visualization",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAccording to the suggested solution, the color mapping should only be mapped at the local level, only for geom_point(), but it seems this error is handled automatically.↩︎",
    "crumbs": [
      "R for Data Science: Outputs",
      "Whole Game",
      "1 Data Visualization"
    ]
  },
  {
    "objectID": "r4ds_outputs/chapter_1.html#visualizing-distributions",
    "href": "r4ds_outputs/chapter_1.html#visualizing-distributions",
    "title": "1 Data Visualization",
    "section": "1.4 Visualizing distributions",
    "text": "1.4 Visualizing distributions\n\nggplot(penguins, aes(x = species)) +\n  geom_bar()\n\n\n\n\n\n\n\n\n\nggplot(penguins, aes(x = fct_infreq(species))) +\n  geom_bar()\n\n\n\n\n\n\n\n\n\nggplot(penguins, aes(x = body_mass_g)) +\n  geom_histogram(binwidth = 200)\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n\n\n\n\n\n\n\n\nggplot(penguins, aes(x = body_mass_g)) +\n  geom_histogram(binwidth = 20)\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n\n\n\n\n\n\nggplot(penguins, aes(x = body_mass_g)) +\n  geom_histogram(binwidth = 2000)\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n\n\n\n\n\n\n\n\nggplot(penguins, aes(x = body_mass_g)) +\n  geom_density()\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_density()`).\n\n\n\n\n\n\n\n\n\n\n1.4.3 Exercises\n\nMake a bar plot of species of penguins, where you assign species to the y aesthetic. How is this plot different?\n\npenguins %&gt;% ggplot(aes(y = species)) + \n  geom_bar()\n\n\n\n\n\n\n\n\nWhen assigned to x, it is a vertical bar graph, and when assigned to y, it is a horizontal bar graph.\nHow are the following two plots different? Which aesthetic, color or fill, is more useful for changing the color of bars?\n\nColorFill\n\n\n\nggplot(penguins, aes(x = species)) +\n  geom_bar(color = \"red\")\n\n\n\n\n\n\n\n\n\n\n\nggplot(penguins, aes(x = species)) +\n  geom_bar(fill = \"red\")\n\n\n\n\n\n\n\n\n\n\n\nIn this context, the color aesthetic only applies a color to the outline of each bar, and on the other hand, the fill aesthetic applies color to the whole bar. The fill aesthetic is more useful in this case.\nWhat does the bins argument in geom_histogram() do?\nIt specifies the number of bins used to represent the distribution. The default is 30.\nMake a histogram of the carat variable in the diamonds dataset that is available when you load the tidyverse package. Experiment with different binwidths. What binwidth reveals the most interesting patterns?\n\nWideMediumNarrow\n\n\n\ndiamonds %&gt;% ggplot(aes(x = carat)) +\n  geom_histogram(binwidth = 1)\n\n\n\n\n\n\n\n\n\n\n\ndiamonds %&gt;% ggplot(aes(x = carat)) +\n  geom_histogram(binwidth = 0.1)\n\n\n\n\n\n\n\n\n\n\n\ndiamonds %&gt;% ggplot(aes(x = carat)) +\n  geom_histogram(binwidth = 0.01)",
    "crumbs": [
      "R for Data Science: Outputs",
      "Whole Game",
      "1 Data Visualization"
    ]
  },
  {
    "objectID": "r4ds_outputs/chapter_1.html#visualizing-relationships",
    "href": "r4ds_outputs/chapter_1.html#visualizing-relationships",
    "title": "1 Data Visualization",
    "section": "1.5 Visualizing relationships",
    "text": "1.5 Visualizing relationships\n\nggplot(penguins, aes(x = species, y = body_mass_g)) +\n  geom_boxplot()\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n\n\n\n\n\n\nggplot(penguins, aes(x = body_mass_g, color = species)) +\n  geom_density(linewidth = 0.75)\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_density()`).\n\n\n\n\n\n\n\n\n\n\nggplot(penguins, aes(x = body_mass_g, color = species, fill = species)) +\n  geom_density(alpha = 0.5) +\n  scale_colour_viridis_d() + scale_fill_viridis_d()\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_density()`).\n\n\n\n\n\n\n\n\n\n\nAbsoluteRelative\n\n\n\nggplot(penguins, aes(x = island, fill = species)) +\n  geom_bar() +\n  scale_fill_viridis_d()\n\n\n\n\n\n\n\n\n\n\n\nggplot(penguins, aes(x = island, fill = species)) +\n  geom_bar(position = \"fill\") +\n  scale_fill_viridis_d()\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(penguins, aes(x = flipper_length_mm, y = body_mass_g)) +\n  geom_point()\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\n\nggplot(penguins, aes(x = flipper_length_mm, y = body_mass_g)) +\n  geom_point(aes(color = species, shape = island))\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\n\nggplot(penguins, aes(x = flipper_length_mm, y = body_mass_g)) +\n  geom_point(aes(color = species, shape = species)) +\n  facet_wrap(~island)\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\n\n1.5.5 Exercises\n\nThe mpg data frame that is bundled with the ggplot2 package contains 234 observations collected by the US Environmental Protection Agency on 38 car models. Which variables in mpg are categorical? Which variables are numerical? (Hint: Type ?mpg to read the documentation for the dataset.) How can you see this information when you run mpg?\n\nglimpse(mpg)\n\nRows: 234\nColumns: 11\n$ manufacturer &lt;chr&gt; \"audi\", \"audi\", \"audi\", \"audi\", \"audi\", \"audi\", \"audi\", \"…\n$ model        &lt;chr&gt; \"a4\", \"a4\", \"a4\", \"a4\", \"a4\", \"a4\", \"a4\", \"a4 quattro\", \"…\n$ displ        &lt;dbl&gt; 1.8, 1.8, 2.0, 2.0, 2.8, 2.8, 3.1, 1.8, 1.8, 2.0, 2.0, 2.…\n$ year         &lt;int&gt; 1999, 1999, 2008, 2008, 1999, 1999, 2008, 1999, 1999, 200…\n$ cyl          &lt;int&gt; 4, 4, 4, 4, 6, 6, 6, 4, 4, 4, 4, 6, 6, 6, 6, 6, 6, 8, 8, …\n$ trans        &lt;chr&gt; \"auto(l5)\", \"manual(m5)\", \"manual(m6)\", \"auto(av)\", \"auto…\n$ drv          &lt;chr&gt; \"f\", \"f\", \"f\", \"f\", \"f\", \"f\", \"f\", \"4\", \"4\", \"4\", \"4\", \"4…\n$ cty          &lt;int&gt; 18, 21, 20, 21, 16, 18, 18, 18, 16, 20, 19, 15, 17, 17, 1…\n$ hwy          &lt;int&gt; 29, 29, 31, 30, 26, 26, 27, 26, 25, 28, 27, 25, 25, 25, 2…\n$ fl           &lt;chr&gt; \"p\", \"p\", \"p\", \"p\", \"p\", \"p\", \"p\", \"p\", \"p\", \"p\", \"p\", \"p…\n$ class        &lt;chr&gt; \"compact\", \"compact\", \"compact\", \"compact\", \"compact\", \"c…\n\n\nThe following variables are categorical: manufacturer, model, trans, drv, fl, class. These are the numerical variables: displ, year, cyl, city, hwy.\nMake a scatterplot of hwy vs. displ using the mpg data frame. Next, map a third, numerical variable to color, then size, then both color and size, then shape. How do these aesthetics behave differently for categorical vs. numerical variables?\n\nBaseColorSizeColor & SizeShape\n\n\n\nmpg %&gt;% ggplot(aes(x = hwy, y = displ)) +\n  geom_point()\n\n\n\n\n\n\n\n\n\n\n\nmpg %&gt;% ggplot(aes(x = hwy, y = displ, color = year)) +\n  geom_point()\n\n\n\n\n\n\n\n\n\n\n\nmpg %&gt;% ggplot(aes(x = hwy, y = displ, size = hwy)) +\n  geom_point()\n\n\n\n\n\n\n\n\n\n\n\nmpg %&gt;% ggplot(aes(x = hwy, y = displ, color = trans, size = cyl)) +\n  geom_point()\n\n\n\n\n\n\n\n\n\n\n\nmpg %&gt;% ggplot(aes(x = hwy, y = displ, shape = drv)) +\n  geom_point()\n\n\n\n\n\n\n\n\n\n\n\nThe shape aesthetic throws an error when you try to map continuous variables instead of categorical variables. The coloraesthetic behaves differently between the two types of variables: a gradient with continuous and discrete colors with categorical.\nIn the scatterplot of hwy vs. displ, what happens if you map a third variable to linewidth?\n\nmpg %&gt;% ggplot(aes(x = hwy, y = displ, linewidth = hwy)) +\n  geom_point()\n\n\n\n\n\n\n\n\nThere is no line width to modify, so nothing changes. No error is given.\nWhat happens if you map the same variable to multiple aesthetics?\n\nmpg %&gt;% ggplot(aes(x = displ, y = displ, color = displ)) +\n  geom_point()\n\n\n\n\n\n\n\n\nYou are generally allowed to do so, but mapping variables to aesthetics should be meaningful.\nMake a scatterplot of bill_depth_mm vs. bill_length_mm and color the points by species. What does adding coloring by species reveal about the relationship between these two variables? What about faceting by species?\n\nColor by SpeciesFacet by Species\n\n\n\npenguins %&gt;% ggplot(aes(x = bill_depth_mm, y = bill_length_mm, color = species)) +\n  geom_point()\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\n\n\n\npenguins %&gt;% ggplot(aes(x = bill_depth_mm, y = bill_length_mm)) +\n  geom_point() +\n  facet_wrap(~species)\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\n\n\n\nBy mapping species to color, you are able to visualize the relationship between two variables and easily compare the difference in this relationship for each species. However, in cases where this could make the visualization cluttered, facet_wrap could be useful. This function allows you to see these relationships in isolation.\nWhy does the following yield two separate legends? How would you fix it to combine the two legends?\n\nggplot(\n  data = penguins,\n  mapping = aes(\n    x = bill_length_mm, y = bill_depth_mm, \n    color = species, shape = species\n  )\n) +\n  geom_point() +\n  labs(color = \"Species\")\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nThe legend is already automatically generated, so you can drop it. In any case, if you want to have one legend, make sure to match the name for each aesthetic that requires a legend.\nCreate the two following stacked bar plots. Which question can you answer with the first one? Which question can you answer with the second one?\n\n12\n\n\nFor each island, this plot shows the species distribution of penguin living within it.\n\nggplot(penguins, aes(x = island, fill = species)) +\n  geom_bar(position = \"fill\")\n\n\n\n\n\n\n\n\n\n\nFor each species, this plot shows the proportion of those that live in each island.\n\nggplot(penguins, aes(x = species, fill = island)) +\n  geom_bar(position = \"fill\")",
    "crumbs": [
      "R for Data Science: Outputs",
      "Whole Game",
      "1 Data Visualization"
    ]
  },
  {
    "objectID": "r4ds_outputs/chapter_1.html#saving-your-plots",
    "href": "r4ds_outputs/chapter_1.html#saving-your-plots",
    "title": "1 Data Visualization",
    "section": "1.6 Saving your plots",
    "text": "1.6 Saving your plots\n\nggplot(penguins, aes(x = flipper_length_mm, y = body_mass_g)) +\n  geom_point()\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\nggsave(filename = \"penguin-plot.png\")\n\nSaving 7 x 5 in image\n\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n1.6.1 Exercises\n\nRun the following lines of code. Which of the two plots is saved as mpg-plot.png? Why?\n\nggplot(mpg, aes(x = class)) +\n  geom_bar()\n\n\n\n\n\n\n\nggplot(mpg, aes(x = cty, y = hwy)) +\n  geom_point()\n\n\n\n\n\n\n\nggsave(\"mpg-plot.png\")\n\nSaving 7 x 5 in image\n\n\nThe scatterplot is saved because it is the most recent plot created.\nWhat do you need to change in the code above to save the plot as a PDF instead of a PNG? How could you find out what types of image files would work in ggsave()?\nYou could change the extension name or use the device argument. More information on this can be found by running ?ggsave.",
    "crumbs": [
      "R for Data Science: Outputs",
      "Whole Game",
      "1 Data Visualization"
    ]
  },
  {
    "objectID": "r4ds_outputs/chapter_2.html",
    "href": "r4ds_outputs/chapter_2.html",
    "title": "2 Workflow: basics",
    "section": "",
    "text": "1 / 200 * 30\n\n[1] 0.15\n\n(59 + 73 + 2) / 3\n\n[1] 44.66667\n\nsin(pi / 2)\n\n[1] 1\n\n\n\nx &lt;- 3 * 4\nx\n\n[1] 12\n\n\n\nprimes &lt;- c(2, 3, 5, 7, 11, 13)\n\n\nprimes * 2\n\n[1]  4  6 10 14 22 26\n\nprimes - 1\n\n[1]  1  2  4  6 10 12",
    "crumbs": [
      "R for Data Science: Outputs",
      "Whole Game",
      "2 Workflow: basics"
    ]
  },
  {
    "objectID": "r4ds_outputs/chapter_2.html#coding-basics",
    "href": "r4ds_outputs/chapter_2.html#coding-basics",
    "title": "2 Workflow: basics",
    "section": "",
    "text": "1 / 200 * 30\n\n[1] 0.15\n\n(59 + 73 + 2) / 3\n\n[1] 44.66667\n\nsin(pi / 2)\n\n[1] 1\n\n\n\nx &lt;- 3 * 4\nx\n\n[1] 12\n\n\n\nprimes &lt;- c(2, 3, 5, 7, 11, 13)\n\n\nprimes * 2\n\n[1]  4  6 10 14 22 26\n\nprimes - 1\n\n[1]  1  2  4  6 10 12",
    "crumbs": [
      "R for Data Science: Outputs",
      "Whole Game",
      "2 Workflow: basics"
    ]
  },
  {
    "objectID": "r4ds_outputs/chapter_2.html#comments",
    "href": "r4ds_outputs/chapter_2.html#comments",
    "title": "2 Workflow: basics",
    "section": "2.2 Comments",
    "text": "2.2 Comments\n\n# create vector of primes\nprimes &lt;- c(2, 3, 5, 7, 11, 13)\n\n# multiply primes by 2\nprimes * 2\n\n[1]  4  6 10 14 22 26\n\n#&gt; [1]  4  6 10 14 22 26",
    "crumbs": [
      "R for Data Science: Outputs",
      "Whole Game",
      "2 Workflow: basics"
    ]
  },
  {
    "objectID": "r4ds_outputs/chapter_2.html#whats-in-a-name",
    "href": "r4ds_outputs/chapter_2.html#whats-in-a-name",
    "title": "2 Workflow: basics",
    "section": "2.3 What’s in a name?",
    "text": "2.3 What’s in a name?\n\nr_rocks &lt;- 2^3\nR_rocks\n\nError in eval(expr, envir, enclos): object 'R_rocks' not found\n\nr_rock\n\nError in eval(expr, envir, enclos): object 'r_rock' not found\n\nr_rocks\n\n[1] 8",
    "crumbs": [
      "R for Data Science: Outputs",
      "Whole Game",
      "2 Workflow: basics"
    ]
  },
  {
    "objectID": "r4ds_outputs/chapter_2.html#calling-functions",
    "href": "r4ds_outputs/chapter_2.html#calling-functions",
    "title": "2 Workflow: basics",
    "section": "2.4 Calling functions",
    "text": "2.4 Calling functions\n\nseq(from = 1, to = 10)\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\nseq(1, 10)\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n\n\nx &lt;- \"hello world\"",
    "crumbs": [
      "R for Data Science: Outputs",
      "Whole Game",
      "2 Workflow: basics"
    ]
  },
  {
    "objectID": "r4ds_outputs/chapter_2.html#exercises",
    "href": "r4ds_outputs/chapter_2.html#exercises",
    "title": "2 Workflow: basics",
    "section": "2.5 Exercises",
    "text": "2.5 Exercises\n\nWhy does this code not work?\n\nmy_variable &lt;- 10\nmy_varıable\n\nError in eval(expr, envir, enclos): object 'my_varıable' not found\n\n#&gt; Error in eval(expr, envir, enclos): object 'my_varıable' not found\n\nThe second line is referring to a second object that does not yet exist.\nTweak each of the following R commands so that they run correctly:\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nggplot(data = mpg, mapping = aes(x = displ, y = hwy)) + \n  geom_point() +\n  geom_smooth(method = \"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nPress Option + Shift + K / Alt + Shift + K. What happens? How can you get to the same place using the menus?\nIt displays shortcut keys. You can find it in Help &gt; Keyboard Shortcuts Help.\nLet’s revisit an exercise from the Section 1.6. Run the following lines of code. Which of the two plots is saved as mpg-plot.png? Why?\n\nmy_bar_plot &lt;- ggplot(mpg, aes(x = class)) +\n  geom_bar()\nmy_scatter_plot &lt;- ggplot(mpg, aes(x = cty, y = hwy)) +\n  geom_point()\nggsave(filename = \"mpg-plot.png\", plot = my_bar_plot)\n\nSaving 7 x 5 in image\n\n\nThe bar plot is saved because it was specified in the argument plot which plot object should be saved.",
    "crumbs": [
      "R for Data Science: Outputs",
      "Whole Game",
      "2 Workflow: basics"
    ]
  },
  {
    "objectID": "r4ds_outputs/chapter_3.html",
    "href": "r4ds_outputs/chapter_3.html",
    "title": "3 Data Transformation",
    "section": "",
    "text": "library(nycflights13)\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n\n\n\n\nflights\n\n# A tibble: 336,776 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     1     1      517            515         2      830            819\n 2  2013     1     1      533            529         4      850            830\n 3  2013     1     1      542            540         2      923            850\n 4  2013     1     1      544            545        -1     1004           1022\n 5  2013     1     1      554            600        -6      812            837\n 6  2013     1     1      554            558        -4      740            728\n 7  2013     1     1      555            600        -5      913            854\n 8  2013     1     1      557            600        -3      709            723\n 9  2013     1     1      557            600        -3      838            846\n10  2013     1     1      558            600        -2      753            745\n# ℹ 336,766 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\n\nglimpse(flights)\n\nRows: 336,776\nColumns: 19\n$ year           &lt;int&gt; 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2…\n$ month          &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ day            &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ dep_time       &lt;int&gt; 517, 533, 542, 544, 554, 554, 555, 557, 557, 558, 558, …\n$ sched_dep_time &lt;int&gt; 515, 529, 540, 545, 600, 558, 600, 600, 600, 600, 600, …\n$ dep_delay      &lt;dbl&gt; 2, 4, 2, -1, -6, -4, -5, -3, -3, -2, -2, -2, -2, -2, -1…\n$ arr_time       &lt;int&gt; 830, 850, 923, 1004, 812, 740, 913, 709, 838, 753, 849,…\n$ sched_arr_time &lt;int&gt; 819, 830, 850, 1022, 837, 728, 854, 723, 846, 745, 851,…\n$ arr_delay      &lt;dbl&gt; 11, 20, 33, -18, -25, 12, 19, -14, -8, 8, -2, -3, 7, -1…\n$ carrier        &lt;chr&gt; \"UA\", \"UA\", \"AA\", \"B6\", \"DL\", \"UA\", \"B6\", \"EV\", \"B6\", \"…\n$ flight         &lt;int&gt; 1545, 1714, 1141, 725, 461, 1696, 507, 5708, 79, 301, 4…\n$ tailnum        &lt;chr&gt; \"N14228\", \"N24211\", \"N619AA\", \"N804JB\", \"N668DN\", \"N394…\n$ origin         &lt;chr&gt; \"EWR\", \"LGA\", \"JFK\", \"JFK\", \"LGA\", \"EWR\", \"EWR\", \"LGA\",…\n$ dest           &lt;chr&gt; \"IAH\", \"IAH\", \"MIA\", \"BQN\", \"ATL\", \"ORD\", \"FLL\", \"IAD\",…\n$ air_time       &lt;dbl&gt; 227, 227, 160, 183, 116, 150, 158, 53, 140, 138, 149, 1…\n$ distance       &lt;dbl&gt; 1400, 1416, 1089, 1576, 762, 719, 1065, 229, 944, 733, …\n$ hour           &lt;dbl&gt; 5, 5, 5, 5, 6, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 5, 6, 6, 6…\n$ minute         &lt;dbl&gt; 15, 29, 40, 45, 0, 58, 0, 0, 0, 0, 0, 0, 0, 0, 0, 59, 0…\n$ time_hour      &lt;dttm&gt; 2013-01-01 05:00:00, 2013-01-01 05:00:00, 2013-01-01 0…\n\n\n\n\n\n\nflights |&gt;\n  filter(dest == \"IAH\") |&gt; \n  group_by(year, month, day) |&gt; \n  summarize(\n    arr_delay = mean(arr_delay, na.rm = TRUE)\n  )\n\n`summarise()` has grouped output by 'year', 'month'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 365 × 4\n# Groups:   year, month [12]\n    year month   day arr_delay\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;     &lt;dbl&gt;\n 1  2013     1     1     17.8 \n 2  2013     1     2      7   \n 3  2013     1     3     18.3 \n 4  2013     1     4     -3.2 \n 5  2013     1     5     20.2 \n 6  2013     1     6      9.28\n 7  2013     1     7     -7.74\n 8  2013     1     8      7.79\n 9  2013     1     9     18.1 \n10  2013     1    10      6.68\n# ℹ 355 more rows",
    "crumbs": [
      "R for Data Science: Outputs",
      "Whole Game",
      "3 Data Transformation"
    ]
  },
  {
    "objectID": "r4ds_outputs/chapter_3.html#introduction",
    "href": "r4ds_outputs/chapter_3.html#introduction",
    "title": "3 Data Transformation",
    "section": "",
    "text": "library(nycflights13)\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n\n\n\n\nflights\n\n# A tibble: 336,776 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     1     1      517            515         2      830            819\n 2  2013     1     1      533            529         4      850            830\n 3  2013     1     1      542            540         2      923            850\n 4  2013     1     1      544            545        -1     1004           1022\n 5  2013     1     1      554            600        -6      812            837\n 6  2013     1     1      554            558        -4      740            728\n 7  2013     1     1      555            600        -5      913            854\n 8  2013     1     1      557            600        -3      709            723\n 9  2013     1     1      557            600        -3      838            846\n10  2013     1     1      558            600        -2      753            745\n# ℹ 336,766 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\n\nglimpse(flights)\n\nRows: 336,776\nColumns: 19\n$ year           &lt;int&gt; 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2…\n$ month          &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ day            &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ dep_time       &lt;int&gt; 517, 533, 542, 544, 554, 554, 555, 557, 557, 558, 558, …\n$ sched_dep_time &lt;int&gt; 515, 529, 540, 545, 600, 558, 600, 600, 600, 600, 600, …\n$ dep_delay      &lt;dbl&gt; 2, 4, 2, -1, -6, -4, -5, -3, -3, -2, -2, -2, -2, -2, -1…\n$ arr_time       &lt;int&gt; 830, 850, 923, 1004, 812, 740, 913, 709, 838, 753, 849,…\n$ sched_arr_time &lt;int&gt; 819, 830, 850, 1022, 837, 728, 854, 723, 846, 745, 851,…\n$ arr_delay      &lt;dbl&gt; 11, 20, 33, -18, -25, 12, 19, -14, -8, 8, -2, -3, 7, -1…\n$ carrier        &lt;chr&gt; \"UA\", \"UA\", \"AA\", \"B6\", \"DL\", \"UA\", \"B6\", \"EV\", \"B6\", \"…\n$ flight         &lt;int&gt; 1545, 1714, 1141, 725, 461, 1696, 507, 5708, 79, 301, 4…\n$ tailnum        &lt;chr&gt; \"N14228\", \"N24211\", \"N619AA\", \"N804JB\", \"N668DN\", \"N394…\n$ origin         &lt;chr&gt; \"EWR\", \"LGA\", \"JFK\", \"JFK\", \"LGA\", \"EWR\", \"EWR\", \"LGA\",…\n$ dest           &lt;chr&gt; \"IAH\", \"IAH\", \"MIA\", \"BQN\", \"ATL\", \"ORD\", \"FLL\", \"IAD\",…\n$ air_time       &lt;dbl&gt; 227, 227, 160, 183, 116, 150, 158, 53, 140, 138, 149, 1…\n$ distance       &lt;dbl&gt; 1400, 1416, 1089, 1576, 762, 719, 1065, 229, 944, 733, …\n$ hour           &lt;dbl&gt; 5, 5, 5, 5, 6, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 5, 6, 6, 6…\n$ minute         &lt;dbl&gt; 15, 29, 40, 45, 0, 58, 0, 0, 0, 0, 0, 0, 0, 0, 0, 59, 0…\n$ time_hour      &lt;dttm&gt; 2013-01-01 05:00:00, 2013-01-01 05:00:00, 2013-01-01 0…\n\n\n\n\n\n\nflights |&gt;\n  filter(dest == \"IAH\") |&gt; \n  group_by(year, month, day) |&gt; \n  summarize(\n    arr_delay = mean(arr_delay, na.rm = TRUE)\n  )\n\n`summarise()` has grouped output by 'year', 'month'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 365 × 4\n# Groups:   year, month [12]\n    year month   day arr_delay\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;     &lt;dbl&gt;\n 1  2013     1     1     17.8 \n 2  2013     1     2      7   \n 3  2013     1     3     18.3 \n 4  2013     1     4     -3.2 \n 5  2013     1     5     20.2 \n 6  2013     1     6      9.28\n 7  2013     1     7     -7.74\n 8  2013     1     8      7.79\n 9  2013     1     9     18.1 \n10  2013     1    10      6.68\n# ℹ 355 more rows",
    "crumbs": [
      "R for Data Science: Outputs",
      "Whole Game",
      "3 Data Transformation"
    ]
  },
  {
    "objectID": "r4ds_outputs/chapter_3.html#rows",
    "href": "r4ds_outputs/chapter_3.html#rows",
    "title": "3 Data Transformation",
    "section": "3.2 Rows",
    "text": "3.2 Rows\n\n3.2.1 filter()\n\nflights |&gt; \n  filter(dep_delay &gt; 120)\n\n# A tibble: 9,723 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     1     1      848           1835       853     1001           1950\n 2  2013     1     1      957            733       144     1056            853\n 3  2013     1     1     1114            900       134     1447           1222\n 4  2013     1     1     1540           1338       122     2020           1825\n 5  2013     1     1     1815           1325       290     2120           1542\n 6  2013     1     1     1842           1422       260     1958           1535\n 7  2013     1     1     1856           1645       131     2212           2005\n 8  2013     1     1     1934           1725       129     2126           1855\n 9  2013     1     1     1938           1703       155     2109           1823\n10  2013     1     1     1942           1705       157     2124           1830\n# ℹ 9,713 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\n\n# Flights that departed on January 1\nflights |&gt; \n  filter(month == 1 & day == 1)\n\n# A tibble: 842 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     1     1      517            515         2      830            819\n 2  2013     1     1      533            529         4      850            830\n 3  2013     1     1      542            540         2      923            850\n 4  2013     1     1      544            545        -1     1004           1022\n 5  2013     1     1      554            600        -6      812            837\n 6  2013     1     1      554            558        -4      740            728\n 7  2013     1     1      555            600        -5      913            854\n 8  2013     1     1      557            600        -3      709            723\n 9  2013     1     1      557            600        -3      838            846\n10  2013     1     1      558            600        -2      753            745\n# ℹ 832 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\n\n# Flights that departed in January or February\nflights |&gt; \n  filter(month == 1 | month == 2)\n\n# A tibble: 51,955 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     1     1      517            515         2      830            819\n 2  2013     1     1      533            529         4      850            830\n 3  2013     1     1      542            540         2      923            850\n 4  2013     1     1      544            545        -1     1004           1022\n 5  2013     1     1      554            600        -6      812            837\n 6  2013     1     1      554            558        -4      740            728\n 7  2013     1     1      555            600        -5      913            854\n 8  2013     1     1      557            600        -3      709            723\n 9  2013     1     1      557            600        -3      838            846\n10  2013     1     1      558            600        -2      753            745\n# ℹ 51,945 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\n\n# A shorter way to select flights that departed in January or February\nflights |&gt; \n  filter(month %in% c(1, 2))\n\n# A tibble: 51,955 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     1     1      517            515         2      830            819\n 2  2013     1     1      533            529         4      850            830\n 3  2013     1     1      542            540         2      923            850\n 4  2013     1     1      544            545        -1     1004           1022\n 5  2013     1     1      554            600        -6      812            837\n 6  2013     1     1      554            558        -4      740            728\n 7  2013     1     1      555            600        -5      913            854\n 8  2013     1     1      557            600        -3      709            723\n 9  2013     1     1      557            600        -3      838            846\n10  2013     1     1      558            600        -2      753            745\n# ℹ 51,945 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\n\njan1 &lt;- flights |&gt; \n  filter(month == 1 & day == 1)\n\n\n\n3.2.2 Common Mistakes\n\nflights |&gt; \n  filter(month = 1)\n\nError in `filter()`:\n! We detected a named input.\nℹ This usually means that you've used `=` instead of `==`.\nℹ Did you mean `month == 1`?\n\n\n\n# Checks condition month == 1 and condition 2\nflights |&gt; \n  filter(month == 1 | 2)\n\n# A tibble: 336,776 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     1     1      517            515         2      830            819\n 2  2013     1     1      533            529         4      850            830\n 3  2013     1     1      542            540         2      923            850\n 4  2013     1     1      544            545        -1     1004           1022\n 5  2013     1     1      554            600        -6      812            837\n 6  2013     1     1      554            558        -4      740            728\n 7  2013     1     1      555            600        -5      913            854\n 8  2013     1     1      557            600        -3      709            723\n 9  2013     1     1      557            600        -3      838            846\n10  2013     1     1      558            600        -2      753            745\n# ℹ 336,766 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\n\n\n3.2.3 arrange()\n\nflights |&gt; \n  filter(month == 1 | 2)\n\n# A tibble: 336,776 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     1     1      517            515         2      830            819\n 2  2013     1     1      533            529         4      850            830\n 3  2013     1     1      542            540         2      923            850\n 4  2013     1     1      544            545        -1     1004           1022\n 5  2013     1     1      554            600        -6      812            837\n 6  2013     1     1      554            558        -4      740            728\n 7  2013     1     1      555            600        -5      913            854\n 8  2013     1     1      557            600        -3      709            723\n 9  2013     1     1      557            600        -3      838            846\n10  2013     1     1      558            600        -2      753            745\n# ℹ 336,766 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\n\nflights |&gt; \n  arrange(desc(dep_delay))\n\n# A tibble: 336,776 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     1     9      641            900      1301     1242           1530\n 2  2013     6    15     1432           1935      1137     1607           2120\n 3  2013     1    10     1121           1635      1126     1239           1810\n 4  2013     9    20     1139           1845      1014     1457           2210\n 5  2013     7    22      845           1600      1005     1044           1815\n 6  2013     4    10     1100           1900       960     1342           2211\n 7  2013     3    17     2321            810       911      135           1020\n 8  2013     6    27      959           1900       899     1236           2226\n 9  2013     7    22     2257            759       898      121           1026\n10  2013    12     5      756           1700       896     1058           2020\n# ℹ 336,766 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\n\n\n3.2.4 distinct()\n\n# Remove duplicate rows, if any\nflights |&gt; \n  distinct()\n\n# A tibble: 336,776 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     1     1      517            515         2      830            819\n 2  2013     1     1      533            529         4      850            830\n 3  2013     1     1      542            540         2      923            850\n 4  2013     1     1      544            545        -1     1004           1022\n 5  2013     1     1      554            600        -6      812            837\n 6  2013     1     1      554            558        -4      740            728\n 7  2013     1     1      555            600        -5      913            854\n 8  2013     1     1      557            600        -3      709            723\n 9  2013     1     1      557            600        -3      838            846\n10  2013     1     1      558            600        -2      753            745\n# ℹ 336,766 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\n\n# Find all unique origin and destination pairs\nflights |&gt; \n  distinct(origin, dest)\n\n# A tibble: 224 × 2\n   origin dest \n   &lt;chr&gt;  &lt;chr&gt;\n 1 EWR    IAH  \n 2 LGA    IAH  \n 3 JFK    MIA  \n 4 JFK    BQN  \n 5 LGA    ATL  \n 6 EWR    ORD  \n 7 EWR    FLL  \n 8 LGA    IAD  \n 9 JFK    MCO  \n10 LGA    ORD  \n# ℹ 214 more rows\n\n\n\nflights |&gt; \n  distinct(origin, dest, .keep_all = TRUE)\n\n# A tibble: 224 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     1     1      517            515         2      830            819\n 2  2013     1     1      533            529         4      850            830\n 3  2013     1     1      542            540         2      923            850\n 4  2013     1     1      544            545        -1     1004           1022\n 5  2013     1     1      554            600        -6      812            837\n 6  2013     1     1      554            558        -4      740            728\n 7  2013     1     1      555            600        -5      913            854\n 8  2013     1     1      557            600        -3      709            723\n 9  2013     1     1      557            600        -3      838            846\n10  2013     1     1      558            600        -2      753            745\n# ℹ 214 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\n\nflights |&gt;\n  count(origin, dest, sort = TRUE)\n\n# A tibble: 224 × 3\n   origin dest      n\n   &lt;chr&gt;  &lt;chr&gt; &lt;int&gt;\n 1 JFK    LAX   11262\n 2 LGA    ATL   10263\n 3 LGA    ORD    8857\n 4 JFK    SFO    8204\n 5 LGA    CLT    6168\n 6 EWR    ORD    6100\n 7 JFK    BOS    5898\n 8 LGA    MIA    5781\n 9 JFK    MCO    5464\n10 EWR    BOS    5327\n# ℹ 214 more rows\n\n\n\n\n3.2.5 Exercises\n\nIn a single pipeline for each condition, find all flights that meet the condition:\n\nHad an arrival delay of two or more hours\nFlew to Houston (IAH or HOU)\nWere operated by United, American, or Delta\nDeparted in summer (July, August, and September)\nArrived more than two hours late, but didn’t leave late\nWere delayed by at least an hour, but made up over 30 minutes in flight\n\n\nabcdef\n\n\n\nflights %&gt;% \n  filter(arr_delay &gt;= 120)\n\n# A tibble: 10,200 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     1     1      811            630       101     1047            830\n 2  2013     1     1      848           1835       853     1001           1950\n 3  2013     1     1      957            733       144     1056            853\n 4  2013     1     1     1114            900       134     1447           1222\n 5  2013     1     1     1505           1310       115     1638           1431\n 6  2013     1     1     1525           1340       105     1831           1626\n 7  2013     1     1     1549           1445        64     1912           1656\n 8  2013     1     1     1558           1359       119     1718           1515\n 9  2013     1     1     1732           1630        62     2028           1825\n10  2013     1     1     1803           1620       103     2008           1750\n# ℹ 10,190 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\n\n\n\nflights %&gt;% \n  filter(dest %in% c(\"IAH\", \"HOU\"))\n\n# A tibble: 9,313 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     1     1      517            515         2      830            819\n 2  2013     1     1      533            529         4      850            830\n 3  2013     1     1      623            627        -4      933            932\n 4  2013     1     1      728            732        -4     1041           1038\n 5  2013     1     1      739            739         0     1104           1038\n 6  2013     1     1      908            908         0     1228           1219\n 7  2013     1     1     1028           1026         2     1350           1339\n 8  2013     1     1     1044           1045        -1     1352           1351\n 9  2013     1     1     1114            900       134     1447           1222\n10  2013     1     1     1205           1200         5     1503           1505\n# ℹ 9,303 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\n\n\n\nflights %&gt;% \n  filter(carrier %in% c(\"UA\", \"AA\", \"DL\"))\n\n# A tibble: 139,504 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     1     1      517            515         2      830            819\n 2  2013     1     1      533            529         4      850            830\n 3  2013     1     1      542            540         2      923            850\n 4  2013     1     1      554            600        -6      812            837\n 5  2013     1     1      554            558        -4      740            728\n 6  2013     1     1      558            600        -2      753            745\n 7  2013     1     1      558            600        -2      924            917\n 8  2013     1     1      558            600        -2      923            937\n 9  2013     1     1      559            600        -1      941            910\n10  2013     1     1      559            600        -1      854            902\n# ℹ 139,494 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\n\n\n\nflights %&gt;% \n  filter(month %in% c(7, 8, 9)) \n\n# A tibble: 86,326 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     7     1        1           2029       212      236           2359\n 2  2013     7     1        2           2359         3      344            344\n 3  2013     7     1       29           2245       104      151              1\n 4  2013     7     1       43           2130       193      322             14\n 5  2013     7     1       44           2150       174      300            100\n 6  2013     7     1       46           2051       235      304           2358\n 7  2013     7     1       48           2001       287      308           2305\n 8  2013     7     1       58           2155       183      335             43\n 9  2013     7     1      100           2146       194      327             30\n10  2013     7     1      100           2245       135      337            135\n# ℹ 86,316 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\n\n\n\nflights %&gt;% \n  filter(arr_delay &gt; 120, dep_delay &lt;= 0)\n\n# A tibble: 29 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     1    27     1419           1420        -1     1754           1550\n 2  2013    10     7     1350           1350         0     1736           1526\n 3  2013    10     7     1357           1359        -2     1858           1654\n 4  2013    10    16      657            700        -3     1258           1056\n 5  2013    11     1      658            700        -2     1329           1015\n 6  2013     3    18     1844           1847        -3       39           2219\n 7  2013     4    17     1635           1640        -5     2049           1845\n 8  2013     4    18      558            600        -2     1149            850\n 9  2013     4    18      655            700        -5     1213            950\n10  2013     5    22     1827           1830        -3     2217           2010\n# ℹ 19 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\n\n\n\nflights %&gt;% \n  filter(dep_delay &gt;= 60, dep_delay - arr_delay &gt; 30)\n\n# A tibble: 1,844 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     1     1     2205           1720       285       46           2040\n 2  2013     1     1     2326           2130       116      131             18\n 3  2013     1     3     1503           1221       162     1803           1555\n 4  2013     1     3     1839           1700        99     2056           1950\n 5  2013     1     3     1850           1745        65     2148           2120\n 6  2013     1     3     1941           1759       102     2246           2139\n 7  2013     1     3     1950           1845        65     2228           2227\n 8  2013     1     3     2015           1915        60     2135           2111\n 9  2013     1     3     2257           2000       177       45           2224\n10  2013     1     4     1917           1700       137     2135           1950\n# ℹ 1,834 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\n\n\n\nSort flights to find the flights with longest departure delays. Find the flights that left earliest in the morning.\n\nflights %&gt;% \n  arrange(desc(dep_delay)) %&gt;% \n  arrange(dep_time)\n\n# A tibble: 336,776 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     4    10        1           1930       271      106           2101\n 2  2013     5    22        1           1935       266      154           2140\n 3  2013     6    24        1           1950       251      105           2130\n 4  2013     7     1        1           2029       212      236           2359\n 5  2013     1    31        1           2100       181      124           2225\n 6  2013     2    11        1           2100       181      111           2225\n 7  2013     3    18        1           2128       153      247           2355\n 8  2013     6    25        1           2130       151      249             14\n 9  2013     2    24        1           2245        76      121           2354\n10  2013     1    13        1           2249        72      108           2357\n# ℹ 336,766 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\nSort flights to find the fastest flights. (Hint: Try including a math calculation inside of your function.) \n\nflights %&gt;% \n  arrange(desc(distance / air_time))\n\n# A tibble: 336,776 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     5    25     1709           1700         9     1923           1937\n 2  2013     7     2     1558           1513        45     1745           1719\n 3  2013     5    13     2040           2025        15     2225           2226\n 4  2013     3    23     1914           1910         4     2045           2043\n 5  2013     1    12     1559           1600        -1     1849           1917\n 6  2013    11    17      650            655        -5     1059           1150\n 7  2013     2    21     2355           2358        -3      412            438\n 8  2013    11    17      759            800        -1     1212           1255\n 9  2013    11    16     2003           1925        38       17             36\n10  2013    11    16     2349           2359       -10      402            440\n# ℹ 336,766 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\nWas there a flight on every day of 2013?\n\nflights %&gt;% \n  filter(year == 2013) %&gt;% \n  distinct(month, day) %&gt;% \n  count()\n\n# A tibble: 1 × 1\n      n\n  &lt;int&gt;\n1   365\n\n\nYes!\nWhich flights traveled the farthest distance? Which traveled the least distance?\n\nFarthest distanceLeast distance\n\n\n\nflights %&gt;% \n  arrange(desc(distance))\n\n# A tibble: 336,776 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     1     1      857            900        -3     1516           1530\n 2  2013     1     2      909            900         9     1525           1530\n 3  2013     1     3      914            900        14     1504           1530\n 4  2013     1     4      900            900         0     1516           1530\n 5  2013     1     5      858            900        -2     1519           1530\n 6  2013     1     6     1019            900        79     1558           1530\n 7  2013     1     7     1042            900       102     1620           1530\n 8  2013     1     8      901            900         1     1504           1530\n 9  2013     1     9      641            900      1301     1242           1530\n10  2013     1    10      859            900        -1     1449           1530\n# ℹ 336,766 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\n\n\n\nflights %&gt;% \n  arrange(distance)\n\n# A tibble: 336,776 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     7    27       NA            106        NA       NA            245\n 2  2013     1     3     2127           2129        -2     2222           2224\n 3  2013     1     4     1240           1200        40     1333           1306\n 4  2013     1     4     1829           1615       134     1937           1721\n 5  2013     1     4     2128           2129        -1     2218           2224\n 6  2013     1     5     1155           1200        -5     1241           1306\n 7  2013     1     6     2125           2129        -4     2224           2224\n 8  2013     1     7     2124           2129        -5     2212           2224\n 9  2013     1     8     2127           2130        -3     2304           2225\n10  2013     1     9     2126           2129        -3     2217           2224\n# ℹ 336,766 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\n\n\n\nDoes it matter what order you used filter() and arrange() if you’re using both? Why/why not? Think about the results and how much work the functions would have to do.\nYes, the order matters. Each function call returns a modified tibble, which the next function uses. Likely, you want to filter the tibble first before arranging it. Order can matter. Each function call returns a modified table, which the next function uses. In this case, however, the results will be the same. I cannot test for it, but it’s probably best to filter before arranging, so you are not sorting values you are not interested in the first place.",
    "crumbs": [
      "R for Data Science: Outputs",
      "Whole Game",
      "3 Data Transformation"
    ]
  },
  {
    "objectID": "r4ds_outputs/chapter_3.html#columns",
    "href": "r4ds_outputs/chapter_3.html#columns",
    "title": "3 Data Transformation",
    "section": "3.3 Columns",
    "text": "3.3 Columns\n\nflights |&gt; \n  mutate(\n    gain = dep_delay - arr_delay,\n    speed = distance / air_time * 60\n  )\n\n# A tibble: 336,776 × 21\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     1     1      517            515         2      830            819\n 2  2013     1     1      533            529         4      850            830\n 3  2013     1     1      542            540         2      923            850\n 4  2013     1     1      544            545        -1     1004           1022\n 5  2013     1     1      554            600        -6      812            837\n 6  2013     1     1      554            558        -4      740            728\n 7  2013     1     1      555            600        -5      913            854\n 8  2013     1     1      557            600        -3      709            723\n 9  2013     1     1      557            600        -3      838            846\n10  2013     1     1      558            600        -2      753            745\n# ℹ 336,766 more rows\n# ℹ 13 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;, gain &lt;dbl&gt;, speed &lt;dbl&gt;\n\n\n\nflights |&gt; \n  mutate(\n    gain = dep_delay - arr_delay,\n    speed = distance / air_time * 60,\n    .before = 1\n  )\n\n# A tibble: 336,776 × 21\n    gain speed  year month   day dep_time sched_dep_time dep_delay arr_time\n   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;\n 1    -9  370.  2013     1     1      517            515         2      830\n 2   -16  374.  2013     1     1      533            529         4      850\n 3   -31  408.  2013     1     1      542            540         2      923\n 4    17  517.  2013     1     1      544            545        -1     1004\n 5    19  394.  2013     1     1      554            600        -6      812\n 6   -16  288.  2013     1     1      554            558        -4      740\n 7   -24  404.  2013     1     1      555            600        -5      913\n 8    11  259.  2013     1     1      557            600        -3      709\n 9     5  405.  2013     1     1      557            600        -3      838\n10   -10  319.  2013     1     1      558            600        -2      753\n# ℹ 336,766 more rows\n# ℹ 12 more variables: sched_arr_time &lt;int&gt;, arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;,\n#   flight &lt;int&gt;, tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;,\n#   distance &lt;dbl&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\n\nflights |&gt; \n  mutate(\n    gain = dep_delay - arr_delay,\n    speed = distance / air_time * 60,\n    .after = day\n  )\n\n# A tibble: 336,776 × 21\n    year month   day  gain speed dep_time sched_dep_time dep_delay arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;\n 1  2013     1     1    -9  370.      517            515         2      830\n 2  2013     1     1   -16  374.      533            529         4      850\n 3  2013     1     1   -31  408.      542            540         2      923\n 4  2013     1     1    17  517.      544            545        -1     1004\n 5  2013     1     1    19  394.      554            600        -6      812\n 6  2013     1     1   -16  288.      554            558        -4      740\n 7  2013     1     1   -24  404.      555            600        -5      913\n 8  2013     1     1    11  259.      557            600        -3      709\n 9  2013     1     1     5  405.      557            600        -3      838\n10  2013     1     1   -10  319.      558            600        -2      753\n# ℹ 336,766 more rows\n# ℹ 12 more variables: sched_arr_time &lt;int&gt;, arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;,\n#   flight &lt;int&gt;, tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;,\n#   distance &lt;dbl&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\n\n# Flights is not modified here\nflights |&gt; \n  mutate(\n    gain = dep_delay - arr_delay,\n    hours = air_time / 60,\n    gain_per_hour = gain / hours,\n    .keep = \"used\"\n  )\n\n# A tibble: 336,776 × 6\n   dep_delay arr_delay air_time  gain hours gain_per_hour\n       &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;         &lt;dbl&gt;\n 1         2        11      227    -9 3.78          -2.38\n 2         4        20      227   -16 3.78          -4.23\n 3         2        33      160   -31 2.67         -11.6 \n 4        -1       -18      183    17 3.05           5.57\n 5        -6       -25      116    19 1.93           9.83\n 6        -4        12      150   -16 2.5           -6.4 \n 7        -5        19      158   -24 2.63          -9.11\n 8        -3       -14       53    11 0.883         12.5 \n 9        -3        -8      140     5 2.33           2.14\n10        -2         8      138   -10 2.3           -4.35\n# ℹ 336,766 more rows\n\n\n\n3.3.2 select()\n\nflights |&gt; \n  select(year, month, day)\n\n# A tibble: 336,776 × 3\n    year month   day\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;\n 1  2013     1     1\n 2  2013     1     1\n 3  2013     1     1\n 4  2013     1     1\n 5  2013     1     1\n 6  2013     1     1\n 7  2013     1     1\n 8  2013     1     1\n 9  2013     1     1\n10  2013     1     1\n# ℹ 336,766 more rows\n\n\n\nflights |&gt; \n  select(year:day)\n\n# A tibble: 336,776 × 3\n    year month   day\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;\n 1  2013     1     1\n 2  2013     1     1\n 3  2013     1     1\n 4  2013     1     1\n 5  2013     1     1\n 6  2013     1     1\n 7  2013     1     1\n 8  2013     1     1\n 9  2013     1     1\n10  2013     1     1\n# ℹ 336,766 more rows\n\n\n\nflights |&gt; \n  select(!year:day) # select(-year:day) works but not recommended\n\n# A tibble: 336,776 × 16\n   dep_time sched_dep_time dep_delay arr_time sched_arr_time arr_delay carrier\n      &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt; &lt;chr&gt;  \n 1      517            515         2      830            819        11 UA     \n 2      533            529         4      850            830        20 UA     \n 3      542            540         2      923            850        33 AA     \n 4      544            545        -1     1004           1022       -18 B6     \n 5      554            600        -6      812            837       -25 DL     \n 6      554            558        -4      740            728        12 UA     \n 7      555            600        -5      913            854        19 B6     \n 8      557            600        -3      709            723       -14 EV     \n 9      557            600        -3      838            846        -8 B6     \n10      558            600        -2      753            745         8 AA     \n# ℹ 336,766 more rows\n# ℹ 9 more variables: flight &lt;int&gt;, tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;,\n#   air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\n\nflights |&gt; \n  select(where(is.character))\n\n# A tibble: 336,776 × 4\n   carrier tailnum origin dest \n   &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;  &lt;chr&gt;\n 1 UA      N14228  EWR    IAH  \n 2 UA      N24211  LGA    IAH  \n 3 AA      N619AA  JFK    MIA  \n 4 B6      N804JB  JFK    BQN  \n 5 DL      N668DN  LGA    ATL  \n 6 UA      N39463  EWR    ORD  \n 7 B6      N516JB  EWR    FLL  \n 8 EV      N829AS  LGA    IAD  \n 9 B6      N593JB  JFK    MCO  \n10 AA      N3ALAA  LGA    ORD  \n# ℹ 336,766 more rows\n\n\n\n# Rename selected variables\nflights |&gt; \n  select(tail_num = tailnum)\n\n# A tibble: 336,776 × 1\n   tail_num\n   &lt;chr&gt;   \n 1 N14228  \n 2 N24211  \n 3 N619AA  \n 4 N804JB  \n 5 N668DN  \n 6 N39463  \n 7 N516JB  \n 8 N829AS  \n 9 N593JB  \n10 N3ALAA  \n# ℹ 336,766 more rows\n\n\n\n\n3.3.3 rename()\n\nflights |&gt; \n  rename(tail_num = tailnum)\n\n# A tibble: 336,776 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     1     1      517            515         2      830            819\n 2  2013     1     1      533            529         4      850            830\n 3  2013     1     1      542            540         2      923            850\n 4  2013     1     1      544            545        -1     1004           1022\n 5  2013     1     1      554            600        -6      812            837\n 6  2013     1     1      554            558        -4      740            728\n 7  2013     1     1      555            600        -5      913            854\n 8  2013     1     1      557            600        -3      709            723\n 9  2013     1     1      557            600        -3      838            846\n10  2013     1     1      558            600        -2      753            745\n# ℹ 336,766 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tail_num &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\n\n\n3.3.4 relocate()\n\nflights |&gt; \n  relocate(time_hour, air_time)\n\n# A tibble: 336,776 × 19\n   time_hour           air_time  year month   day dep_time sched_dep_time\n   &lt;dttm&gt;                 &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;\n 1 2013-01-01 05:00:00      227  2013     1     1      517            515\n 2 2013-01-01 05:00:00      227  2013     1     1      533            529\n 3 2013-01-01 05:00:00      160  2013     1     1      542            540\n 4 2013-01-01 05:00:00      183  2013     1     1      544            545\n 5 2013-01-01 06:00:00      116  2013     1     1      554            600\n 6 2013-01-01 05:00:00      150  2013     1     1      554            558\n 7 2013-01-01 06:00:00      158  2013     1     1      555            600\n 8 2013-01-01 06:00:00       53  2013     1     1      557            600\n 9 2013-01-01 06:00:00      140  2013     1     1      557            600\n10 2013-01-01 06:00:00      138  2013     1     1      558            600\n# ℹ 336,766 more rows\n# ℹ 12 more variables: dep_delay &lt;dbl&gt;, arr_time &lt;int&gt;, sched_arr_time &lt;int&gt;,\n#   arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;, origin &lt;chr&gt;,\n#   dest &lt;chr&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;\n\n\n\nflights |&gt; \n  relocate(year:dep_time, .after = time_hour)\n\n# A tibble: 336,776 × 19\n   sched_dep_time dep_delay arr_time sched_arr_time arr_delay carrier flight\n            &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt; &lt;chr&gt;    &lt;int&gt;\n 1            515         2      830            819        11 UA        1545\n 2            529         4      850            830        20 UA        1714\n 3            540         2      923            850        33 AA        1141\n 4            545        -1     1004           1022       -18 B6         725\n 5            600        -6      812            837       -25 DL         461\n 6            558        -4      740            728        12 UA        1696\n 7            600        -5      913            854        19 B6         507\n 8            600        -3      709            723       -14 EV        5708\n 9            600        -3      838            846        -8 B6          79\n10            600        -2      753            745         8 AA         301\n# ℹ 336,766 more rows\n# ℹ 12 more variables: tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;,\n#   distance &lt;dbl&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;, year &lt;int&gt;,\n#   month &lt;int&gt;, day &lt;int&gt;, dep_time &lt;int&gt;\n\nflights |&gt; \n  relocate(starts_with(\"arr\"), .before = dep_time)\n\n# A tibble: 336,776 × 19\n    year month   day arr_time arr_delay dep_time sched_dep_time dep_delay\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;\n 1  2013     1     1      830        11      517            515         2\n 2  2013     1     1      850        20      533            529         4\n 3  2013     1     1      923        33      542            540         2\n 4  2013     1     1     1004       -18      544            545        -1\n 5  2013     1     1      812       -25      554            600        -6\n 6  2013     1     1      740        12      554            558        -4\n 7  2013     1     1      913        19      555            600        -5\n 8  2013     1     1      709       -14      557            600        -3\n 9  2013     1     1      838        -8      557            600        -3\n10  2013     1     1      753         8      558            600        -2\n# ℹ 336,766 more rows\n# ℹ 11 more variables: sched_arr_time &lt;int&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\n\n\n3.3.5 Exercises\n\nCompare dep_time, sched_dep_time, and dep_delay. How would you expect those three numbers to be related?\n\nflights %&gt;% \n  relocate(dep_time, sched_dep_time, dep_delay)\n\n# A tibble: 336,776 × 19\n   dep_time sched_dep_time dep_delay  year month   day arr_time sched_arr_time\n      &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;\n 1      517            515         2  2013     1     1      830            819\n 2      533            529         4  2013     1     1      850            830\n 3      542            540         2  2013     1     1      923            850\n 4      544            545        -1  2013     1     1     1004           1022\n 5      554            600        -6  2013     1     1      812            837\n 6      554            558        -4  2013     1     1      740            728\n 7      555            600        -5  2013     1     1      913            854\n 8      557            600        -3  2013     1     1      709            723\n 9      557            600        -3  2013     1     1      838            846\n10      558            600        -2  2013     1     1      753            745\n# ℹ 336,766 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\nIt is probably the difference between those two values (dep_time - sched_dep_time).\n\n# This gives out wrong answers to some due to the formatting.\nflights %&gt;% \n  relocate(dep_time, sched_dep_time, dep_delay) %&gt;% \n  mutate(dep_delay_check = dep_time - sched_dep_time, .after = dep_delay)\n\n# A tibble: 336,776 × 20\n   dep_time sched_dep_time dep_delay dep_delay_check  year month   day arr_time\n      &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;           &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;\n 1      517            515         2               2  2013     1     1      830\n 2      533            529         4               4  2013     1     1      850\n 3      542            540         2               2  2013     1     1      923\n 4      544            545        -1              -1  2013     1     1     1004\n 5      554            600        -6             -46  2013     1     1      812\n 6      554            558        -4              -4  2013     1     1      740\n 7      555            600        -5             -45  2013     1     1      913\n 8      557            600        -3             -43  2013     1     1      709\n 9      557            600        -3             -43  2013     1     1      838\n10      558            600        -2             -42  2013     1     1      753\n# ℹ 336,766 more rows\n# ℹ 12 more variables: sched_arr_time &lt;int&gt;, arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;,\n#   flight &lt;int&gt;, tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;,\n#   distance &lt;dbl&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\nBrainstorm as many ways as possible to select dep_time, dep_delay, arr_time, and arr_delay from flights.\n\nglimpse(flights)\n\nRows: 336,776\nColumns: 19\n$ year           &lt;int&gt; 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2…\n$ month          &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ day            &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ dep_time       &lt;int&gt; 517, 533, 542, 544, 554, 554, 555, 557, 557, 558, 558, …\n$ sched_dep_time &lt;int&gt; 515, 529, 540, 545, 600, 558, 600, 600, 600, 600, 600, …\n$ dep_delay      &lt;dbl&gt; 2, 4, 2, -1, -6, -4, -5, -3, -3, -2, -2, -2, -2, -2, -1…\n$ arr_time       &lt;int&gt; 830, 850, 923, 1004, 812, 740, 913, 709, 838, 753, 849,…\n$ sched_arr_time &lt;int&gt; 819, 830, 850, 1022, 837, 728, 854, 723, 846, 745, 851,…\n$ arr_delay      &lt;dbl&gt; 11, 20, 33, -18, -25, 12, 19, -14, -8, 8, -2, -3, 7, -1…\n$ carrier        &lt;chr&gt; \"UA\", \"UA\", \"AA\", \"B6\", \"DL\", \"UA\", \"B6\", \"EV\", \"B6\", \"…\n$ flight         &lt;int&gt; 1545, 1714, 1141, 725, 461, 1696, 507, 5708, 79, 301, 4…\n$ tailnum        &lt;chr&gt; \"N14228\", \"N24211\", \"N619AA\", \"N804JB\", \"N668DN\", \"N394…\n$ origin         &lt;chr&gt; \"EWR\", \"LGA\", \"JFK\", \"JFK\", \"LGA\", \"EWR\", \"EWR\", \"LGA\",…\n$ dest           &lt;chr&gt; \"IAH\", \"IAH\", \"MIA\", \"BQN\", \"ATL\", \"ORD\", \"FLL\", \"IAD\",…\n$ air_time       &lt;dbl&gt; 227, 227, 160, 183, 116, 150, 158, 53, 140, 138, 149, 1…\n$ distance       &lt;dbl&gt; 1400, 1416, 1089, 1576, 762, 719, 1065, 229, 944, 733, …\n$ hour           &lt;dbl&gt; 5, 5, 5, 5, 6, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 5, 6, 6, 6…\n$ minute         &lt;dbl&gt; 15, 29, 40, 45, 0, 58, 0, 0, 0, 0, 0, 0, 0, 0, 0, 59, 0…\n$ time_hour      &lt;dttm&gt; 2013-01-01 05:00:00, 2013-01-01 05:00:00, 2013-01-01 0…\n\n\n\nflights %&gt;% \n  select(dep_time, dep_delay, arr_time, arr_delay)\n\n# A tibble: 336,776 × 4\n   dep_time dep_delay arr_time arr_delay\n      &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;     &lt;dbl&gt;\n 1      517         2      830        11\n 2      533         4      850        20\n 3      542         2      923        33\n 4      544        -1     1004       -18\n 5      554        -6      812       -25\n 6      554        -4      740        12\n 7      555        -5      913        19\n 8      557        -3      709       -14\n 9      557        -3      838        -8\n10      558        -2      753         8\n# ℹ 336,766 more rows\n\nflights %&gt;% \n  select(starts_with(c(\"dep_\", \"arr_\")))\n\n# A tibble: 336,776 × 4\n   dep_time dep_delay arr_time arr_delay\n      &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;     &lt;dbl&gt;\n 1      517         2      830        11\n 2      533         4      850        20\n 3      542         2      923        33\n 4      544        -1     1004       -18\n 5      554        -6      812       -25\n 6      554        -4      740        12\n 7      555        -5      913        19\n 8      557        -3      709       -14\n 9      557        -3      838        -8\n10      558        -2      753         8\n# ℹ 336,766 more rows\n\n# flights |&gt;\n#   select(dep_time:arr_delay, -contains(\"sched\"))\n\nWhat happens if you specify the name of the same variable multiple times in a select() call?\n\nflights %&gt;% \n  select(year, year)\n\n# A tibble: 336,776 × 1\n    year\n   &lt;int&gt;\n 1  2013\n 2  2013\n 3  2013\n 4  2013\n 5  2013\n 6  2013\n 7  2013\n 8  2013\n 9  2013\n10  2013\n# ℹ 336,766 more rows\n\n\nIt returns only one copy of that column.\nWhat does the any_of() function do? Why might it be helpful in conjunction with this vector?\n\nvariables &lt;- c(\"year\", \"month\", \"day\", \"dep_delay\", \"arr_delay\")\n\n\nflights %&gt;% \n  select(any_of(variables))\n\n# A tibble: 336,776 × 5\n    year month   day dep_delay arr_delay\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n 1  2013     1     1         2        11\n 2  2013     1     1         4        20\n 3  2013     1     1         2        33\n 4  2013     1     1        -1       -18\n 5  2013     1     1        -6       -25\n 6  2013     1     1        -4        12\n 7  2013     1     1        -5        19\n 8  2013     1     1        -3       -14\n 9  2013     1     1        -3        -8\n10  2013     1     1        -2         8\n# ℹ 336,766 more rows\n\n\nIt helps specify an array of variables, but more importantly, it allows you to specify columns that may or may not exist.\nDoes the result of running the following code surprise you? How do the select helpers deal with upper and lower case by default? How can you change that default?\n\nflights |&gt; select(contains(\"TIME\"))\n\n# A tibble: 336,776 × 6\n   dep_time sched_dep_time arr_time sched_arr_time air_time time_hour          \n      &lt;int&gt;          &lt;int&gt;    &lt;int&gt;          &lt;int&gt;    &lt;dbl&gt; &lt;dttm&gt;             \n 1      517            515      830            819      227 2013-01-01 05:00:00\n 2      533            529      850            830      227 2013-01-01 05:00:00\n 3      542            540      923            850      160 2013-01-01 05:00:00\n 4      544            545     1004           1022      183 2013-01-01 05:00:00\n 5      554            600      812            837      116 2013-01-01 06:00:00\n 6      554            558      740            728      150 2013-01-01 05:00:00\n 7      555            600      913            854      158 2013-01-01 06:00:00\n 8      557            600      709            723       53 2013-01-01 06:00:00\n 9      557            600      838            846      140 2013-01-01 06:00:00\n10      558            600      753            745      138 2013-01-01 06:00:00\n# ℹ 336,766 more rows\n\n\nI expected it to be case-sensitive, but it is not by default. You can make it case-sensitive by setting the argument ignore.case to FALSE.\nRename air_time to air_time_min to indicate units of measurement and move it to the beginning of the data frame.\n\nflights %&gt;% \n  mutate(air_time_min = air_time, .before = 1)\n\n# A tibble: 336,776 × 20\n   air_time_min  year month   day dep_time sched_dep_time dep_delay arr_time\n          &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;\n 1          227  2013     1     1      517            515         2      830\n 2          227  2013     1     1      533            529         4      850\n 3          160  2013     1     1      542            540         2      923\n 4          183  2013     1     1      544            545        -1     1004\n 5          116  2013     1     1      554            600        -6      812\n 6          150  2013     1     1      554            558        -4      740\n 7          158  2013     1     1      555            600        -5      913\n 8           53  2013     1     1      557            600        -3      709\n 9          140  2013     1     1      557            600        -3      838\n10          138  2013     1     1      558            600        -2      753\n# ℹ 336,766 more rows\n# ℹ 12 more variables: sched_arr_time &lt;int&gt;, arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;,\n#   flight &lt;int&gt;, tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;,\n#   distance &lt;dbl&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\nWhy doesn’t the following work, and what does the error mean?\n\nflights |&gt; \n  select(tailnum) |&gt; \n  arrange(arr_delay)\n\nError in `arrange()`:\nℹ In argument: `..1 = arr_delay`.\nCaused by error:\n! object 'arr_delay' not found\n\n\nThe select function returns a new tibble that only contains the column tailnum, so when it is passed on to the next function, arr_delay, it can no longer find the column arr_delay.",
    "crumbs": [
      "R for Data Science: Outputs",
      "Whole Game",
      "3 Data Transformation"
    ]
  },
  {
    "objectID": "r4ds_outputs/chapter_3.html#the-pipe",
    "href": "r4ds_outputs/chapter_3.html#the-pipe",
    "title": "3 Data Transformation",
    "section": "3.4 The pipe",
    "text": "3.4 The pipe\n\nflights |&gt; \n  filter(dest == \"IAH\") |&gt; \n  mutate(speed = distance / air_time * 60) |&gt; \n  select(year:day, dep_time, carrier, flight, speed) |&gt; \n  arrange(desc(speed))\n\n# A tibble: 7,198 × 7\n    year month   day dep_time carrier flight speed\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt; &lt;chr&gt;    &lt;int&gt; &lt;dbl&gt;\n 1  2013     7     9      707 UA         226  522.\n 2  2013     8    27     1850 UA        1128  521.\n 3  2013     8    28      902 UA        1711  519.\n 4  2013     8    28     2122 UA        1022  519.\n 5  2013     6    11     1628 UA        1178  515.\n 6  2013     8    27     1017 UA         333  515.\n 7  2013     8    27     1205 UA        1421  515.\n 8  2013     8    27     1758 UA         302  515.\n 9  2013     9    27      521 UA         252  515.\n10  2013     8    28      625 UA         559  515.\n# ℹ 7,188 more rows\n\n\n\narrange(\n  select(\n    mutate(\n      filter(\n        flights, \n        dest == \"IAH\"\n      ),\n      speed = distance / air_time * 60\n    ),\n    year:day, dep_time, carrier, flight, speed\n  ),\n  desc(speed)\n)\n\n# A tibble: 7,198 × 7\n    year month   day dep_time carrier flight speed\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt; &lt;chr&gt;    &lt;int&gt; &lt;dbl&gt;\n 1  2013     7     9      707 UA         226  522.\n 2  2013     8    27     1850 UA        1128  521.\n 3  2013     8    28      902 UA        1711  519.\n 4  2013     8    28     2122 UA        1022  519.\n 5  2013     6    11     1628 UA        1178  515.\n 6  2013     8    27     1017 UA         333  515.\n 7  2013     8    27     1205 UA        1421  515.\n 8  2013     8    27     1758 UA         302  515.\n 9  2013     9    27      521 UA         252  515.\n10  2013     8    28      625 UA         559  515.\n# ℹ 7,188 more rows\n\n\n\nflights1 &lt;- filter(flights, dest == \"IAH\")\nflights2 &lt;- mutate(flights1, speed = distance / air_time * 60)\nflights3 &lt;- select(flights2, year:day, dep_time, carrier, flight, speed)\narrange(flights3, desc(speed))\n\n# A tibble: 7,198 × 7\n    year month   day dep_time carrier flight speed\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt; &lt;chr&gt;    &lt;int&gt; &lt;dbl&gt;\n 1  2013     7     9      707 UA         226  522.\n 2  2013     8    27     1850 UA        1128  521.\n 3  2013     8    28      902 UA        1711  519.\n 4  2013     8    28     2122 UA        1022  519.\n 5  2013     6    11     1628 UA        1178  515.\n 6  2013     8    27     1017 UA         333  515.\n 7  2013     8    27     1205 UA        1421  515.\n 8  2013     8    27     1758 UA         302  515.\n 9  2013     9    27      521 UA         252  515.\n10  2013     8    28      625 UA         559  515.\n# ℹ 7,188 more rows",
    "crumbs": [
      "R for Data Science: Outputs",
      "Whole Game",
      "3 Data Transformation"
    ]
  },
  {
    "objectID": "r4ds_outputs/chapter_3.html#groups",
    "href": "r4ds_outputs/chapter_3.html#groups",
    "title": "3 Data Transformation",
    "section": "3.5 Groups",
    "text": "3.5 Groups\n\n3.5.1 group_by()\n\nflights |&gt; \n  group_by(month)\n\n# A tibble: 336,776 × 19\n# Groups:   month [12]\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     1     1      517            515         2      830            819\n 2  2013     1     1      533            529         4      850            830\n 3  2013     1     1      542            540         2      923            850\n 4  2013     1     1      544            545        -1     1004           1022\n 5  2013     1     1      554            600        -6      812            837\n 6  2013     1     1      554            558        -4      740            728\n 7  2013     1     1      555            600        -5      913            854\n 8  2013     1     1      557            600        -3      709            723\n 9  2013     1     1      557            600        -3      838            846\n10  2013     1     1      558            600        -2      753            745\n# ℹ 336,766 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\n\n\n3.5.2 summarize()\n\nflights |&gt; \n  group_by(month) |&gt; \n  summarize(\n    avg_delay = mean(dep_delay)\n  )\n\n# A tibble: 12 × 2\n   month avg_delay\n   &lt;int&gt;     &lt;dbl&gt;\n 1     1        NA\n 2     2        NA\n 3     3        NA\n 4     4        NA\n 5     5        NA\n 6     6        NA\n 7     7        NA\n 8     8        NA\n 9     9        NA\n10    10        NA\n11    11        NA\n12    12        NA\n\nflights |&gt; \n  group_by(month) |&gt; \n  summarize(\n    avg_delay = mean(dep_delay, na.rm = TRUE)\n  )\n\n# A tibble: 12 × 2\n   month avg_delay\n   &lt;int&gt;     &lt;dbl&gt;\n 1     1     10.0 \n 2     2     10.8 \n 3     3     13.2 \n 4     4     13.9 \n 5     5     13.0 \n 6     6     20.8 \n 7     7     21.7 \n 8     8     12.6 \n 9     9      6.72\n10    10      6.24\n11    11      5.44\n12    12     16.6 \n\n\n\nflights |&gt; \n  group_by(month) |&gt; \n  summarize(\n    avg_delay = mean(dep_delay, na.rm = TRUE), \n    n = n()\n  )\n\n# A tibble: 12 × 3\n   month avg_delay     n\n   &lt;int&gt;     &lt;dbl&gt; &lt;int&gt;\n 1     1     10.0  27004\n 2     2     10.8  24951\n 3     3     13.2  28834\n 4     4     13.9  28330\n 5     5     13.0  28796\n 6     6     20.8  28243\n 7     7     21.7  29425\n 8     8     12.6  29327\n 9     9      6.72 27574\n10    10      6.24 28889\n11    11      5.44 27268\n12    12     16.6  28135\n\n\n\n\n3.5.3 The slice_ functions\n\n# Include all rows with ties not one\nflights |&gt; \n  group_by(dest) |&gt; \n  slice_max(arr_delay, n = 1) |&gt;\n  relocate(dest)\n\n# A tibble: 108 × 19\n# Groups:   dest [105]\n   dest   year month   day dep_time sched_dep_time dep_delay arr_time\n   &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;\n 1 ABQ    2013     7    22     2145           2007        98      132\n 2 ACK    2013     7    23     1139            800       219     1250\n 3 ALB    2013     1    25      123           2000       323      229\n 4 ANC    2013     8    17     1740           1625        75     2042\n 5 ATL    2013     7    22     2257            759       898      121\n 6 AUS    2013     7    10     2056           1505       351     2347\n 7 AVL    2013     8    13     1156            832       204     1417\n 8 BDL    2013     2    21     1728           1316       252     1839\n 9 BGR    2013    12     1     1504           1056       248     1628\n10 BHM    2013     4    10       25           1900       325      136\n# ℹ 98 more rows\n# ℹ 11 more variables: sched_arr_time &lt;int&gt;, arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;,\n#   flight &lt;int&gt;, tailnum &lt;chr&gt;, origin &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\n\n\n3.5.4 Grouping by multiple variables\n\ndaily &lt;- flights |&gt;  \n  group_by(year, month, day)\ndaily\n\n# A tibble: 336,776 × 19\n# Groups:   year, month, day [365]\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     1     1      517            515         2      830            819\n 2  2013     1     1      533            529         4      850            830\n 3  2013     1     1      542            540         2      923            850\n 4  2013     1     1      544            545        -1     1004           1022\n 5  2013     1     1      554            600        -6      812            837\n 6  2013     1     1      554            558        -4      740            728\n 7  2013     1     1      555            600        -5      913            854\n 8  2013     1     1      557            600        -3      709            723\n 9  2013     1     1      557            600        -3      838            846\n10  2013     1     1      558            600        -2      753            745\n# ℹ 336,766 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\n\ndaily_flights &lt;- daily |&gt; \n  summarize(n = n())\n\n`summarise()` has grouped output by 'year', 'month'. You can override using the\n`.groups` argument.\n\ndaily_flights\n\n# A tibble: 365 × 4\n# Groups:   year, month [12]\n    year month   day     n\n   &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;\n 1  2013     1     1   842\n 2  2013     1     2   943\n 3  2013     1     3   914\n 4  2013     1     4   915\n 5  2013     1     5   720\n 6  2013     1     6   832\n 7  2013     1     7   933\n 8  2013     1     8   899\n 9  2013     1     9   902\n10  2013     1    10   932\n# ℹ 355 more rows\n\n\n\ndaily_flights &lt;- daily |&gt; \n  summarize(\n    n = n(), \n    .groups = \"drop_last\"\n  )\ndaily_flights\n\n# A tibble: 365 × 4\n# Groups:   year, month [12]\n    year month   day     n\n   &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;\n 1  2013     1     1   842\n 2  2013     1     2   943\n 3  2013     1     3   914\n 4  2013     1     4   915\n 5  2013     1     5   720\n 6  2013     1     6   832\n 7  2013     1     7   933\n 8  2013     1     8   899\n 9  2013     1     9   902\n10  2013     1    10   932\n# ℹ 355 more rows\n\n\n\n\n3.5.5 Ungrouping\n\ndaily |&gt; \n  ungroup()\n\n# A tibble: 336,776 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     1     1      517            515         2      830            819\n 2  2013     1     1      533            529         4      850            830\n 3  2013     1     1      542            540         2      923            850\n 4  2013     1     1      544            545        -1     1004           1022\n 5  2013     1     1      554            600        -6      812            837\n 6  2013     1     1      554            558        -4      740            728\n 7  2013     1     1      555            600        -5      913            854\n 8  2013     1     1      557            600        -3      709            723\n 9  2013     1     1      557            600        -3      838            846\n10  2013     1     1      558            600        -2      753            745\n# ℹ 336,766 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\n\ndaily |&gt; \n  ungroup() |&gt;\n  summarize(\n    avg_delay = mean(dep_delay, na.rm = TRUE), \n    flights = n()\n  )\n\n# A tibble: 1 × 2\n  avg_delay flights\n      &lt;dbl&gt;   &lt;int&gt;\n1      12.6  336776\n\n\n\n\n3.5.6 .by\n\nflights |&gt; \n  summarize(\n    delay = mean(dep_delay, na.rm = TRUE), \n    n = n(),\n    .by = month\n  )\n\n# A tibble: 12 × 3\n   month delay     n\n   &lt;int&gt; &lt;dbl&gt; &lt;int&gt;\n 1     1 10.0  27004\n 2    10  6.24 28889\n 3    11  5.44 27268\n 4    12 16.6  28135\n 5     2 10.8  24951\n 6     3 13.2  28834\n 7     4 13.9  28330\n 8     5 13.0  28796\n 9     6 20.8  28243\n10     7 21.7  29425\n11     8 12.6  29327\n12     9  6.72 27574\n\n\n\nflights |&gt; \n  summarize(\n    delay = mean(dep_delay, na.rm = TRUE), \n    n = n(),\n    .by = c(origin, dest)\n  )\n\n# A tibble: 224 × 4\n   origin dest  delay     n\n   &lt;chr&gt;  &lt;chr&gt; &lt;dbl&gt; &lt;int&gt;\n 1 EWR    IAH   11.8   3973\n 2 LGA    IAH    9.06  2951\n 3 JFK    MIA    9.34  3314\n 4 JFK    BQN    6.67   599\n 5 LGA    ATL   11.4  10263\n 6 EWR    ORD   14.6   6100\n 7 EWR    FLL   13.5   3793\n 8 LGA    IAD   16.7   1803\n 9 JFK    MCO   10.6   5464\n10 LGA    ORD   10.7   8857\n# ℹ 214 more rows\n\n\n\n\n3.5.7 Exercises\n\nWhich carrier has the worst average delays? Challenge: can you disentangle the effects of bad airports vs. bad carriers? Why/why not? (Hint: think about flights |&gt; group_by(carrier, dest) |&gt; summarize(n()))\n\nflights |&gt; \n  summarize(avg_delay = mean(dep_delay, na.rm = TRUE), .by = carrier) |&gt; \n  arrange(desc(avg_delay))\n\n# A tibble: 16 × 2\n   carrier avg_delay\n   &lt;chr&gt;       &lt;dbl&gt;\n 1 F9          20.2 \n 2 EV          20.0 \n 3 YV          19.0 \n 4 FL          18.7 \n 5 WN          17.7 \n 6 9E          16.7 \n 7 B6          13.0 \n 8 VX          12.9 \n 9 OO          12.6 \n10 UA          12.1 \n11 MQ          10.6 \n12 DL           9.26\n13 AA           8.59\n14 AS           5.80\n15 HA           4.90\n16 US           3.78\n\n\nFrontier Airlines has the worst average delay.\n\nflights |&gt; \n  summarize(avg_delay = mean(dep_delay, na.rm = TRUE), .by = c(carrier, dest)) |&gt; \n  arrange(desc(avg_delay))\n\n# A tibble: 314 × 3\n   carrier dest  avg_delay\n   &lt;chr&gt;   &lt;chr&gt;     &lt;dbl&gt;\n 1 UA      STL        77.5\n 2 OO      ORD        67  \n 3 OO      DTW        61  \n 4 UA      RDU        60  \n 5 EV      PBI        48.7\n 6 EV      TYS        41.8\n 7 EV      CAE        36.7\n 8 EV      TUL        34.9\n 9 9E      BGR        34  \n10 WN      MSY        33.4\n# ℹ 304 more rows\n\n\nYou can specify the worst delay by different groupings, but I don’t think there is a way to disentangle the effects.\nFind the flights that are most delayed upon departure from each destination.\n\nflights |&gt; \n  slice_max(dep_delay, n = 1, by = dest) |&gt; \n  arrange(dest) |&gt; \n  relocate(dest, dep_delay)\n\n# A tibble: 105 × 19\n   dest  dep_delay  year month   day dep_time sched_dep_time arr_time\n   &lt;chr&gt;     &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;    &lt;int&gt;\n 1 ABQ         142  2013    12    14     2223           2001      133\n 2 ACK         219  2013     7    23     1139            800     1250\n 3 ALB         323  2013     1    25      123           2000      229\n 4 ANC          75  2013     8    17     1740           1625     2042\n 5 ATL         898  2013     7    22     2257            759      121\n 6 AUS         351  2013     7    10     2056           1505     2347\n 7 AVL         222  2013     6    14     1158            816     1335\n 8 BDL         252  2013     2    21     1728           1316     1839\n 9 BGR         248  2013    12     1     1504           1056     1628\n10 BHM         325  2013     4    10       25           1900      136\n# ℹ 95 more rows\n# ℹ 11 more variables: sched_arr_time &lt;int&gt;, arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;,\n#   flight &lt;int&gt;, tailnum &lt;chr&gt;, origin &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\nHow do delays vary over the course of the day. Illustrate your answer with a plot.\n\nflights |&gt; \n  ggplot(aes(x = hour, y = dep_delay)) +\n  geom_smooth()\n\n`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n\n\nWarning: Removed 8255 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\n\n\n\n\n\n\n\n\nflights |&gt; \n  summarize(avg_delay = mean(dep_delay, na.rm = TRUE), .by = hour) |&gt; \n  ggplot(aes(x = hour, y = avg_delay)) +\n  geom_smooth()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\nWarning: Removed 1 row containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\n\n\n\n\n\n\n\nFrom the earliest hour, the delay is increasing and peaks at about 7 pm. Thereafter, the delay decreases.\nWhat happens if you supply a negative n to slice_min() and friends?\n\nflights |&gt; \n  slice_max(dep_delay, n = -3, by = dest) |&gt; \n  relocate(dest, dep_delay)\n\n# A tibble: 336,728 × 19\n   dest  dep_delay  year month   day dep_time sched_dep_time arr_time\n   &lt;chr&gt;     &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;    &lt;int&gt;\n 1 IAH         761  2013     4    19      606           1725      923\n 2 IAH         427  2013     4    18     2200           1453      106\n 3 IAH         405  2013     6    27     2348           1703      224\n 4 IAH         397  2013     9    12     2157           1520       23\n 5 IAH         390  2013     3    25     1530            900     1824\n 6 IAH         387  2013     7    25     2147           1520        8\n 7 IAH         361  2013     9     2     2301           1700      157\n 8 IAH         348  2013     7    28       48           1900      315\n 9 IAH         331  2013     7    10       40           1909      301\n10 IAH         310  2013     9    25     1706           1156     1935\n# ℹ 336,718 more rows\n# ℹ 11 more variables: sched_arr_time &lt;int&gt;, arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;,\n#   flight &lt;int&gt;, tailnum &lt;chr&gt;, origin &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\nflights |&gt; \n  slice_max(dep_delay, n = 3, by = dest) |&gt; \n  relocate(dest, dep_delay)\n\n# A tibble: 314 × 19\n   dest  dep_delay  year month   day dep_time sched_dep_time arr_time\n   &lt;chr&gt;     &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;    &lt;int&gt;\n 1 IAH         761  2013     4    19      606           1725      923\n 2 IAH         427  2013     4    18     2200           1453      106\n 3 IAH         405  2013     6    27     2348           1703      224\n 4 MIA         896  2013    12     5      756           1700     1058\n 5 MIA         845  2013    12    17      705           1700     1026\n 6 MIA         660  2013    12    15      625           1925      933\n 7 BQN         220  2013     7    19       30           2050      407\n 8 BQN         216  2013     4    17       14           2038      339\n 9 BQN         212  2013     6    13        6           2034      358\n10 ATL         898  2013     7    22     2257            759      121\n# ℹ 304 more rows\n# ℹ 11 more variables: sched_arr_time &lt;int&gt;, arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;,\n#   flight &lt;int&gt;, tailnum &lt;chr&gt;, origin &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\nIt does not return abs(n) observations for each group.\nExplain what count() does in terms of the dplyr verbs you just learned. What does the sort argument to count() do?\nIt counts unique values for one or more variables. In the documentation, it states: df %&gt;% count(a, b) is roughly equivalent to df %&gt;% group_by(a, b) %&gt;% summarise(n = n()). count(). If sort = TRUE, the highest number will be sorted from highest to lowest.\nSuppose we have the following tiny data frame:\n\ndf &lt;- tibble(\n  x = 1:5,\n  y = c(\"a\", \"b\", \"a\", \"a\", \"b\"),\n  z = c(\"K\", \"K\", \"L\", \"L\", \"K\")\n)\ndf\n\n# A tibble: 5 × 3\n      x y     z    \n  &lt;int&gt; &lt;chr&gt; &lt;chr&gt;\n1     1 a     K    \n2     2 b     K    \n3     3 a     L    \n4     4 a     L    \n5     5 b     K    \n\n\n\nabcdef\n\n\nWrite down what you think the output will look like, then check if you were correct, and describe what group_by() does.\nIt will return the same data frame, but with grouping specified.\n\ndf |&gt;\n  group_by(y)\n\n# A tibble: 5 × 3\n# Groups:   y [2]\n      x y     z    \n  &lt;int&gt; &lt;chr&gt; &lt;chr&gt;\n1     1 a     K    \n2     2 b     K    \n3     3 a     L    \n4     4 a     L    \n5     5 b     K    \n\n\n\n\nWrite down what you think the output will look like, then check if you were correct, and describe what arrange() does. Also comment on how it’s different from the group_by() in part (a).\nIt will arrange the data frame by the values in column y in ascending order.\n\ndf |&gt;\n  arrange(y)\n\n# A tibble: 5 × 3\n      x y     z    \n  &lt;int&gt; &lt;chr&gt; &lt;chr&gt;\n1     1 a     K    \n2     3 a     L    \n3     4 a     L    \n4     2 b     K    \n5     5 b     K    \n\n\n\n\nWrite down what you think the output will look like, then check if you were correct, and describe what the pipeline does.\nThe data frame will be grouped by the values in y. For each of those group, the mean value of x will be calculated.\n\ndf |&gt;\n  group_by(y) |&gt;\n  summarize(mean_x = mean(x))\n\n# A tibble: 2 × 2\n  y     mean_x\n  &lt;chr&gt;  &lt;dbl&gt;\n1 a       2.67\n2 b       3.5 \n\n\n\n\nWrite down what you think the output will look like, then check if you were correct, and describe what the pipeline does. Then, comment on what the message says.\nThe data frame will grouped by the values of y then z.For each group, the mean values of x will be calculated. You will receive a message regarding the grouping.\n\ndf |&gt;\n  group_by(y, z) |&gt;\n  summarize(mean_x = mean(x))\n\n`summarise()` has grouped output by 'y'. You can override using the `.groups`\nargument.\n\n\n# A tibble: 3 × 3\n# Groups:   y [2]\n  y     z     mean_x\n  &lt;chr&gt; &lt;chr&gt;  &lt;dbl&gt;\n1 a     K        1  \n2 a     L        3.5\n3 b     K        3.5\n\n\n\n\nWrite down what you think the output will look like, then check if you were correct, and describe what the pipeline does. How is the output different from the one in part (d)?\nThe data frame will grouped by the values of y then z. For each group, the mean values of x will be calculated. Then, the grouping will be dropped.\n\ndf |&gt;\n  group_by(y, z) |&gt;\n  summarize(mean_x = mean(x), .groups = \"drop\")\n\n# A tibble: 3 × 3\n  y     z     mean_x\n  &lt;chr&gt; &lt;chr&gt;  &lt;dbl&gt;\n1 a     K        1  \n2 a     L        3.5\n3 b     K        3.5\n\n\n\n\nWrite down what you think the outputs will look like, then check if you were correct, and describe what each pipeline does. How are the outputs of the two pipelines different?\nThe data frame will grouped by the values of y then z. For each group, the mean values of x will be calculated. For the first one, only the group names and the mean_x variable will be displayed, but for the second one, all variables will be retained.1\n\ndf |&gt;\n  group_by(y, z) |&gt;\n  summarize(mean_x = mean(x))\n\n`summarise()` has grouped output by 'y'. You can override using the `.groups`\nargument.\n\n\n# A tibble: 3 × 3\n# Groups:   y [2]\n  y     z     mean_x\n  &lt;chr&gt; &lt;chr&gt;  &lt;dbl&gt;\n1 a     K        1  \n2 a     L        3.5\n3 b     K        3.5\n\ndf |&gt;\n  group_by(y, z) |&gt;\n  mutate(mean_x = mean(x))\n\n# A tibble: 5 × 4\n# Groups:   y, z [3]\n      x y     z     mean_x\n  &lt;int&gt; &lt;chr&gt; &lt;chr&gt;  &lt;dbl&gt;\n1     1 a     K        1  \n2     2 b     K        3.5\n3     3 a     L        3.5\n4     4 a     L        3.5\n5     5 b     K        3.5",
    "crumbs": [
      "R for Data Science: Outputs",
      "Whole Game",
      "3 Data Transformation"
    ]
  },
  {
    "objectID": "r4ds_outputs/chapter_3.html#case-study-aggregates-and-sample-size",
    "href": "r4ds_outputs/chapter_3.html#case-study-aggregates-and-sample-size",
    "title": "3 Data Transformation",
    "section": "3.6 Case study: aggregates and sample size",
    "text": "3.6 Case study: aggregates and sample size\n\nbatters &lt;- Lahman::Batting |&gt; \n  group_by(playerID) |&gt; \n  summarize(\n    performance = sum(H, na.rm = TRUE) / sum(AB, na.rm = TRUE),\n    n = sum(AB, na.rm = TRUE)\n  )\nbatters\n\n# A tibble: 20,469 × 3\n   playerID  performance     n\n   &lt;chr&gt;           &lt;dbl&gt; &lt;int&gt;\n 1 aardsda01      0          4\n 2 aaronha01      0.305  12364\n 3 aaronto01      0.229    944\n 4 aasedo01       0          5\n 5 abadan01       0.0952    21\n 6 abadfe01       0.111      9\n 7 abadijo01      0.224     49\n 8 abbated01      0.254   3044\n 9 abbeybe01      0.169    225\n10 abbeych01      0.281   1756\n# ℹ 20,459 more rows\n\n\n\nbatters |&gt; \n  filter(n &gt; 100) |&gt; \n  ggplot(aes(x = n, y = performance)) +\n  geom_point(alpha = 1 / 10) + \n  geom_smooth(se = FALSE)\n\n`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n\n\n\n\n\n\n\n\n\n\nbatters |&gt; \n  arrange(desc(performance))\n\n# A tibble: 20,469 × 3\n   playerID  performance     n\n   &lt;chr&gt;           &lt;dbl&gt; &lt;int&gt;\n 1 abramge01           1     1\n 2 alberan01           1     1\n 3 banisje01           1     1\n 4 bartocl01           1     1\n 5 bassdo01            1     1\n 6 birasst01           1     2\n 7 bruneju01           1     1\n 8 burnscb01           1     1\n 9 cammaer01           1     1\n10 campsh01            1     1\n# ℹ 20,459 more rows",
    "crumbs": [
      "R for Data Science: Outputs",
      "Whole Game",
      "3 Data Transformation"
    ]
  },
  {
    "objectID": "r4ds_outputs/chapter_5.html",
    "href": "r4ds_outputs/chapter_5.html",
    "title": "5 Data tidying",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors",
    "crumbs": [
      "R for Data Science: Outputs",
      "Whole Game",
      "5 Data tidying"
    ]
  },
  {
    "objectID": "r4ds_outputs/chapter_5.html#introduction",
    "href": "r4ds_outputs/chapter_5.html#introduction",
    "title": "5 Data tidying",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors",
    "crumbs": [
      "R for Data Science: Outputs",
      "Whole Game",
      "5 Data tidying"
    ]
  },
  {
    "objectID": "r4ds_outputs/chapter_5.html#tidy-data",
    "href": "r4ds_outputs/chapter_5.html#tidy-data",
    "title": "5 Data tidying",
    "section": "5.2 Tidy data",
    "text": "5.2 Tidy data\n\ntable1\n\n# A tibble: 6 × 4\n  country      year  cases population\n  &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;\n1 Afghanistan  1999    745   19987071\n2 Afghanistan  2000   2666   20595360\n3 Brazil       1999  37737  172006362\n4 Brazil       2000  80488  174504898\n5 China        1999 212258 1272915272\n6 China        2000 213766 1280428583\n\n\n\ntable2\n\n# A tibble: 12 × 4\n   country      year type            count\n   &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;           &lt;dbl&gt;\n 1 Afghanistan  1999 cases             745\n 2 Afghanistan  1999 population   19987071\n 3 Afghanistan  2000 cases            2666\n 4 Afghanistan  2000 population   20595360\n 5 Brazil       1999 cases           37737\n 6 Brazil       1999 population  172006362\n 7 Brazil       2000 cases           80488\n 8 Brazil       2000 population  174504898\n 9 China        1999 cases          212258\n10 China        1999 population 1272915272\n11 China        2000 cases          213766\n12 China        2000 population 1280428583\n\n\n\ntable3\n\n# A tibble: 6 × 3\n  country      year rate             \n  &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;            \n1 Afghanistan  1999 745/19987071     \n2 Afghanistan  2000 2666/20595360    \n3 Brazil       1999 37737/172006362  \n4 Brazil       2000 80488/174504898  \n5 China        1999 212258/1272915272\n6 China        2000 213766/1280428583\n\n\n\n# Compute rate per 10,000\ntable1 |&gt;\n  mutate(rate = cases / population * 10000)\n\n# A tibble: 6 × 5\n  country      year  cases population  rate\n  &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt; &lt;dbl&gt;\n1 Afghanistan  1999    745   19987071 0.373\n2 Afghanistan  2000   2666   20595360 1.29 \n3 Brazil       1999  37737  172006362 2.19 \n4 Brazil       2000  80488  174504898 4.61 \n5 China        1999 212258 1272915272 1.67 \n6 China        2000 213766 1280428583 1.67 \n\n\n\n# Compute total cases per year\ntable1 |&gt; \n  group_by(year) |&gt; \n  summarize(total_cases = sum(cases))\n\n# A tibble: 2 × 2\n   year total_cases\n  &lt;dbl&gt;       &lt;dbl&gt;\n1  1999      250740\n2  2000      296920\n\n\n\n# Visualize changes over time\nggplot(table1, aes(x = year, y = cases)) +\n  geom_line(aes(group = country), color = \"grey50\") +\n  geom_point(aes(color = country, shape = country)) +\n  scale_x_continuous(breaks = c(1999, 2000)) # x-axis breaks at 1999 and 2000\n\n\n\n\n\n\n\n\n\n5.2.1 Exercises\n\nFor each of the sample tables, describe what each observation and each column represents.\n\ntable1table2table 3\n\n\nThere are four columns, one for each of the following variable: country, year, cases, and population. Each observation is represented by one row.\n\ntable1\n\n# A tibble: 6 × 4\n  country      year  cases population\n  &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;\n1 Afghanistan  1999    745   19987071\n2 Afghanistan  2000   2666   20595360\n3 Brazil       1999  37737  172006362\n4 Brazil       2000  80488  174504898\n5 China        1999 212258 1272915272\n6 China        2000 213766 1280428583\n\n\n\n\nThere are four columns: country, year, type, and count. Each observation is contained within two rows. The type column has values for variables names with the next column count containing the corresponding value for that variable.\n\ntable2\n\n# A tibble: 12 × 4\n   country      year type            count\n   &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;           &lt;dbl&gt;\n 1 Afghanistan  1999 cases             745\n 2 Afghanistan  1999 population   19987071\n 3 Afghanistan  2000 cases            2666\n 4 Afghanistan  2000 population   20595360\n 5 Brazil       1999 cases           37737\n 6 Brazil       1999 population  172006362\n 7 Brazil       2000 cases           80488\n 8 Brazil       2000 population  174504898\n 9 China        1999 cases          212258\n10 China        1999 population 1272915272\n11 China        2000 cases          213766\n12 China        2000 population 1280428583\n\n\n\n\nThere are three columns: country, year, rate. Each observation correspond to one row each. The rate column contains two values from unknown variables to calculate its values.\n\ntable3\n\n# A tibble: 6 × 3\n  country      year rate             \n  &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;            \n1 Afghanistan  1999 745/19987071     \n2 Afghanistan  2000 2666/20595360    \n3 Brazil       1999 37737/172006362  \n4 Brazil       2000 80488/174504898  \n5 China        1999 212258/1272915272\n6 China        2000 213766/1280428583\n\n\n\n\n\nSketch out the process you’d use to calculate the rate for table2 and table3. You will need to perform four operations:\n\nExtract the number of TB cases per country per year.\nFor table2, make sure to identify by country and year. Look for the type cases and its corresponding values in the count column. For table3, each row contain the number of cases for each country and year. You need to extract the values before the “\\”.\nExtract the matching population per country per year\nFor table2, make sure to identify by country and year. Look for the type population and its corresponding values in the count column. For table3, each row contain the number of cases for each country and year. You need to extract the values after the “\\”.\nDivide cases by population, and multiply by 10000.\nAfter extracting the values from the earlier steps, you can perform the arithmetic required.\nStore back in the appropriate place.\nAfter calculating the values, you can assign it to a new column.",
    "crumbs": [
      "R for Data Science: Outputs",
      "Whole Game",
      "5 Data tidying"
    ]
  },
  {
    "objectID": "r4ds_outputs/chapter_5.html#lengthening-data",
    "href": "r4ds_outputs/chapter_5.html#lengthening-data",
    "title": "5 Data tidying",
    "section": "5.3 Lengthening data",
    "text": "5.3 Lengthening data\n\n5.3.1 Data in column names\n\nbillboard\n\n# A tibble: 317 × 79\n   artist     track date.entered   wk1   wk2   wk3   wk4   wk5   wk6   wk7   wk8\n   &lt;chr&gt;      &lt;chr&gt; &lt;date&gt;       &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 2 Pac      Baby… 2000-02-26      87    82    72    77    87    94    99    NA\n 2 2Ge+her    The … 2000-09-02      91    87    92    NA    NA    NA    NA    NA\n 3 3 Doors D… Kryp… 2000-04-08      81    70    68    67    66    57    54    53\n 4 3 Doors D… Loser 2000-10-21      76    76    72    69    67    65    55    59\n 5 504 Boyz   Wobb… 2000-04-15      57    34    25    17    17    31    36    49\n 6 98^0       Give… 2000-08-19      51    39    34    26    26    19     2     2\n 7 A*Teens    Danc… 2000-07-08      97    97    96    95   100    NA    NA    NA\n 8 Aaliyah    I Do… 2000-01-29      84    62    51    41    38    35    35    38\n 9 Aaliyah    Try … 2000-03-18      59    53    38    28    21    18    16    14\n10 Adams, Yo… Open… 2000-08-26      76    76    74    69    68    67    61    58\n# ℹ 307 more rows\n# ℹ 68 more variables: wk9 &lt;dbl&gt;, wk10 &lt;dbl&gt;, wk11 &lt;dbl&gt;, wk12 &lt;dbl&gt;,\n#   wk13 &lt;dbl&gt;, wk14 &lt;dbl&gt;, wk15 &lt;dbl&gt;, wk16 &lt;dbl&gt;, wk17 &lt;dbl&gt;, wk18 &lt;dbl&gt;,\n#   wk19 &lt;dbl&gt;, wk20 &lt;dbl&gt;, wk21 &lt;dbl&gt;, wk22 &lt;dbl&gt;, wk23 &lt;dbl&gt;, wk24 &lt;dbl&gt;,\n#   wk25 &lt;dbl&gt;, wk26 &lt;dbl&gt;, wk27 &lt;dbl&gt;, wk28 &lt;dbl&gt;, wk29 &lt;dbl&gt;, wk30 &lt;dbl&gt;,\n#   wk31 &lt;dbl&gt;, wk32 &lt;dbl&gt;, wk33 &lt;dbl&gt;, wk34 &lt;dbl&gt;, wk35 &lt;dbl&gt;, wk36 &lt;dbl&gt;,\n#   wk37 &lt;dbl&gt;, wk38 &lt;dbl&gt;, wk39 &lt;dbl&gt;, wk40 &lt;dbl&gt;, wk41 &lt;dbl&gt;, wk42 &lt;dbl&gt;, …\n\n\n\nbillboard |&gt; \n  pivot_longer(\n    cols = starts_with(\"wk\"), \n    names_to = \"week\", \n    values_to = \"rank\"\n  )\n\n# A tibble: 24,092 × 5\n   artist track                   date.entered week   rank\n   &lt;chr&gt;  &lt;chr&gt;                   &lt;date&gt;       &lt;chr&gt; &lt;dbl&gt;\n 1 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk1      87\n 2 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk2      82\n 3 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk3      72\n 4 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk4      77\n 5 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk5      87\n 6 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk6      94\n 7 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk7      99\n 8 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk8      NA\n 9 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk9      NA\n10 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk10     NA\n# ℹ 24,082 more rows\n\n\n\nbillboard |&gt; \n  pivot_longer(\n    cols = starts_with(\"wk\"), \n    names_to = \"week\", \n    values_to = \"rank\",\n    values_drop_na = TRUE\n  )\n\n# A tibble: 5,307 × 5\n   artist  track                   date.entered week   rank\n   &lt;chr&gt;   &lt;chr&gt;                   &lt;date&gt;       &lt;chr&gt; &lt;dbl&gt;\n 1 2 Pac   Baby Don't Cry (Keep... 2000-02-26   wk1      87\n 2 2 Pac   Baby Don't Cry (Keep... 2000-02-26   wk2      82\n 3 2 Pac   Baby Don't Cry (Keep... 2000-02-26   wk3      72\n 4 2 Pac   Baby Don't Cry (Keep... 2000-02-26   wk4      77\n 5 2 Pac   Baby Don't Cry (Keep... 2000-02-26   wk5      87\n 6 2 Pac   Baby Don't Cry (Keep... 2000-02-26   wk6      94\n 7 2 Pac   Baby Don't Cry (Keep... 2000-02-26   wk7      99\n 8 2Ge+her The Hardest Part Of ... 2000-09-02   wk1      91\n 9 2Ge+her The Hardest Part Of ... 2000-09-02   wk2      87\n10 2Ge+her The Hardest Part Of ... 2000-09-02   wk3      92\n# ℹ 5,297 more rows\n\n\n\nbillboard_longer &lt;- billboard |&gt; \n  pivot_longer(\n    cols = starts_with(\"wk\"), \n    names_to = \"week\", \n    values_to = \"rank\",\n    values_drop_na = TRUE\n  ) |&gt; \n  mutate(\n    week = parse_number(week)\n  )\nbillboard_longer\n\n# A tibble: 5,307 × 5\n   artist  track                   date.entered  week  rank\n   &lt;chr&gt;   &lt;chr&gt;                   &lt;date&gt;       &lt;dbl&gt; &lt;dbl&gt;\n 1 2 Pac   Baby Don't Cry (Keep... 2000-02-26       1    87\n 2 2 Pac   Baby Don't Cry (Keep... 2000-02-26       2    82\n 3 2 Pac   Baby Don't Cry (Keep... 2000-02-26       3    72\n 4 2 Pac   Baby Don't Cry (Keep... 2000-02-26       4    77\n 5 2 Pac   Baby Don't Cry (Keep... 2000-02-26       5    87\n 6 2 Pac   Baby Don't Cry (Keep... 2000-02-26       6    94\n 7 2 Pac   Baby Don't Cry (Keep... 2000-02-26       7    99\n 8 2Ge+her The Hardest Part Of ... 2000-09-02       1    91\n 9 2Ge+her The Hardest Part Of ... 2000-09-02       2    87\n10 2Ge+her The Hardest Part Of ... 2000-09-02       3    92\n# ℹ 5,297 more rows\n\n\n\nbillboard_longer |&gt; \n  ggplot(aes(x = week, y = rank, group = track)) + \n  geom_line(alpha = 0.25) + \n  scale_y_reverse()\n\n\n\n\n\n\n\n\n\n\n5.3.2 How does pivoting work?\n\ndf &lt;- tribble(\n  ~id,  ~bp1, ~bp2,\n   \"A\",  100,  120,\n   \"B\",  140,  115,\n   \"C\",  120,  125\n)\n\n\ndf |&gt; \n  pivot_longer(\n    cols = bp1:bp2,\n    names_to = \"measurement\",\n    values_to = \"value\"\n  )\n\n# A tibble: 6 × 3\n  id    measurement value\n  &lt;chr&gt; &lt;chr&gt;       &lt;dbl&gt;\n1 A     bp1           100\n2 A     bp2           120\n3 B     bp1           140\n4 B     bp2           115\n5 C     bp1           120\n6 C     bp2           125\n\n\n\n\n5.3.3 Many variables in column names\n\nwho2\n\n# A tibble: 7,240 × 58\n   country      year sp_m_014 sp_m_1524 sp_m_2534 sp_m_3544 sp_m_4554 sp_m_5564\n   &lt;chr&gt;       &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n 1 Afghanistan  1980       NA        NA        NA        NA        NA        NA\n 2 Afghanistan  1981       NA        NA        NA        NA        NA        NA\n 3 Afghanistan  1982       NA        NA        NA        NA        NA        NA\n 4 Afghanistan  1983       NA        NA        NA        NA        NA        NA\n 5 Afghanistan  1984       NA        NA        NA        NA        NA        NA\n 6 Afghanistan  1985       NA        NA        NA        NA        NA        NA\n 7 Afghanistan  1986       NA        NA        NA        NA        NA        NA\n 8 Afghanistan  1987       NA        NA        NA        NA        NA        NA\n 9 Afghanistan  1988       NA        NA        NA        NA        NA        NA\n10 Afghanistan  1989       NA        NA        NA        NA        NA        NA\n# ℹ 7,230 more rows\n# ℹ 50 more variables: sp_m_65 &lt;dbl&gt;, sp_f_014 &lt;dbl&gt;, sp_f_1524 &lt;dbl&gt;,\n#   sp_f_2534 &lt;dbl&gt;, sp_f_3544 &lt;dbl&gt;, sp_f_4554 &lt;dbl&gt;, sp_f_5564 &lt;dbl&gt;,\n#   sp_f_65 &lt;dbl&gt;, sn_m_014 &lt;dbl&gt;, sn_m_1524 &lt;dbl&gt;, sn_m_2534 &lt;dbl&gt;,\n#   sn_m_3544 &lt;dbl&gt;, sn_m_4554 &lt;dbl&gt;, sn_m_5564 &lt;dbl&gt;, sn_m_65 &lt;dbl&gt;,\n#   sn_f_014 &lt;dbl&gt;, sn_f_1524 &lt;dbl&gt;, sn_f_2534 &lt;dbl&gt;, sn_f_3544 &lt;dbl&gt;,\n#   sn_f_4554 &lt;dbl&gt;, sn_f_5564 &lt;dbl&gt;, sn_f_65 &lt;dbl&gt;, ep_m_014 &lt;dbl&gt;, …\n\n\n\nwho2 |&gt; \n  pivot_longer(\n    cols = !(country:year),\n    names_to = c(\"diagnosis\", \"gender\", \"age\"), \n    names_sep = \"_\",\n    values_to = \"count\"\n  )\n\n# A tibble: 405,440 × 6\n   country      year diagnosis gender age   count\n   &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt;  &lt;chr&gt; &lt;dbl&gt;\n 1 Afghanistan  1980 sp        m      014      NA\n 2 Afghanistan  1980 sp        m      1524     NA\n 3 Afghanistan  1980 sp        m      2534     NA\n 4 Afghanistan  1980 sp        m      3544     NA\n 5 Afghanistan  1980 sp        m      4554     NA\n 6 Afghanistan  1980 sp        m      5564     NA\n 7 Afghanistan  1980 sp        m      65       NA\n 8 Afghanistan  1980 sp        f      014      NA\n 9 Afghanistan  1980 sp        f      1524     NA\n10 Afghanistan  1980 sp        f      2534     NA\n# ℹ 405,430 more rows\n\n\n\n\n5.3.4 Data and variable names in the column headers\n\nhousehold\n\n# A tibble: 5 × 5\n  family dob_child1 dob_child2 name_child1 name_child2\n   &lt;int&gt; &lt;date&gt;     &lt;date&gt;     &lt;chr&gt;       &lt;chr&gt;      \n1      1 1998-11-26 2000-01-29 Susan       Jose       \n2      2 1996-06-22 NA         Mark        &lt;NA&gt;       \n3      3 2002-07-11 2004-04-05 Sam         Seth       \n4      4 2004-10-10 2009-08-27 Craig       Khai       \n5      5 2000-12-05 2005-02-28 Parker      Gracie     \n\n\n\nhousehold |&gt; \n  pivot_longer(\n    cols = !family, \n    names_to = c(\".value\", \"child\"), \n    names_sep = \"_\", \n    values_drop_na = TRUE\n  )\n\n# A tibble: 9 × 4\n  family child  dob        name  \n   &lt;int&gt; &lt;chr&gt;  &lt;date&gt;     &lt;chr&gt; \n1      1 child1 1998-11-26 Susan \n2      1 child2 2000-01-29 Jose  \n3      2 child1 1996-06-22 Mark  \n4      3 child1 2002-07-11 Sam   \n5      3 child2 2004-04-05 Seth  \n6      4 child1 2004-10-10 Craig \n7      4 child2 2009-08-27 Khai  \n8      5 child1 2000-12-05 Parker\n9      5 child2 2005-02-28 Gracie",
    "crumbs": [
      "R for Data Science: Outputs",
      "Whole Game",
      "5 Data tidying"
    ]
  },
  {
    "objectID": "r4ds_outputs/chapter_5.html#widening-data",
    "href": "r4ds_outputs/chapter_5.html#widening-data",
    "title": "5 Data tidying",
    "section": "5.4 Widening Data",
    "text": "5.4 Widening Data\n\ncms_patient_experience\n\n# A tibble: 500 × 5\n   org_pac_id org_nm                           measure_cd measure_title prf_rate\n   &lt;chr&gt;      &lt;chr&gt;                            &lt;chr&gt;      &lt;chr&gt;            &lt;dbl&gt;\n 1 0446157747 USC CARE MEDICAL GROUP INC       CAHPS_GRP… CAHPS for MI…       63\n 2 0446157747 USC CARE MEDICAL GROUP INC       CAHPS_GRP… CAHPS for MI…       87\n 3 0446157747 USC CARE MEDICAL GROUP INC       CAHPS_GRP… CAHPS for MI…       86\n 4 0446157747 USC CARE MEDICAL GROUP INC       CAHPS_GRP… CAHPS for MI…       57\n 5 0446157747 USC CARE MEDICAL GROUP INC       CAHPS_GRP… CAHPS for MI…       85\n 6 0446157747 USC CARE MEDICAL GROUP INC       CAHPS_GRP… CAHPS for MI…       24\n 7 0446162697 ASSOCIATION OF UNIVERSITY PHYSI… CAHPS_GRP… CAHPS for MI…       59\n 8 0446162697 ASSOCIATION OF UNIVERSITY PHYSI… CAHPS_GRP… CAHPS for MI…       85\n 9 0446162697 ASSOCIATION OF UNIVERSITY PHYSI… CAHPS_GRP… CAHPS for MI…       83\n10 0446162697 ASSOCIATION OF UNIVERSITY PHYSI… CAHPS_GRP… CAHPS for MI…       63\n# ℹ 490 more rows\n\n\n\ncms_patient_experience |&gt; \n  distinct(measure_cd, measure_title)\n\n# A tibble: 6 × 2\n  measure_cd   measure_title                                                    \n  &lt;chr&gt;        &lt;chr&gt;                                                            \n1 CAHPS_GRP_1  CAHPS for MIPS SSM: Getting Timely Care, Appointments, and Infor…\n2 CAHPS_GRP_2  CAHPS for MIPS SSM: How Well Providers Communicate               \n3 CAHPS_GRP_3  CAHPS for MIPS SSM: Patient's Rating of Provider                 \n4 CAHPS_GRP_5  CAHPS for MIPS SSM: Health Promotion and Education               \n5 CAHPS_GRP_8  CAHPS for MIPS SSM: Courteous and Helpful Office Staff           \n6 CAHPS_GRP_12 CAHPS for MIPS SSM: Stewardship of Patient Resources             \n\n\n\ncms_patient_experience |&gt; \n  pivot_wider(\n    names_from = measure_cd,\n    values_from = prf_rate\n  )\n\n# A tibble: 500 × 9\n   org_pac_id org_nm           measure_title CAHPS_GRP_1 CAHPS_GRP_2 CAHPS_GRP_3\n   &lt;chr&gt;      &lt;chr&gt;            &lt;chr&gt;               &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;\n 1 0446157747 USC CARE MEDICA… CAHPS for MI…          63          NA          NA\n 2 0446157747 USC CARE MEDICA… CAHPS for MI…          NA          87          NA\n 3 0446157747 USC CARE MEDICA… CAHPS for MI…          NA          NA          86\n 4 0446157747 USC CARE MEDICA… CAHPS for MI…          NA          NA          NA\n 5 0446157747 USC CARE MEDICA… CAHPS for MI…          NA          NA          NA\n 6 0446157747 USC CARE MEDICA… CAHPS for MI…          NA          NA          NA\n 7 0446162697 ASSOCIATION OF … CAHPS for MI…          59          NA          NA\n 8 0446162697 ASSOCIATION OF … CAHPS for MI…          NA          85          NA\n 9 0446162697 ASSOCIATION OF … CAHPS for MI…          NA          NA          83\n10 0446162697 ASSOCIATION OF … CAHPS for MI…          NA          NA          NA\n# ℹ 490 more rows\n# ℹ 3 more variables: CAHPS_GRP_5 &lt;dbl&gt;, CAHPS_GRP_8 &lt;dbl&gt;, CAHPS_GRP_12 &lt;dbl&gt;\n\n\n\ncms_patient_experience |&gt; \n  pivot_wider(\n    id_cols = starts_with(\"org\"),\n    names_from = measure_cd,\n    values_from = prf_rate\n  )\n\n# A tibble: 95 × 8\n   org_pac_id org_nm CAHPS_GRP_1 CAHPS_GRP_2 CAHPS_GRP_3 CAHPS_GRP_5 CAHPS_GRP_8\n   &lt;chr&gt;      &lt;chr&gt;        &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;\n 1 0446157747 USC C…          63          87          86          57          85\n 2 0446162697 ASSOC…          59          85          83          63          88\n 3 0547164295 BEAVE…          49          NA          75          44          73\n 4 0749333730 CAPE …          67          84          85          65          82\n 5 0840104360 ALLIA…          66          87          87          64          87\n 6 0840109864 REX H…          73          87          84          67          91\n 7 0840513552 SCL H…          58          83          76          58          78\n 8 0941545784 GRITM…          46          86          81          54          NA\n 9 1052612785 COMMU…          65          84          80          58          87\n10 1254237779 OUR L…          61          NA          NA          65          NA\n# ℹ 85 more rows\n# ℹ 1 more variable: CAHPS_GRP_12 &lt;dbl&gt;\n\n\n\n5.4.1 How does pivot_wider() work\n\ndf &lt;- tribble(\n  ~id, ~measurement, ~value,\n  \"A\",        \"bp1\",    100,\n  \"B\",        \"bp1\",    140,\n  \"B\",        \"bp2\",    115, \n  \"A\",        \"bp2\",    120,\n  \"A\",        \"bp3\",    105\n)\n\n\ndf |&gt; \n  pivot_wider(\n    names_from = measurement,\n    values_from = value\n  )\n\n# A tibble: 2 × 4\n  id      bp1   bp2   bp3\n  &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 A       100   120   105\n2 B       140   115    NA\n\n\n\ndf |&gt; \n  distinct(measurement) |&gt; \n  pull()\n\n[1] \"bp1\" \"bp2\" \"bp3\"\n\n\n\ndf |&gt; \n  select(-measurement, -value) |&gt; \n  distinct()\n\n# A tibble: 2 × 1\n  id   \n  &lt;chr&gt;\n1 A    \n2 B    \n\n\n\ndf |&gt; \n  select(-measurement, -value) |&gt; \n  distinct() |&gt; \n  mutate(x = NA, y = NA, z = NA)\n\n# A tibble: 2 × 4\n  id    x     y     z    \n  &lt;chr&gt; &lt;lgl&gt; &lt;lgl&gt; &lt;lgl&gt;\n1 A     NA    NA    NA   \n2 B     NA    NA    NA   \n\n\n\ndf &lt;- tribble(\n  ~id, ~measurement, ~value,\n  \"A\",        \"bp1\",    100,\n  \"A\",        \"bp1\",    102,\n  \"A\",        \"bp2\",    120,\n  \"B\",        \"bp1\",    140, \n  \"B\",        \"bp2\",    115\n)\n\n\ndf |&gt;\n  pivot_wider(\n    names_from = measurement,\n    values_from = value\n  )\n\nWarning: Values from `value` are not uniquely identified; output will contain list-cols.\n• Use `values_fn = list` to suppress this warning.\n• Use `values_fn = {summary_fun}` to summarise duplicates.\n• Use the following dplyr code to identify duplicates.\n  {data} |&gt;\n  dplyr::summarise(n = dplyr::n(), .by = c(id, measurement)) |&gt;\n  dplyr::filter(n &gt; 1L)\n\n\n# A tibble: 2 × 3\n  id    bp1       bp2      \n  &lt;chr&gt; &lt;list&gt;    &lt;list&gt;   \n1 A     &lt;dbl [2]&gt; &lt;dbl [1]&gt;\n2 B     &lt;dbl [1]&gt; &lt;dbl [1]&gt;\n\n\n\ndf |&gt; \n  group_by(id, measurement) |&gt; \n  summarize(n = n(), .groups = \"drop\") |&gt; \n  filter(n &gt; 1)\n\n# A tibble: 1 × 3\n  id    measurement     n\n  &lt;chr&gt; &lt;chr&gt;       &lt;int&gt;\n1 A     bp1             2",
    "crumbs": [
      "R for Data Science: Outputs",
      "Whole Game",
      "5 Data tidying"
    ]
  },
  {
    "objectID": "r4ds_outputs/chapter_3.html#footnotes",
    "href": "r4ds_outputs/chapter_3.html#footnotes",
    "title": "3 Data Transformation",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAlso, the same number of rows will be retained from the original data frame.↩︎",
    "crumbs": [
      "R for Data Science: Outputs",
      "Whole Game",
      "3 Data Transformation"
    ]
  },
  {
    "objectID": "r4ds_outputs/chapter_4.html",
    "href": "r4ds_outputs/chapter_4.html",
    "title": "4 Workflow: code style",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(nycflights13)",
    "crumbs": [
      "R for Data Science: Outputs",
      "Whole Game",
      "4 Workflow: code style"
    ]
  },
  {
    "objectID": "r4ds_outputs/chapter_4.html#names",
    "href": "r4ds_outputs/chapter_4.html#names",
    "title": "4 Workflow: code style",
    "section": "4.1 Names",
    "text": "4.1 Names\n\n# Strive for:\nshort_flights &lt;- flights |&gt; filter(air_time &lt; 60)\n\n# Avoid:\nSHORTFLIGHTS &lt;- flights |&gt; filter(air_time &lt; 60)",
    "crumbs": [
      "R for Data Science: Outputs",
      "Whole Game",
      "4 Workflow: code style"
    ]
  },
  {
    "objectID": "r4ds_outputs/chapter_4.html#spaces",
    "href": "r4ds_outputs/chapter_4.html#spaces",
    "title": "4 Workflow: code style",
    "section": "4.2 Spaces",
    "text": "4.2 Spaces\n\n# Strive for\n# z &lt;- (a + b)^2 / d\n\n# Avoid\n# z&lt;-( a + b ) ^ 2/d\n\n\n# Strive for\n# mean(x, na.rm = TRUE)\n\n# Avoid\n# mean (x ,na.rm=TRUE)\n\n\nflights |&gt; \n  mutate(\n    speed      = distance / air_time,\n    dep_hour   = dep_time %/% 100,\n    dep_minute = dep_time %%  100\n  )\n\n# A tibble: 336,776 × 22\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     1     1      517            515         2      830            819\n 2  2013     1     1      533            529         4      850            830\n 3  2013     1     1      542            540         2      923            850\n 4  2013     1     1      544            545        -1     1004           1022\n 5  2013     1     1      554            600        -6      812            837\n 6  2013     1     1      554            558        -4      740            728\n 7  2013     1     1      555            600        -5      913            854\n 8  2013     1     1      557            600        -3      709            723\n 9  2013     1     1      557            600        -3      838            846\n10  2013     1     1      558            600        -2      753            745\n# ℹ 336,766 more rows\n# ℹ 14 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;, speed &lt;dbl&gt;, dep_hour &lt;dbl&gt;,\n#   dep_minute &lt;dbl&gt;",
    "crumbs": [
      "R for Data Science: Outputs",
      "Whole Game",
      "4 Workflow: code style"
    ]
  },
  {
    "objectID": "r4ds_outputs/chapter_4.html#pipes",
    "href": "r4ds_outputs/chapter_4.html#pipes",
    "title": "4 Workflow: code style",
    "section": "4.3 Pipes",
    "text": "4.3 Pipes\n\n# Strive for \nflights |&gt;  \n  filter(!is.na(arr_delay), !is.na(tailnum)) |&gt; \n  count(dest)\n\n# A tibble: 104 × 2\n   dest      n\n   &lt;chr&gt; &lt;int&gt;\n 1 ABQ     254\n 2 ACK     264\n 3 ALB     418\n 4 ANC       8\n 5 ATL   16837\n 6 AUS    2411\n 7 AVL     261\n 8 BDL     412\n 9 BGR     358\n10 BHM     269\n# ℹ 94 more rows\n\n# Avoid\nflights|&gt;filter(!is.na(arr_delay), !is.na(tailnum))|&gt;count(dest)\n\n# A tibble: 104 × 2\n   dest      n\n   &lt;chr&gt; &lt;int&gt;\n 1 ABQ     254\n 2 ACK     264\n 3 ALB     418\n 4 ANC       8\n 5 ATL   16837\n 6 AUS    2411\n 7 AVL     261\n 8 BDL     412\n 9 BGR     358\n10 BHM     269\n# ℹ 94 more rows\n\n\n\n# Strive for\nflights |&gt;  \n  group_by(tailnum) |&gt; \n  summarize(\n    delay = mean(arr_delay, na.rm = TRUE),\n    n = n()\n  )\n\n# A tibble: 4,044 × 3\n   tailnum  delay     n\n   &lt;chr&gt;    &lt;dbl&gt; &lt;int&gt;\n 1 D942DN  31.5       4\n 2 N0EGMQ   9.98    371\n 3 N10156  12.7     153\n 4 N102UW   2.94     48\n 5 N103US  -6.93     46\n 6 N104UW   1.80     47\n 7 N10575  20.7     289\n 8 N105UW  -0.267    45\n 9 N107US  -5.73     41\n10 N108UW  -1.25     60\n# ℹ 4,034 more rows\n\n# Avoid\nflights |&gt;\n  group_by(\n    tailnum\n  ) |&gt; \n  summarize(delay = mean(arr_delay, na.rm = TRUE), n = n())\n\n# A tibble: 4,044 × 3\n   tailnum  delay     n\n   &lt;chr&gt;    &lt;dbl&gt; &lt;int&gt;\n 1 D942DN  31.5       4\n 2 N0EGMQ   9.98    371\n 3 N10156  12.7     153\n 4 N102UW   2.94     48\n 5 N103US  -6.93     46\n 6 N104UW   1.80     47\n 7 N10575  20.7     289\n 8 N105UW  -0.267    45\n 9 N107US  -5.73     41\n10 N108UW  -1.25     60\n# ℹ 4,034 more rows\n\n\n\n# Strive for \nflights |&gt;  \n  group_by(tailnum) |&gt; \n  summarize(\n    delay = mean(arr_delay, na.rm = TRUE),\n    n = n()\n  )\n\n# A tibble: 4,044 × 3\n   tailnum  delay     n\n   &lt;chr&gt;    &lt;dbl&gt; &lt;int&gt;\n 1 D942DN  31.5       4\n 2 N0EGMQ   9.98    371\n 3 N10156  12.7     153\n 4 N102UW   2.94     48\n 5 N103US  -6.93     46\n 6 N104UW   1.80     47\n 7 N10575  20.7     289\n 8 N105UW  -0.267    45\n 9 N107US  -5.73     41\n10 N108UW  -1.25     60\n# ℹ 4,034 more rows\n\n# Avoid\nflights|&gt;\n  group_by(tailnum) |&gt; \n  summarize(\n             delay = mean(arr_delay, na.rm = TRUE), \n             n = n()\n           )\n\n# A tibble: 4,044 × 3\n   tailnum  delay     n\n   &lt;chr&gt;    &lt;dbl&gt; &lt;int&gt;\n 1 D942DN  31.5       4\n 2 N0EGMQ   9.98    371\n 3 N10156  12.7     153\n 4 N102UW   2.94     48\n 5 N103US  -6.93     46\n 6 N104UW   1.80     47\n 7 N10575  20.7     289\n 8 N105UW  -0.267    45\n 9 N107US  -5.73     41\n10 N108UW  -1.25     60\n# ℹ 4,034 more rows\n\n# Avoid\nflights|&gt;\n  group_by(tailnum) |&gt; \n  summarize(\n  delay = mean(arr_delay, na.rm = TRUE), \n  n = n()\n  )\n\n# A tibble: 4,044 × 3\n   tailnum  delay     n\n   &lt;chr&gt;    &lt;dbl&gt; &lt;int&gt;\n 1 D942DN  31.5       4\n 2 N0EGMQ   9.98    371\n 3 N10156  12.7     153\n 4 N102UW   2.94     48\n 5 N103US  -6.93     46\n 6 N104UW   1.80     47\n 7 N10575  20.7     289\n 8 N105UW  -0.267    45\n 9 N107US  -5.73     41\n10 N108UW  -1.25     60\n# ℹ 4,034 more rows\n\n\n\n# This fits compactly on one line\n# df |&gt; mutate(y = x + 1)\n\n# While this takes up 4x as many lines, it's easily extended to \n# more variables and more steps in the future\n# df |&gt; \n#   mutate(\n#     y = x + 1\n#   )",
    "crumbs": [
      "R for Data Science: Outputs",
      "Whole Game",
      "4 Workflow: code style"
    ]
  },
  {
    "objectID": "r4ds_outputs/chapter_4.html#ggplot2",
    "href": "r4ds_outputs/chapter_4.html#ggplot2",
    "title": "4 Workflow: code style",
    "section": "4.4 ggplot2",
    "text": "4.4 ggplot2\n\nflights |&gt; \n  group_by(month) |&gt; \n  summarize(\n    delay = mean(arr_delay, na.rm = TRUE)\n  ) |&gt; \n  ggplot(aes(x = month, y = delay)) +\n  geom_point() + \n  geom_line()\n\n\n\n\n\n\n\n\n\nflights |&gt; \n  group_by(dest) |&gt; \n  summarize(\n    distance = mean(distance),\n    speed = mean(distance / air_time, na.rm = TRUE)\n  ) |&gt; \n  ggplot(aes(x = distance, y = speed)) +\n  geom_smooth(\n    method = \"loess\",\n    span = 0.5,\n    se = FALSE, \n    color = \"white\", \n    linewidth = 4\n  ) +\n  geom_point()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 1 row containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_point()`).",
    "crumbs": [
      "R for Data Science: Outputs",
      "Whole Game",
      "4 Workflow: code style"
    ]
  },
  {
    "objectID": "r4ds_outputs/chapter_4.html#sectioning-comments",
    "href": "r4ds_outputs/chapter_4.html#sectioning-comments",
    "title": "4 Workflow: code style",
    "section": "4.5 Sectioning Comments",
    "text": "4.5 Sectioning Comments\n\n# Load data --------------------------------------\n\n# Plot data --------------------------------------",
    "crumbs": [
      "R for Data Science: Outputs",
      "Whole Game",
      "4 Workflow: code style"
    ]
  },
  {
    "objectID": "r4ds_outputs/chapter_4.html#exercises",
    "href": "r4ds_outputs/chapter_4.html#exercises",
    "title": "4 Workflow: code style",
    "section": "4.6 Exercises",
    "text": "4.6 Exercises\nRestyle the following pipelines following the guidelines above.\n\nflights|&gt;filter(dest==\"IAH\")|&gt;group_by(year,month,day)|&gt;summarize(n=n(),\ndelay=mean(arr_delay,na.rm=TRUE))|&gt;filter(n&gt;10)\n\n`summarise()` has grouped output by 'year', 'month'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 365 × 5\n# Groups:   year, month [12]\n    year month   day     n delay\n   &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt;\n 1  2013     1     1    20 17.8 \n 2  2013     1     2    20  7   \n 3  2013     1     3    19 18.3 \n 4  2013     1     4    20 -3.2 \n 5  2013     1     5    13 20.2 \n 6  2013     1     6    18  9.28\n 7  2013     1     7    19 -7.74\n 8  2013     1     8    19  7.79\n 9  2013     1     9    19 18.1 \n10  2013     1    10    19  6.68\n# ℹ 355 more rows\n\nflights|&gt;filter(carrier==\"UA\",dest%in%c(\"IAH\",\"HOU\"),sched_dep_time&gt;\n0900,sched_arr_time&lt;2000)|&gt;group_by(flight)|&gt;summarize(delay=mean(\narr_delay,na.rm=TRUE),cancelled=sum(is.na(arr_delay)),n=n())|&gt;filter(n&gt;10)\n\n# A tibble: 74 × 4\n   flight delay cancelled     n\n    &lt;int&gt; &lt;dbl&gt;     &lt;int&gt; &lt;int&gt;\n 1     53 12.5          2    18\n 2    112 14.1          0    14\n 3    205 -1.71         0    14\n 4    235 -5.36         0    14\n 5    255 -9.47         0    15\n 6    268 38.6          1    15\n 7    292  6.57         0    21\n 8    318 10.7          1    20\n 9    337 20.1          2    21\n10    370 17.5          0    11\n# ℹ 64 more rows\n\n\n\nflights |&gt; \n  filter(dest==\"IAH\") |&gt; \n  group_by(year, month, day) |&gt; \n  summarize(\n    n = n(),\n    delay = mean(arr_delay, na.rm=TRUE)\n  ) |&gt; \n  filter(n &gt; 10)\n\n`summarise()` has grouped output by 'year', 'month'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 365 × 5\n# Groups:   year, month [12]\n    year month   day     n delay\n   &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt;\n 1  2013     1     1    20 17.8 \n 2  2013     1     2    20  7   \n 3  2013     1     3    19 18.3 \n 4  2013     1     4    20 -3.2 \n 5  2013     1     5    13 20.2 \n 6  2013     1     6    18  9.28\n 7  2013     1     7    19 -7.74\n 8  2013     1     8    19  7.79\n 9  2013     1     9    19 18.1 \n10  2013     1    10    19  6.68\n# ℹ 355 more rows\n\nflights |&gt; \n  filter(\n    carrier==\"UA\",\n    dest %in% c(\"IAH\", \"HOU\"),\n    sched_dep_time &gt; 0900,\n    sched_arr_time &lt; 2000\n  ) |&gt; \n  group_by(flight) |&gt; \n  summarize(\n    delay = mean(arr_delay, na.rm=TRUE),\n    cancelled = sum(is.na(arr_delay)), n = n()\n  ) |&gt; \n  filter(n&gt;10)\n\n# A tibble: 74 × 4\n   flight delay cancelled     n\n    &lt;int&gt; &lt;dbl&gt;     &lt;int&gt; &lt;int&gt;\n 1     53 12.5          2    18\n 2    112 14.1          0    14\n 3    205 -1.71         0    14\n 4    235 -5.36         0    14\n 5    255 -9.47         0    15\n 6    268 38.6          1    15\n 7    292  6.57         0    21\n 8    318 10.7          1    20\n 9    337 20.1          2    21\n10    370 17.5          0    11\n# ℹ 64 more rows",
    "crumbs": [
      "R for Data Science: Outputs",
      "Whole Game",
      "4 Workflow: code style"
    ]
  },
  {
    "objectID": "r4ds_outputs/chapter_7.html",
    "href": "r4ds_outputs/chapter_7.html",
    "title": "7 Data import",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors",
    "crumbs": [
      "R for Data Science: Outputs",
      "Whole Game",
      "7 Data import"
    ]
  },
  {
    "objectID": "r4ds_outputs/chapter_7.html#introduction",
    "href": "r4ds_outputs/chapter_7.html#introduction",
    "title": "7 Data import",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors",
    "crumbs": [
      "R for Data Science: Outputs",
      "Whole Game",
      "7 Data import"
    ]
  },
  {
    "objectID": "r4ds_outputs/chapter_7.html#reading-data-from-file",
    "href": "r4ds_outputs/chapter_7.html#reading-data-from-file",
    "title": "7 Data import",
    "section": "7.2 Reading data from file",
    "text": "7.2 Reading data from file\n\nstudents &lt;- read_csv(\"https://pos.it/r4ds-students-csv\")\n\nRows: 6 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (4): Full Name, favourite.food, mealPlan, AGE\ndbl (1): Student ID\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n7.2.1 Practical Advice\n\nstudents\n\n# A tibble: 6 × 5\n  `Student ID` `Full Name`      favourite.food     mealPlan            AGE  \n         &lt;dbl&gt; &lt;chr&gt;            &lt;chr&gt;              &lt;chr&gt;               &lt;chr&gt;\n1            1 Sunil Huffmann   Strawberry yoghurt Lunch only          4    \n2            2 Barclay Lynn     French fries       Lunch only          5    \n3            3 Jayendra Lyne    N/A                Breakfast and lunch 7    \n4            4 Leon Rossini     Anchovies          Lunch only          &lt;NA&gt; \n5            5 Chidiegwu Dunkel Pizza              Breakfast and lunch five \n6            6 Güvenç Attila    Ice cream          Lunch only          6    \n\n\n\nstudents &lt;- read_csv(\"https://pos.it/r4ds-students-csv\", na = c(\"N/A\", \"\"))\n\nRows: 6 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (4): Full Name, favourite.food, mealPlan, AGE\ndbl (1): Student ID\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nstudents\n\n# A tibble: 6 × 5\n  `Student ID` `Full Name`      favourite.food     mealPlan            AGE  \n         &lt;dbl&gt; &lt;chr&gt;            &lt;chr&gt;              &lt;chr&gt;               &lt;chr&gt;\n1            1 Sunil Huffmann   Strawberry yoghurt Lunch only          4    \n2            2 Barclay Lynn     French fries       Lunch only          5    \n3            3 Jayendra Lyne    &lt;NA&gt;               Breakfast and lunch 7    \n4            4 Leon Rossini     Anchovies          Lunch only          &lt;NA&gt; \n5            5 Chidiegwu Dunkel Pizza              Breakfast and lunch five \n6            6 Güvenç Attila    Ice cream          Lunch only          6    \n\n\n\nstudents |&gt; \n  rename(\n    student_id = `Student ID`,\n    full_name = `Full Name`\n  )\n\n# A tibble: 6 × 5\n  student_id full_name        favourite.food     mealPlan            AGE  \n       &lt;dbl&gt; &lt;chr&gt;            &lt;chr&gt;              &lt;chr&gt;               &lt;chr&gt;\n1          1 Sunil Huffmann   Strawberry yoghurt Lunch only          4    \n2          2 Barclay Lynn     French fries       Lunch only          5    \n3          3 Jayendra Lyne    &lt;NA&gt;               Breakfast and lunch 7    \n4          4 Leon Rossini     Anchovies          Lunch only          &lt;NA&gt; \n5          5 Chidiegwu Dunkel Pizza              Breakfast and lunch five \n6          6 Güvenç Attila    Ice cream          Lunch only          6    \n\n\n\nstudents |&gt; janitor::clean_names()\n\n# A tibble: 6 × 5\n  student_id full_name        favourite_food     meal_plan           age  \n       &lt;dbl&gt; &lt;chr&gt;            &lt;chr&gt;              &lt;chr&gt;               &lt;chr&gt;\n1          1 Sunil Huffmann   Strawberry yoghurt Lunch only          4    \n2          2 Barclay Lynn     French fries       Lunch only          5    \n3          3 Jayendra Lyne    &lt;NA&gt;               Breakfast and lunch 7    \n4          4 Leon Rossini     Anchovies          Lunch only          &lt;NA&gt; \n5          5 Chidiegwu Dunkel Pizza              Breakfast and lunch five \n6          6 Güvenç Attila    Ice cream          Lunch only          6    \n\n\n\nstudents |&gt;\n  janitor::clean_names() |&gt;\n  mutate(meal_plan = factor(meal_plan))\n\n# A tibble: 6 × 5\n  student_id full_name        favourite_food     meal_plan           age  \n       &lt;dbl&gt; &lt;chr&gt;            &lt;chr&gt;              &lt;fct&gt;               &lt;chr&gt;\n1          1 Sunil Huffmann   Strawberry yoghurt Lunch only          4    \n2          2 Barclay Lynn     French fries       Lunch only          5    \n3          3 Jayendra Lyne    &lt;NA&gt;               Breakfast and lunch 7    \n4          4 Leon Rossini     Anchovies          Lunch only          &lt;NA&gt; \n5          5 Chidiegwu Dunkel Pizza              Breakfast and lunch five \n6          6 Güvenç Attila    Ice cream          Lunch only          6    \n\n\n\nstudents &lt;- students |&gt;\n  janitor::clean_names() |&gt;\n  mutate(\n    meal_plan = factor(meal_plan),\n    age = parse_number(if_else(age == \"five\", \"5\", age))\n  )\nstudents\n\n# A tibble: 6 × 5\n  student_id full_name        favourite_food     meal_plan             age\n       &lt;dbl&gt; &lt;chr&gt;            &lt;chr&gt;              &lt;fct&gt;               &lt;dbl&gt;\n1          1 Sunil Huffmann   Strawberry yoghurt Lunch only              4\n2          2 Barclay Lynn     French fries       Lunch only              5\n3          3 Jayendra Lyne    &lt;NA&gt;               Breakfast and lunch     7\n4          4 Leon Rossini     Anchovies          Lunch only             NA\n5          5 Chidiegwu Dunkel Pizza              Breakfast and lunch     5\n6          6 Güvenç Attila    Ice cream          Lunch only              6\n\n\n\n\n7.2.2 Other arguments\n\nread_csv(\n  \"a,b,c\n  1,2,3\n  4,5,6\"\n)\n\nRows: 2 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (3): a, b, c\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n# A tibble: 2 × 3\n      a     b     c\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     1     2     3\n2     4     5     6\n\n\n\nread_csv(\n  \"The first line of metadata\n  The second line of metadata\n  x,y,z\n  1,2,3\",\n  skip = 2\n)\n\nRows: 1 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (3): x, y, z\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n# A tibble: 1 × 3\n      x     y     z\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     1     2     3\n\n\n\nread_csv(\n  \"# A comment I want to skip\n  x,y,z\n  1,2,3\",\n  comment = \"#\"\n)\n\nRows: 1 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (3): x, y, z\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n# A tibble: 1 × 3\n      x     y     z\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     1     2     3\n\n\n\nread_csv(\n  \"1,2,3\n  4,5,6\",\n  col_names = FALSE\n)\n\nRows: 2 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (3): X1, X2, X3\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n# A tibble: 2 × 3\n     X1    X2    X3\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     1     2     3\n2     4     5     6\n\n\n\nread_csv(\n  \"1,2,3\n  4,5,6\",\n  col_names = c(\"x\", \"y\", \"z\")\n)\n\nRows: 2 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (3): x, y, z\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n# A tibble: 2 × 3\n      x     y     z\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     1     2     3\n2     4     5     6\n\n\n\n\n7.2.3 Other file types\n\n\n7.2.4 Exercises\n\nWhat function would you use to read a file where fields were separated with “|”?\nI will use the function read_delim() and specify the delimiter.\nApart from file, skip, and comment, what other arguments do read_csv() and read_tsv() have in common?\nThey are the same because they are essentially doing the same with only the delimiter difference.\nWhat are the most important arguments to read_fwf()?\nThe col_positions is the most important argument to specify when running this function. This gives instruction on how to parse the file.\nSometimes strings in a CSV file contain commas. To prevent them from causing problems, they need to be surrounded by a quoting character, like \" or '. By default, read_csv() assumes that the quoting character will be \". To read the following text into a data frame, what argument to read_csv() do you need to specify?\n\nread_csv(\n  \"x,y\n  1,'a,b'\",\n  quote = \"\\'\"\n)\n\nRows: 1 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): y\ndbl (1): x\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n# A tibble: 1 × 2\n      x y    \n  &lt;dbl&gt; &lt;chr&gt;\n1     1 a,b  \n\n\nIdentify what is wrong with each of the following inline CSV files. What happens when you run the code?\n\nabcde\n\n\n\n# There's not enough column names from the first row.\nread_csv(\"a,b\\n1,2,3\\n4,5,6\")\n\nWarning: One or more parsing issues, call `problems()` on your data frame for details,\ne.g.:\n  dat &lt;- vroom(...)\n  problems(dat)\n\n\nRows: 2 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (1): a\nnum (1): b\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n# A tibble: 2 × 2\n      a     b\n  &lt;dbl&gt; &lt;dbl&gt;\n1     1    23\n2     4    56\n\n\n\n\n\n# There's a mismatch in the number of columns and values per row.\nread_csv(\"a,b,c\\n1,2\\n1,2,3,4\")\n\nWarning: One or more parsing issues, call `problems()` on your data frame for details,\ne.g.:\n  dat &lt;- vroom(...)\n  problems(dat)\n\n\nRows: 2 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (2): a, b\nnum (1): c\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n# A tibble: 2 × 3\n      a     b     c\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     1     2    NA\n2     1     2    34\n\n\n\n\n\n# There's extraneous symbols\nread_csv(\"a,b\\n\\\"1\")\n\nRows: 0 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): a, b\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n# A tibble: 0 × 2\n# ℹ 2 variables: a &lt;chr&gt;, b &lt;chr&gt;\n\n\n\n\n\n# There's a mix of numeric and character values in one variable/column.\nread_csv(\"a,b\\n1,2\\na,b\")\n\nRows: 2 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): a, b\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n# A tibble: 2 × 2\n  a     b    \n  &lt;chr&gt; &lt;chr&gt;\n1 1     2    \n2 a     b    \n\n\n\n\n\n# A different delimiter is expected.\nread_csv(\"a;b\\n1;3\")\n\nRows: 1 Columns: 1\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): a;b\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n# A tibble: 1 × 1\n  `a;b`\n  &lt;chr&gt;\n1 1;3  \n\n\n\n\n\nPractice referring to non-syntactic names in the following data frame by:\n\nannoying &lt;- tibble(\n  `1` = 1:10,\n  `2` = `1` * 2 + rnorm(length(`1`))\n)\nannoying\n\n# A tibble: 10 × 2\n     `1`   `2`\n   &lt;int&gt; &lt;dbl&gt;\n 1     1  2.20\n 2     2  3.96\n 3     3  3.44\n 4     4  9.20\n 5     5  9.64\n 6     6 12.0 \n 7     7 14.4 \n 8     8 16.1 \n 9     9 18.4 \n10    10 19.9 \n\n\n\nabcd\n\n\nExtracting the variable called 1.\n\nannoying |&gt; \n  select(`1`)\n\n# A tibble: 10 × 1\n     `1`\n   &lt;int&gt;\n 1     1\n 2     2\n 3     3\n 4     4\n 5     5\n 6     6\n 7     7\n 8     8\n 9     9\n10    10\n\n\n\n\nPlotting a scatterplot of 1 vs. 2.\n\nannoying |&gt; \n  ggplot(aes(x = `1`, y = `2`)) +\n  geom_point()\n\n\n\n\n\n\n\n\n\n\nCreating a new column called 3, which is 2 divided by 1.\n\nannoying |&gt; \n  mutate(`3` = `2` / `1`)\n\n# A tibble: 10 × 3\n     `1`   `2`   `3`\n   &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1     1  2.20  2.20\n 2     2  3.96  1.98\n 3     3  3.44  1.15\n 4     4  9.20  2.30\n 5     5  9.64  1.93\n 6     6 12.0   2.00\n 7     7 14.4   2.06\n 8     8 16.1   2.02\n 9     9 18.4   2.04\n10    10 19.9   1.99\n\n\n\n\nRenaming the columns to one, two, and three.\n\nannoying |&gt; \n  mutate(`3` = `2` / `1`) |&gt; \n  rename(\n    \"one\" = `1`,\n    \"two\" = `2`,\n    \"three\" = `3`\n  )\n\n# A tibble: 10 × 3\n     one   two three\n   &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1     1  2.20  2.20\n 2     2  3.96  1.98\n 3     3  3.44  1.15\n 4     4  9.20  2.30\n 5     5  9.64  1.93\n 6     6 12.0   2.00\n 7     7 14.4   2.06\n 8     8 16.1   2.02\n 9     9 18.4   2.04\n10    10 19.9   1.99",
    "crumbs": [
      "R for Data Science: Outputs",
      "Whole Game",
      "7 Data import"
    ]
  },
  {
    "objectID": "r4ds_outputs/chapter_6.html",
    "href": "r4ds_outputs/chapter_6.html",
    "title": "6 Workflow: scripts and projects",
    "section": "",
    "text": "library(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(nycflights13)\n\nnot_cancelled &lt;- flights |&gt; \n  filter(!is.na(dep_delay), !is.na(arr_delay))\n\nnot_cancelled |&gt; \n  group_by(year, month, day) |&gt; \n  summarize(mean = mean(dep_delay))\n\n`summarise()` has grouped output by 'year', 'month'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 365 × 4\n# Groups:   year, month [12]\n    year month   day  mean\n   &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt;\n 1  2013     1     1 11.4 \n 2  2013     1     2 13.7 \n 3  2013     1     3 10.9 \n 4  2013     1     4  8.97\n 5  2013     1     5  5.73\n 6  2013     1     6  7.15\n 7  2013     1     7  5.42\n 8  2013     1     8  2.56\n 9  2013     1     9  2.30\n10  2013     1    10  2.84\n# ℹ 355 more rows",
    "crumbs": [
      "R for Data Science: Outputs",
      "Whole Game",
      "6 Workflow: scripts and projects"
    ]
  },
  {
    "objectID": "r4ds_outputs/chapter_6.html#scripts",
    "href": "r4ds_outputs/chapter_6.html#scripts",
    "title": "6 Workflow: scripts and projects",
    "section": "",
    "text": "library(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(nycflights13)\n\nnot_cancelled &lt;- flights |&gt; \n  filter(!is.na(dep_delay), !is.na(arr_delay))\n\nnot_cancelled |&gt; \n  group_by(year, month, day) |&gt; \n  summarize(mean = mean(dep_delay))\n\n`summarise()` has grouped output by 'year', 'month'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 365 × 4\n# Groups:   year, month [12]\n    year month   day  mean\n   &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt;\n 1  2013     1     1 11.4 \n 2  2013     1     2 13.7 \n 3  2013     1     3 10.9 \n 4  2013     1     4  8.97\n 5  2013     1     5  5.73\n 6  2013     1     6  7.15\n 7  2013     1     7  5.42\n 8  2013     1     8  2.56\n 9  2013     1     9  2.30\n10  2013     1    10  2.84\n# ℹ 355 more rows",
    "crumbs": [
      "R for Data Science: Outputs",
      "Whole Game",
      "6 Workflow: scripts and projects"
    ]
  },
  {
    "objectID": "r4ds_outputs/chapter_6.html#projects",
    "href": "r4ds_outputs/chapter_6.html#projects",
    "title": "6 Workflow: scripts and projects",
    "section": "6.2 Projects",
    "text": "6.2 Projects\n\n6.2.1 What is the source of truth?\n\n\n6.2.2 Where does your analysis live?\n\n\n6.2.3 Rstudio projects\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ forcats   1.0.0     ✔ readr     2.1.5\n✔ ggplot2   3.5.1     ✔ stringr   1.5.1\n✔ lubridate 1.9.3     ✔ tibble    3.2.1\n✔ purrr     1.0.2     ✔ tidyr     1.3.1\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nggplot(diamonds, aes(x = carat, y = price)) + \n  geom_hex()\n\n\n\n\n\n\n\nggsave(\"diamonds.png\")\n\nSaving 7 x 5 in image\n\nwrite_csv(diamonds, \"data/diamonds.csv\")\n\n\n\n6.2.4 Relative and absolute paths",
    "crumbs": [
      "R for Data Science: Outputs",
      "Whole Game",
      "6 Workflow: scripts and projects"
    ]
  },
  {
    "objectID": "r4ds_outputs/chapter_6.html#exercises",
    "href": "r4ds_outputs/chapter_6.html#exercises",
    "title": "6 Workflow: scripts and projects",
    "section": "6.3 Exercises",
    "text": "6.3 Exercises\n\nGo to the RStudio Tips Twitter account, https://twitter.com/rstudiotips and find one tip that looks interesting. Practice using it!\nI don’t have a twitter account.\nWhat other common mistakes will RStudio diagnostics report? Read https://support.posit.co/hc/en-us/articles/205753617-Code-Diagnostics to find out.",
    "crumbs": [
      "R for Data Science: Outputs",
      "Whole Game",
      "6 Workflow: scripts and projects"
    ]
  },
  {
    "objectID": "r4ds_outputs/chapter_7.html#controlling-column-types",
    "href": "r4ds_outputs/chapter_7.html#controlling-column-types",
    "title": "7 Data import",
    "section": "7.3 Controlling column types",
    "text": "7.3 Controlling column types\n\n7.3.1 Guessing types\n\nread_csv(\"\n  logical,numeric,date,string\n  TRUE,1,2021-01-15,abc\n  false,4.5,2021-02-15,def\n  T,Inf,2021-02-16,ghi\n\")\n\nRows: 3 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (1): string\ndbl  (1): numeric\nlgl  (1): logical\ndate (1): date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n# A tibble: 3 × 4\n  logical numeric date       string\n  &lt;lgl&gt;     &lt;dbl&gt; &lt;date&gt;     &lt;chr&gt; \n1 TRUE        1   2021-01-15 abc   \n2 FALSE       4.5 2021-02-15 def   \n3 TRUE      Inf   2021-02-16 ghi   \n\n\n\n\n7.3.2 Missing values, column types, and problems\n\nsimple_csv &lt;- \"\n  x\n  10\n  .\n  20\n  30\"\n\n\nread_csv(simple_csv)\n\nRows: 4 Columns: 1\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): x\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n# A tibble: 4 × 1\n  x    \n  &lt;chr&gt;\n1 10   \n2 .    \n3 20   \n4 30   \n\n\n\ndf &lt;- read_csv(\n  simple_csv, \n  col_types = list(x = col_double())\n)\n\nWarning: One or more parsing issues, call `problems()` on your data frame for details,\ne.g.:\n  dat &lt;- vroom(...)\n  problems(dat)\n\n\n\nproblems(df)\n\n# A tibble: 1 × 5\n    row   col expected actual file                            \n  &lt;int&gt; &lt;int&gt; &lt;chr&gt;    &lt;chr&gt;  &lt;chr&gt;                           \n1     3     1 a double .      /tmp/Rtmpc01KO4/file3b292876c4e1\n\n\n\nread_csv(simple_csv, na = \".\")\n\nRows: 4 Columns: 1\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (1): x\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n# A tibble: 4 × 1\n      x\n  &lt;dbl&gt;\n1    10\n2    NA\n3    20\n4    30\n\n\n\n\n7.3.3 Column types\n\nanother_csv &lt;- \"\nx,y,z\n1,2,3\"\n\nread_csv(\n  another_csv, \n  col_types = cols(.default = col_character())\n)\n\n# A tibble: 1 × 3\n  x     y     z    \n  &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;\n1 1     2     3    \n\n\n\nread_csv(\n  another_csv,\n  col_types = cols_only(x = col_character())\n)\n\n# A tibble: 1 × 1\n  x    \n  &lt;chr&gt;\n1 1",
    "crumbs": [
      "R for Data Science: Outputs",
      "Whole Game",
      "7 Data import"
    ]
  },
  {
    "objectID": "r4ds_outputs/chapter_7.html#reading-data-from-multiple-files",
    "href": "r4ds_outputs/chapter_7.html#reading-data-from-multiple-files",
    "title": "7 Data import",
    "section": "7.4 Reading data from multiple files",
    "text": "7.4 Reading data from multiple files\n\nsales_files &lt;- c(\n  \"https://pos.it/r4ds-01-sales\",\n  \"https://pos.it/r4ds-02-sales\",\n  \"https://pos.it/r4ds-03-sales\"\n)\nread_csv(sales_files, id = \"file\")\n\nRows: 19 Columns: 6\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): month\ndbl (4): year, brand, item, n\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n# A tibble: 19 × 6\n   file                         month     year brand  item     n\n   &lt;chr&gt;                        &lt;chr&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 https://pos.it/r4ds-01-sales January   2019     1  1234     3\n 2 https://pos.it/r4ds-01-sales January   2019     1  8721     9\n 3 https://pos.it/r4ds-01-sales January   2019     1  1822     2\n 4 https://pos.it/r4ds-01-sales January   2019     2  3333     1\n 5 https://pos.it/r4ds-01-sales January   2019     2  2156     9\n 6 https://pos.it/r4ds-01-sales January   2019     2  3987     6\n 7 https://pos.it/r4ds-01-sales January   2019     2  3827     6\n 8 https://pos.it/r4ds-02-sales February  2019     1  1234     8\n 9 https://pos.it/r4ds-02-sales February  2019     1  8721     2\n10 https://pos.it/r4ds-02-sales February  2019     1  1822     3\n11 https://pos.it/r4ds-02-sales February  2019     2  3333     1\n12 https://pos.it/r4ds-02-sales February  2019     2  2156     3\n13 https://pos.it/r4ds-02-sales February  2019     2  3987     6\n14 https://pos.it/r4ds-03-sales March     2019     1  1234     3\n15 https://pos.it/r4ds-03-sales March     2019     1  3627     1\n16 https://pos.it/r4ds-03-sales March     2019     1  8820     3\n17 https://pos.it/r4ds-03-sales March     2019     2  7253     1\n18 https://pos.it/r4ds-03-sales March     2019     2  8766     3\n19 https://pos.it/r4ds-03-sales March     2019     2  8288     6\n\n\n\n# sales_files &lt;- list.files(\"data\", pattern = \"sales\\\\.csv$\", full.names = TRUE)\n# sales_files\n# #&gt; [1] \"data/01-sales.csv\" \"data/02-sales.csv\" \"data/03-sales.csv\"",
    "crumbs": [
      "R for Data Science: Outputs",
      "Whole Game",
      "7 Data import"
    ]
  },
  {
    "objectID": "r4ds_outputs/chapter_7.html#writing-to-a-file",
    "href": "r4ds_outputs/chapter_7.html#writing-to-a-file",
    "title": "7 Data import",
    "section": "7.5 Writing to a file",
    "text": "7.5 Writing to a file\n\nwrite_csv(students, \"data/students.csv\")\n\n\nstudents\n\n# A tibble: 6 × 5\n  student_id full_name        favourite_food     meal_plan             age\n       &lt;dbl&gt; &lt;chr&gt;            &lt;chr&gt;              &lt;fct&gt;               &lt;dbl&gt;\n1          1 Sunil Huffmann   Strawberry yoghurt Lunch only              4\n2          2 Barclay Lynn     French fries       Lunch only              5\n3          3 Jayendra Lyne    &lt;NA&gt;               Breakfast and lunch     7\n4          4 Leon Rossini     Anchovies          Lunch only             NA\n5          5 Chidiegwu Dunkel Pizza              Breakfast and lunch     5\n6          6 Güvenç Attila    Ice cream          Lunch only              6\n\nwrite_csv(students, \"data/students-2.csv\")\nread_csv(\"data/students-2.csv\")\n\nRows: 6 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (3): full_name, favourite_food, meal_plan\ndbl (2): student_id, age\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n# A tibble: 6 × 5\n  student_id full_name        favourite_food     meal_plan             age\n       &lt;dbl&gt; &lt;chr&gt;            &lt;chr&gt;              &lt;chr&gt;               &lt;dbl&gt;\n1          1 Sunil Huffmann   Strawberry yoghurt Lunch only              4\n2          2 Barclay Lynn     French fries       Lunch only              5\n3          3 Jayendra Lyne    &lt;NA&gt;               Breakfast and lunch     7\n4          4 Leon Rossini     Anchovies          Lunch only             NA\n5          5 Chidiegwu Dunkel Pizza              Breakfast and lunch     5\n6          6 Güvenç Attila    Ice cream          Lunch only              6\n\n\n\nwrite_rds(students, \"data/students.rds\")\nread_rds(\"data/students.rds\")\n\n# A tibble: 6 × 5\n  student_id full_name        favourite_food     meal_plan             age\n       &lt;dbl&gt; &lt;chr&gt;            &lt;chr&gt;              &lt;fct&gt;               &lt;dbl&gt;\n1          1 Sunil Huffmann   Strawberry yoghurt Lunch only              4\n2          2 Barclay Lynn     French fries       Lunch only              5\n3          3 Jayendra Lyne    &lt;NA&gt;               Breakfast and lunch     7\n4          4 Leon Rossini     Anchovies          Lunch only             NA\n5          5 Chidiegwu Dunkel Pizza              Breakfast and lunch     5\n6          6 Güvenç Attila    Ice cream          Lunch only              6\n\n\n\nlibrary(arrow)\n\nSome features are not enabled in this build of Arrow. Run `arrow_info()` for more information.\n\n\n\nAttaching package: 'arrow'\n\n\nThe following object is masked from 'package:lubridate':\n\n    duration\n\n\nThe following object is masked from 'package:utils':\n\n    timestamp\n\nwrite_parquet(students, \"data/students.parquet\")\nread_parquet(\"data/students.parquet\")\n\n# A tibble: 6 × 5\n  student_id full_name        favourite_food     meal_plan             age\n       &lt;dbl&gt; &lt;chr&gt;            &lt;chr&gt;              &lt;fct&gt;               &lt;dbl&gt;\n1          1 Sunil Huffmann   Strawberry yoghurt Lunch only              4\n2          2 Barclay Lynn     French fries       Lunch only              5\n3          3 Jayendra Lyne    &lt;NA&gt;               Breakfast and lunch     7\n4          4 Leon Rossini     Anchovies          Lunch only             NA\n5          5 Chidiegwu Dunkel Pizza              Breakfast and lunch     5\n6          6 Güvenç Attila    Ice cream          Lunch only              6",
    "crumbs": [
      "R for Data Science: Outputs",
      "Whole Game",
      "7 Data import"
    ]
  },
  {
    "objectID": "r4ds_outputs/chapter_7.html#data-entry",
    "href": "r4ds_outputs/chapter_7.html#data-entry",
    "title": "7 Data import",
    "section": "7.6 Data entry",
    "text": "7.6 Data entry\n\ntibble(\n  x = c(1, 2, 5), \n  y = c(\"h\", \"m\", \"g\"),\n  z = c(0.08, 0.83, 0.60)\n)\n\n# A tibble: 3 × 3\n      x y         z\n  &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt;\n1     1 h      0.08\n2     2 m      0.83\n3     5 g      0.6 \n\n\n\ntribble(\n  ~x, ~y, ~z,\n  1, \"h\", 0.08,\n  2, \"m\", 0.83,\n  5, \"g\", 0.60\n)\n\n# A tibble: 3 × 3\n      x y         z\n  &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt;\n1     1 h      0.08\n2     2 m      0.83\n3     5 g      0.6",
    "crumbs": [
      "R for Data Science: Outputs",
      "Whole Game",
      "7 Data import"
    ]
  },
  {
    "objectID": "r4ds_outputs/chapter_9.html",
    "href": "r4ds_outputs/chapter_9.html",
    "title": "9 Layers",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors",
    "crumbs": [
      "R for Data Science: Outputs",
      "Visualize",
      "9 Layers"
    ]
  },
  {
    "objectID": "r4ds_outputs/chapter_9.html#introduction",
    "href": "r4ds_outputs/chapter_9.html#introduction",
    "title": "9 Layers",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors",
    "crumbs": [
      "R for Data Science: Outputs",
      "Visualize",
      "9 Layers"
    ]
  },
  {
    "objectID": "r4ds_outputs/chapter_9.html#aesthetic-mappings",
    "href": "r4ds_outputs/chapter_9.html#aesthetic-mappings",
    "title": "9 Layers",
    "section": "9.2 Aesthetic mappings",
    "text": "9.2 Aesthetic mappings\n\nmpg\n\n# A tibble: 234 × 11\n   manufacturer model      displ  year   cyl trans drv     cty   hwy fl    class\n   &lt;chr&gt;        &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt;\n 1 audi         a4           1.8  1999     4 auto… f        18    29 p     comp…\n 2 audi         a4           1.8  1999     4 manu… f        21    29 p     comp…\n 3 audi         a4           2    2008     4 manu… f        20    31 p     comp…\n 4 audi         a4           2    2008     4 auto… f        21    30 p     comp…\n 5 audi         a4           2.8  1999     6 auto… f        16    26 p     comp…\n 6 audi         a4           2.8  1999     6 manu… f        18    26 p     comp…\n 7 audi         a4           3.1  2008     6 auto… f        18    27 p     comp…\n 8 audi         a4 quattro   1.8  1999     4 manu… 4        18    26 p     comp…\n 9 audi         a4 quattro   1.8  1999     4 auto… 4        16    25 p     comp…\n10 audi         a4 quattro   2    2008     4 manu… 4        20    28 p     comp…\n# ℹ 224 more rows\n\n\n\n# Left\nggplot(mpg, aes(x = displ, y = hwy, color = class)) +\n  geom_point()\n\n\n\n\n\n\n\n# Right\nggplot(mpg, aes(x = displ, y = hwy, shape = class)) +\n  geom_point()\n\nWarning: The shape palette can deal with a maximum of 6 discrete values because more\nthan 6 becomes difficult to discriminate\nℹ you have requested 7 values. Consider specifying shapes manually if you need\n  that many have them.\n\n\nWarning: Removed 62 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\n\n# Left\nggplot(mpg, aes(x = displ, y = hwy, size = class)) +\n  geom_point()\n\nWarning: Using size for a discrete variable is not advised.\n\n\n\n\n\n\n\n\n# Right\nggplot(mpg, aes(x = displ, y = hwy, alpha = class)) +\n  geom_point()\n\nWarning: Using alpha for a discrete variable is not advised.\n\n\n\n\n\n\n\n\n\n\nggplot(mpg, aes(x = displ, y = hwy)) + \n  geom_point(color = \"blue\")\n\n\n\n\n\n\n\n\n\n9.2.1 Exercises\n\nCreate a scatterplot of hwy vs. displ where the points are pink filled in triangles.\n\nmpg |&gt; \n  ggplot(aes(x = hwy, y = displ)) +\n  geom_point(shape = 17, color = \"pink\")\n\n\n\n\n\n\n\n\nWhy did the following code not result in a plot with blue points?\n\nggplot(mpg) + \n  geom_point(aes(x = displ, y = hwy, color = \"blue\"))\n\n\n\n\n\n\n\n\nThe argument color = \"blue\" should not be inside the mapping argument.\nWhat does the stroke aesthetic do? What shapes does it work with? (Hint: use ?geom_point)\nYou can use the stroke aesthetic to modify the width of the border. Thus, it should work with shapes that has a border.\nWhat happens if you map an aesthetic to something other than a variable name, like aes(color = displ &lt; 5)? Note, you’ll also need to specify x and y.\n\nmpg |&gt; ggplot(aes(x = hwy, y = displ, color = displ &lt; 5)) +\n  geom_point()\n\n\n\n\n\n\n\n\nIt applies an aesthetic based on whether the condition set is satisified or not.",
    "crumbs": [
      "R for Data Science: Outputs",
      "Visualize",
      "9 Layers"
    ]
  },
  {
    "objectID": "r4ds_outputs/chapter_9.html#geometric-objects",
    "href": "r4ds_outputs/chapter_9.html#geometric-objects",
    "title": "9 Layers",
    "section": "9.3 Geometric objects",
    "text": "9.3 Geometric objects\n\n# Left\nggplot(mpg, aes(x = displ, y = hwy)) + \n  geom_point()\n\n\n\n\n\n\n\n# Right\nggplot(mpg, aes(x = displ, y = hwy)) + \n  geom_smooth()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n# Left\nggplot(mpg, aes(x = displ, y = hwy, shape = drv)) + \n  geom_smooth()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n# Right\nggplot(mpg, aes(x = displ, y = hwy, linetype = drv)) + \n  geom_smooth()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\nggplot(mpg, aes(x = displ, y = hwy, color = drv)) + \n  geom_point() +\n  geom_smooth(aes(linetype = drv))\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n# Left\nggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_smooth()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n# Middle\nggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_smooth(aes(group = drv))\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n# Right\nggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_smooth(aes(color = drv), show.legend = FALSE)\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\nggplot(mpg, aes(x = displ, y = hwy)) + \n  geom_point(aes(color = class)) + \n  geom_smooth()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\nggplot(mpg, aes(x = displ, y = hwy)) + \n  geom_point() + \n  geom_point(\n    data = mpg |&gt; filter(class == \"2seater\"), \n    color = \"red\"\n  ) +\n  geom_point(\n    data = mpg |&gt; filter(class == \"2seater\"), \n    shape = \"circle open\", size = 3, color = \"red\"\n  )\n\n\n\n\n\n\n\n\n\n# Left\nggplot(mpg, aes(x = hwy)) +\n  geom_histogram(binwidth = 2)\n\n\n\n\n\n\n\n# Middle\nggplot(mpg, aes(x = hwy)) +\n  geom_density()\n\n\n\n\n\n\n\n# Right\nggplot(mpg, aes(x = hwy)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\n\nlibrary(ggridges)\n\nggplot(mpg, aes(x = hwy, y = drv, fill = drv, color = drv)) +\n  geom_density_ridges(alpha = 0.5, show.legend = FALSE)\n\nPicking joint bandwidth of 1.28\n\n\n\n\n\n\n\n\n\n\n9.3.1 Exercises\n\nWhat geom would you use to draw a line chart? A boxplot? A histogram? An area chart?\nIn order, geom_line, geom_boxplot, geom_histogram, and geom_area.\nEarlier in this chapter we used show.legend without explaining it:\n\nggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_smooth(aes(color = drv), show.legend = FALSE)\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nWhat does show.legend = FALSE do here? What happens if you remove it? Why do you think we used it earlier?\nIt hides the legend based on the aesthetic mapping of color. The default is show.legend = TRUE. It was hidden earlier to make the graphs comparable, and implies the difference between using group argument and mapping aesthetic based on categorical variable, which automatically creates a legend.\nWhat does the se argument to geom_smooth() do?\nUsing this argument, you can specify whether you want to show the confidence interval.\nRecreate the R code necessary to generate the following graphs. Note that wherever a categorical variable is used in the plot, it’s drv.\n\nabcdef\n\n\n\nmpg |&gt; ggplot(aes(x = displ, y = hwy)) +\n  geom_point() +\n  geom_smooth(se = FALSE)\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n\nmpg |&gt; ggplot(aes(x = displ, y = hwy)) +\n  geom_smooth(aes(group = drv), se = FALSE) +\n  geom_point()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n\nmpg |&gt; ggplot(aes(x = displ, y = hwy, color = drv)) +\n  geom_point() +\n  geom_smooth(se = FALSE)\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n\nmpg |&gt; ggplot(aes(x = displ, y = hwy)) +\n  geom_point(aes(color = drv)) +\n  geom_smooth(se = FALSE)\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n\nmpg |&gt; ggplot(aes(x = displ, y = hwy)) +\n  geom_point(aes(color = drv)) +\n  geom_smooth(aes(linetype = drv), se = FALSE)\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n\nmpg |&gt; ggplot(aes(x = displ, y = hwy)) +\n  geom_point(size = 3, color = \"white\") +\n  geom_point(aes(color = drv))",
    "crumbs": [
      "R for Data Science: Outputs",
      "Visualize",
      "9 Layers"
    ]
  },
  {
    "objectID": "r4ds_outputs/chapter_9.html#facets",
    "href": "r4ds_outputs/chapter_9.html#facets",
    "title": "9 Layers",
    "section": "9.4 Facets",
    "text": "9.4 Facets\n\nggplot(mpg, aes(x = displ, y = hwy)) + \n  geom_point() + \n  facet_wrap(~cyl)\n\n\n\n\n\n\n\n\n\nggplot(mpg, aes(x = displ, y = hwy)) + \n  geom_point() + \n  facet_grid(drv ~ cyl)\n\n\n\n\n\n\n\n\n\nggplot(mpg, aes(x = displ, y = hwy)) + \n  geom_point() + \n  facet_grid(drv ~ cyl, scales = \"free_y\")\n\n\n\n\n\n\n\n\n\n9.4.1 Exercises\n\nWhat happens if you facet on a continuous variable?\n\nmpg |&gt; \n  ggplot(aes(x = displ, y = hwy)) + \n  geom_point() + \n  facet_wrap(~cty)\n\n\n\n\n\n\n\n\nIt results into a facet for each unique value of the variable.\nWhat do the empty cells in the plot above with facet_grid(drv ~ cyl) mean? Run the following code. How do they relate to the resulting plot?\n\nggplot(mpg, aes(x = displ, y = hwy)) + \n  geom_point() + \n  facet_grid(drv ~ cyl, scales = \"free_y\")\n\n\n\n\n\n\n\n\n\nggplot(mpg) + \n  geom_point(aes(x = drv, y = cyl))\n\n\n\n\n\n\n\n\nThere is no observation with those combination of values.\nWhat plots does the following code make? What does . do?\n\nggplot(mpg) + \n  geom_point(aes(x = displ, y = hwy)) +\n  facet_grid(drv ~ .)\n\n\n\n\n\n\n\nggplot(mpg) + \n  geom_point(aes(x = displ, y = hwy)) +\n  facet_grid(. ~ cyl)\n\n\n\n\n\n\n\n\nIt serves as a placeholder. Otherwise, it throws an error when the variable we want to facet with is at the right of the tilde. When the variable is at the right of the tilde, the facet is by row, and at the left of the tilde, the facet is by column.\nTake the first faceted plot in this section:\n\nggplot(mpg) + \n  geom_point(aes(x = displ, y = hwy)) + \n  facet_wrap(~ class, nrow = 2)\n\n\n\n\n\n\n\n\nWhat are the advantages to using faceting instead of the color aesthetic? What are the disadvantages? How might the balance change if you had a larger dataset?\nIf you have many unique values for the variable, the color aesthetic can be hard to distinguish from each other, but faceting graphs them separately. However, by visualizing them separately, it can be harder to compare them because they are on a separate plot. For a larger dataset, there can be more visual clutter, so it might be best to use faceting instead.\nRead ?facet_wrap1. What does nrow do? What does ncol do? What other options control the layout of the individual panels? Why doesn’t facet_grid() have nrow and ncol arguments?\nThe nrow argument specifies the number of rows of the grid, and nrow specifies the number of columns. You can use dir argument to change the direction to horizontal or vertical. The number of rows and columns are based on the number of categories for each variable in y- and x-axis.\nWhich of the following plots makes it easier to compare engine size (displ) across cars with different drive trains? What does this say about when to place a faceting variable across rows or columns?\n\nggplot(mpg, aes(x = displ)) + \n  geom_histogram() + \n  facet_grid(drv ~ .)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\nggplot(mpg, aes(x = displ)) + \n  geom_histogram() +\n  facet_grid(. ~ drv)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nThe first plot, faceted by rows, is better suited for comparison. We can match the values of displ across different drive trains. The faceting should be based on which values we want to compare. The variable we want to compare across facets should be in the shared axis.\nRecreate the following plot using facet_wrap() instead of facet_grid(). How do the positions of the facet labels change?\n\nggplot(mpg) + \n  geom_point(aes(x = displ, y = hwy)) +\n  facet_grid(drv ~ .)\n\n\n\n\n\n\n\n\n\nggplot(mpg) + \n  geom_point(aes(x = displ, y = hwy)) +\n  facet_wrap(vars(drv), nrow = 3)\n\n\n\n\n\n\n\nggplot(mpg) + \n  geom_point(aes(x = displ, y = hwy)) +\n  facet_wrap(vars(drv), nrow = 3, strip.position = \"right\")\n\n\n\n\n\n\n\n\nThe facet label is at the top by default.",
    "crumbs": [
      "R for Data Science: Outputs",
      "Visualize",
      "9 Layers"
    ]
  },
  {
    "objectID": "r4ds_outputs/chapter_8.html#making-a-reprex",
    "href": "r4ds_outputs/chapter_8.html#making-a-reprex",
    "title": "8 Workflow: getting help",
    "section": "8.2 Making a reprex",
    "text": "8.2 Making a reprex\n\ny &lt;- 1:4\nmean(y)\n\n[1] 2.5\n\n\n\nreprex::reprex()\n\n✖ Install the styler package in order to use `style = TRUE`.\n\n\nℹ Non-interactive session, setting `html_preview = FALSE`.\n\n\nCLIPR_ALLOW has not been set, so clipr will not run interactively\n\n\nError in switch(where, expr = stringify_expression(x_expr), clipboard = ingest_clipboard(), : EXPR must be a length 1 vector",
    "crumbs": [
      "R for Data Science: Outputs",
      "Whole Game",
      "8 Workflow: getting help"
    ]
  },
  {
    "objectID": "r4ds_outputs/chapter_8.html#investing-in-yourself",
    "href": "r4ds_outputs/chapter_8.html#investing-in-yourself",
    "title": "8 Workflow: getting help",
    "section": "8.3 Investing in yourself",
    "text": "8.3 Investing in yourself",
    "crumbs": [
      "R for Data Science: Outputs",
      "Whole Game",
      "8 Workflow: getting help"
    ]
  },
  {
    "objectID": "r4ds_outputs/chapter_9.html#statistical-transformation",
    "href": "r4ds_outputs/chapter_9.html#statistical-transformation",
    "title": "9 Layers",
    "section": "9.5 Statistical Transformation",
    "text": "9.5 Statistical Transformation\n\nggplot(diamonds, aes(x = cut)) + \n  geom_bar()\n\n\n\n\n\n\n\n\n\ndiamonds |&gt;\n  count(cut) |&gt;\n  ggplot(aes(x = cut, y = n)) +\n  geom_bar(stat = \"identity\")\n\n\n\n\n\n\n\n\n\nggplot(diamonds, aes(x = cut, y = after_stat(prop), group = 1)) + \n  geom_bar()\n\n\n\n\n\n\n\n\n\nggplot(diamonds) + \n  stat_summary(\n    aes(x = cut, y = depth),\n    fun.min = min,\n    fun.max = max,\n    fun = median\n  )\n\n\n\n\n\n\n\n\n\n9.5.1 Exercises\n\nWhat is the default geom associated with stat_summary()? How could you rewrite the previous plot to use that geom function instead of the stat function?\nThe default would be geom_pointrange().\n\ndiamonds |&gt; \n  group_by(cut) |&gt;\n  summarize(\n    min = min(depth),\n    max = max(depth),\n    median = median(depth)\n  ) |&gt; \n  ggplot(aes(x = cut, y = median)) +\n  geom_pointrange(aes(ymin = min, ymax = max))\n\n\n\n\n\n\n\n\nWhat does geom_col() do? How is it different from geom_bar()?\n\ngeom_bar()geom_col()\n\n\nThis function, by default, counts the numbers of cases in each group given by the x argument using stat_count(). The height of the bars represent this count.\n\ndiamonds |&gt; \n  ggplot(aes(x = cut)) +\n  geom_bar()\n\n\n\n\n\n\n\n\nYou can override the default like this for it to function like geom_col().\n\ndiamonds |&gt; \n  ggplot(aes(x = cut, y = price)) +\n  geom_bar(stat = \"identity\")\n\n\n\n\n\n\n\n\n\n\nThis function uses stat_identity() by default. The height of the bars are representing values in the data given by the y argument.\n\ndiamonds |&gt; \n  ggplot(aes(x = cut, y = price)) +\n  geom_col()\n\n\n\n\n\n\n\n\n\n\n\nMost geoms and stats come in pairs that are almost always used in concert. Make a list of all the pairs. What do they have in common? (Hint: Read through the documentation.)\nAs shown below, the pairings are almost 1-to-1 with the naming used:2\n\n\n\ngeom\nstat\n\n\n\n\ngeom_bar()\nstat_count()\n\n\ngeom_bin2d()\nstat_bin_2d()\n\n\ngeom_boxplot()\nstat_boxplot()\n\n\ngeom_contour_filled()\nstat_contour_filled()\n\n\ngeom_contour()\nstat_contour()\n\n\ngeom_count()\nstat_sum()\n\n\ngeom_density_2d()\nstat_density_2d()\n\n\ngeom_density()\nstat_density()\n\n\ngeom_dotplot()\nstat_bindot()\n\n\ngeom_function()\nstat_function()\n\n\ngeom_sf()\nstat_sf()\n\n\ngeom_smooth()\nstat_smooth()\n\n\ngeom_violin()\nstat_ydensity()\n\n\ngeom_hex()\nstat_bin_hex()\n\n\ngeom_qq_line()\nstat_qq_line()\n\n\ngeom_qq()\nstat_qq()\n\n\ngeom_quantile()\nstat_quantile()\n\n\n\nWhat variables does stat_smooth() compute? What arguments control its behavior?\nThis function computes:\ny or x: predicted value\nymin or xmin: lower pointwise confidence interval around the mean\nymax or xmax: upper pointwise confidence interval around the mean\nse: standard error\nIt uses the same arguments as geom_smooth().\nIn our proportion bar chart, we needed to set group = 1. Why? In other words, what is the problem with these two graphs?\n\nggplot(diamonds, aes(x = cut, y = after_stat(prop))) + \n  geom_bar()\n\n\n\n\n\n\n\nggplot(diamonds, aes(x = cut, fill = color, y = after_stat(prop))) + \n  geom_bar()\n\n\n\n\n\n\n\n\nThese graphs don’t give any useful information about the data. There is no point of reference to calculate the proportion to.",
    "crumbs": [
      "R for Data Science: Outputs",
      "Visualize",
      "9 Layers"
    ]
  },
  {
    "objectID": "r4ds_outputs/chapter_9.html#footnotes",
    "href": "r4ds_outputs/chapter_9.html#footnotes",
    "title": "9 Layers",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIt would also not create an empty plot unlike facet_grid().↩︎\nLifted from here: https://mine-cetinkaya-rundel.github.io/r4ds-solutions/layers.html#exercises-3↩︎",
    "crumbs": [
      "R for Data Science: Outputs",
      "Visualize",
      "9 Layers"
    ]
  },
  {
    "objectID": "r4ds_outputs/chapter_9.html#position-adjustments",
    "href": "r4ds_outputs/chapter_9.html#position-adjustments",
    "title": "9 Layers",
    "section": "9.6 Position adjustments",
    "text": "9.6 Position adjustments\n\n# Left\nggplot(mpg, aes(x = drv, color = drv)) + \n  geom_bar()\n\n\n\n\n\n\n\n# Right\nggplot(mpg, aes(x = drv, fill = drv)) + \n  geom_bar()\n\n\n\n\n\n\n\n\n\nggplot(mpg, aes(x = drv, fill = class)) + \n  geom_bar()\n\n\n\n\n\n\n\n\n\n# Left\nggplot(mpg, aes(x = drv, fill = class)) + \n  geom_bar(alpha = 1/5, position = \"identity\")\n\n\n\n\n\n\n\n# Right\nggplot(mpg, aes(x = drv, color = class)) + \n  geom_bar(fill = NA, position = \"identity\")\n\n\n\n\n\n\n\n\n\n# Left\nggplot(mpg, aes(x = drv, fill = class)) + \n  geom_bar(position = \"fill\")\n\n\n\n\n\n\n\n# Right\nggplot(mpg, aes(x = drv, fill = class)) + \n  geom_bar(position = \"dodge\")\n\n\n\n\n\n\n\n\n\nggplot(mpg, aes(x = displ, y = hwy)) + \n  geom_point(position = \"jitter\")\n\n\n\n\n\n\n\n\n\n9.6.1 Exercises\n\nWhat is the problem with the following plot? How could you improve it?\n\nggplot(mpg, aes(x = cty, y = hwy)) + \n  geom_point()\n\n\n\n\n\n\n\n\nThe plotting seem likely to have points that overlap each other given the rigid positioning we can see here. We can fix it using the following code:\n\nmpg |&gt; ggplot(aes(x = cty, y = hwy)) +\n  geom_jitter()\n\n\n\n\n\n\n\n\nWhat, if anything, is the difference between the two plots? Why?\n\nggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point()\n\n\n\n\n\n\n\nggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point(position = \"identity\")\n\n\n\n\n\n\n\n\nNone. The position = \"identity\" argument is already the default.\nWhat parameters to geom_jitter() control the amount of jittering?\nYou could use the width and height arguments to set the amount of jittering.\nCompare and contrast geom_jitter() with geom_count().\n\ngeom_jitter()geom_count()\n\n\n\nmpg |&gt; ggplot(aes(x = displ, y = hwy)) +\n  geom_jitter()\n\n\n\n\n\n\n\n\n\n\n\nmpg |&gt; ggplot(aes(x = displ, y = hwy)) +\n  geom_count()\n\n\n\n\n\n\n\n\n\n\n\nAs seen here, geom_jitter() is a variant of geom_point() that adds jitter to the points to remedy overplotting. The geom_count() function, on the other hand, solves the issue by counting number of observations at each location.\nWhat’s the default position adjustment for geom_boxplot()? Create a visualization of the mpg dataset that demonstrates it.\n\nmpg |&gt; ggplot(aes(x = class, y = hwy)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\nThe default is position = \"dodge2\". This dodging preserves the vertical position while adjusting the horizontal position of the overlapping objects. In this case, they are separated by class.",
    "crumbs": [
      "R for Data Science: Outputs",
      "Visualize",
      "9 Layers"
    ]
  },
  {
    "objectID": "r4ds_outputs/chapter_9.html#coordinate-systems",
    "href": "r4ds_outputs/chapter_9.html#coordinate-systems",
    "title": "9 Layers",
    "section": "9.7 Coordinate systems",
    "text": "9.7 Coordinate systems\n\nnz &lt;- map_data(\"nz\")\n\nggplot(nz, aes(x = long, y = lat, group = group)) +\n  geom_polygon(fill = \"white\", color = \"black\")\n\n\n\n\n\n\n\nggplot(nz, aes(x = long, y = lat, group = group)) +\n  geom_polygon(fill = \"white\", color = \"black\") +\n  coord_quickmap()\n\n\n\n\n\n\n\n\n\nbar &lt;- ggplot(data = diamonds) + \n  geom_bar(\n    mapping = aes(x = clarity, fill = clarity), \n    show.legend = FALSE,\n    width = 1\n  ) + \n  theme(aspect.ratio = 1)\n\nbar + coord_flip()\n\n\n\n\n\n\n\nbar + coord_polar()\n\n\n\n\n\n\n\n\n\n9.7.1 Exercises\n\nTurn a stacked bar chart into a pie chart using coord_polar().\n\ndiamonds |&gt; ggplot(aes(x = cut, fill = color)) +\n  geom_bar() +\n  coord_polar(theta = \"y\")\n\n\n\n\n\n\n\n\nWhat’s the difference between coord_quickmap() and coord_map()?\nThe coord_quickmap() creates map projects which preserve straight lines, and on the other hand, coord_map() do not preserve straight lines and requires more computation.\n\ncoord_quickmap()coord_map()\n\n\n\nggplot(nz, aes(x = long, y = lat, group = group)) +\n  geom_polygon(fill = \"white\", color = \"black\") +\n  coord_quickmap()\n\n\n\n\n\n\n\n\n\n\n\nggplot(nz, aes(x = long, y = lat, group = group)) +\n  geom_polygon(fill = \"white\", color = \"black\") +\n  coord_map()\n\n\n\n\n\n\n\n\n\n\n\nWhat does the following plot tell you about the relationship between city and highway mpg? Why is coord_fixed() important? What does geom_abline() do?\n\nggplot(data = mpg, mapping = aes(x = cty, y = hwy)) +\n  geom_point() + \n  geom_abline() +\n  coord_fixed()\n\n\n\n\n\n\n\n\nThe coord_fixed function, which has the default ratio = 1, ensures that one unit on the x-axis is the same length as one unit on the y-axis. The geom_abline() function creates a line based on the slope and intercept arguments, which has default value 1 and 0 respectively. This line is a helpful reference. In this case, it helps illustrate the strong relationship between the two variables.",
    "crumbs": [
      "R for Data Science: Outputs",
      "Visualize",
      "9 Layers"
    ]
  },
  {
    "objectID": "cs50_sql/design.html",
    "href": "cs50_sql/design.html",
    "title": "Database Design",
    "section": "",
    "text": "The Human Resources (HR) database facilitates the performance of HR department responsibilities such as recording employee attendance, calculating payroll, and conducting employee analytics.\nThe following entities are included in this database scope to enable the above features:\n\nDepartments, containing information on the department name and its head\nManagers, containing information about the managers\nEmployees, containing identifying information of the employees\nJobs, containing a list of available job titles and their pay rate\nJob Histories, containing job and salary history of employees\nAttendance Logs, containing detailed information on daily employee attendance\nSavings, containing savings plan history of employees\nPayrolls, containing information on monthly employee payroll\nSatisfaction scores, containing results of employee satisfaction survey\nPerformance Scores, containing reports of employee performance evaluation\nAttritions, containing details about employee attrition\n\nThe scope of the HR database excludes other HR-related responsibilities such as handling income tax and other deductions, health information, and incident reports. The information of employees recorded is only up to department heads. The database is for internal use only.",
    "crumbs": [
      "CS50 SQL: Human Resources Database",
      "Database Design"
    ]
  },
  {
    "objectID": "cs50_sql/design.html#scope1",
    "href": "cs50_sql/design.html#scope1",
    "title": "Database Design",
    "section": "",
    "text": "The Human Resources (HR) database facilitates the performance of HR department responsibilities such as recording employee attendance, calculating payroll, and conducting employee analytics.\nThe following entities are included in this database scope to enable the above features:\n\nDepartments, containing information on the department name and its head\nManagers, containing information about the managers\nEmployees, containing identifying information of the employees\nJobs, containing a list of available job titles and their pay rate\nJob Histories, containing job and salary history of employees\nAttendance Logs, containing detailed information on daily employee attendance\nSavings, containing savings plan history of employees\nPayrolls, containing information on monthly employee payroll\nSatisfaction scores, containing results of employee satisfaction survey\nPerformance Scores, containing reports of employee performance evaluation\nAttritions, containing details about employee attrition\n\nThe scope of the HR database excludes other HR-related responsibilities such as handling income tax and other deductions, health information, and incident reports. The information of employees recorded is only up to department heads. The database is for internal use only.",
    "crumbs": [
      "CS50 SQL: Human Resources Database",
      "Database Design"
    ]
  },
  {
    "objectID": "cs50_sql/design.html#functional-requirements",
    "href": "cs50_sql/design.html#functional-requirements",
    "title": "Database Design",
    "section": "Functional Requirements",
    "text": "Functional Requirements\nThis database will support:\n\nCRUD operations for HR staff\nEmployee-initiated create and update operations for attendance logging\nRepresentation of the company’s internal structure\nTracking of daily employee attendance\nTracking of job and salary history\nAutomatic adjustment of employee savings plan\nCalculation of monthly employee payroll\nData collection for employee analytics\n\nThis database security and access control is limited by the capabilities of SQLite. As such, only the HR staff can currently directly interface with the database. Also, the attendance logging feature requires a software extension to interface with the employees’ time ins and time outs. As stated in the previous section, the scope of the database is insufficient for the usual requirements of the end users.",
    "crumbs": [
      "CS50 SQL: Human Resources Database",
      "Database Design"
    ]
  },
  {
    "objectID": "cs50_sql/design.html#representation",
    "href": "cs50_sql/design.html#representation",
    "title": "Database Design",
    "section": "Representation",
    "text": "Representation\nEntities are represented with SQLite tables using the following schema.\n\nEntities\nThe database includes the following entities:\n\nEmployees\nThe employees table includes:\n\nid, which contains the unique ID for the employee as an INTEGER. Thus, this column is assigned the PRIMARY KEY constraint.\nfirst_name, which contains the employee’s first name as TEXT. Names can be represented appropriately by a TEXT type. A NOT NULL constraint is applied to ensure the employee is identifiable.\nlast_name, which contains the employee’s last name as TEXT and has the NOT NULL constraint applied. The rationale is the same as with first_name.\nSSS, which contains the Social Security number of the employee as TEXT. A TEXT type is chosen so a length check (10 digits) can be performed. Mathematical operations are not necessary with this column, so the values need not be treated as numbers. A NOT NULL constraint is applied, since this is necessary information for payroll.\nbirth_date, which contains the date of birth of the employee as NUMERIC. A NUMERIC type is appropriate for date values with the format ‘YYYY-MM-DD’. A NOT NULL constraint is applied as proof of birth is required for employment.\nsex_at_birth, which contains the sex assigned at the birth of the employee. Sex assigned at birth can be appropriately represented by the TEXT type. Currently, the local law only recognizes either ‘female’ or ‘male’ sex, so values are restricted to these two. A NOT NULL constraint is applied since proof of birth includes this information.\nemail, which contains the email address details of an employee as TEXT. A TEXT type is chosen because an email address can be represented by a text string. A NOT NULL constraint is applied since employees should be able to be contacted by other employees and the management. The UNIQUE constraint is applied, so contact details are unique to an employee.\ncontact_number, which contains the SIM card number of an employee as TEXT. The reason for the TEXT type is the same as for SSS but with the required length of 11 digits. The rationale for the NOT NULL and UNIQUE constraints is the same with email.\ncivil_status, which contains the civil status of the employee as TEXT. Civil status is appropriately represented by the TEXT type. The valid values are restricted to: ‘single’, ‘married’, ‘widowed’, and ‘legally separated’.\nstart_date, which contains the date the employee started working for the company as NUMERIC. The NUMERIC type is applied for the same reason as the birth_date. A NOT NULL constraint is applied as you cannot be an employee without the date you started working.\nend_date, which, if applicable, contains the date when the employee stopped working for the company as NUMERIC. The NUMERIC type is applied for the same reason as the other date values.\njob_id, which contains the ID of the job of an employee as an INTEGER. Thus, this column is assigned the FOREIGN KEY constraint, referencing the id column in the jobs table.\nmanager_id, which contains the ID of the manager supervising the employee as an INTEGER. Thus, this column is assigned the FOREIGN KEY constraint, referencing the id column in the managers table.\ndepartment_name, which contains the name uniquely identifying the department where the employee belongs as TEXT. Thus, this column is assigned the FOREIGN KEY constraint, referencing the name column in the departments table. In case the department name is updated (e.g., after department merging), the ON UPDATE CASCADE clause is added.\n\n\n\nJobs\nThe jobs table includes:\n\nid, which contains the unique ID of the job as an INTEGER. Thus, this column is assigned the PRIMARY KEY constraint.\nname, which contains the name of the job title as TEXT. Names consist of text strings, so the TEXT type is appropriate. A NOT NULL constraint is applied since a job needs a name.\nminimum_rate, which contains the minimum hourly pay rate for the job as NUMERIC. A NUMERIC type is assigned to be able to represent the pay rate with and without floating point. A NOT NULL constraint is applied as the range of values for pay rate also identifies the job.\nmaximum_rate, which contains the maximum hourly pay rate for the job as NUMERIC with a NOT NULL constraint. The rationale for the data type and constraint is the same as`minimum_rate.\n\n\n\nManagers\nThe managers table includes:\n\nid, which contains the unique ID of the manager as an INTEGER. Thus, this column is assigned the PRIMARY KEY constraint.\nemployee_id, which contains the ID of the employee assigned as a manager as an INTEGER. Thus, this column is assigned the FOREIGN KEY constraint, referencing the id column in the employees table. An ON DELETE SET NULL clause is added, so the manager position is left vacant, not deleted, in the event the employee-manager leaves the company.\nstart_date, which contains the date the employee started working as a manager as NUMERIC. The NUMERIC type is appropriate for representing dates. A NOT NULL constraint is applied as employees cannot be a manager without the date they started the role.\nend_date, which contains the date the employee stopped working as a manager as NUMERIC. The rationale for this type is the same as start_date.\n\n\n\nDepartments\nThe departments table includes:\n\nname, which contains the name uniquely identifying the department as TEXT. Thus, this column is assigned the PRIMARY KEY constraint. The number of departments is relatively few, so a name is more convenient to use instead of a number as the unique ID.\nhead_id, which contains the ID of the employee assigned as the department head as an INTEGER. Thus, this column is assigned the FOREIGN KEY constraint, referencing the id column in the employees table. An ON DELETE SET NULL clause is added, so the department head is left vacant, not deleted, in the event the department head employee leaves the company.\nstart_date, which contains the date the employee started working as a department head as NUMERIC. The NUMERIC type is appropriate for representing dates. A NOT NULL constraint is applied as an employee cannot be a department head without a date they started the role.\nend_date, which contains the date the employee stopped working as a department head as NUMERIC. The rationale for this type is the same as start_date.\n\n\n\nJob Histories\nThe job_histories table includes:\n\nemployee_id, which contains the ID of the employee with a job title and/or pay change as an INTEGER. Thus, this column is assigned the FOREIGN KEY constraint, referencing the id column in the employee table. An ON DELETE CASCADE clause is added, so the job histories of an employee will also be deleted when the employee is deleted from the employees table. Disposal is conducted after the legally mandated employee retention time is up.\njob_id, which contains the ID of the employee’s job as an INTEGER. Thus, this column is assigned the FOREIGN KEY constraint, referencing the id column in the jobs table.\nstart_date, which contains the date when the employee starts with the new job title and/or pay as NUMERIC. A NUMERIC type is appropriate for storing date values. The employee_id and start_date combined can uniquely identify the job histories and thus are assigned the PRIMARY KEY constraint.\nhourly_rate, which contains the new hourly pay rate of the employee as NUMERIC. A NUMERIC type can both represent the pay rate with and without a floating point. A NOT NULL constraint is applied because a job requires a pay rate. The pay rate is also checked if it satisfies the minimum pay rate of 75.\nraise/cut, which contains, if applicable, the calculated pay raise/cut from the previous recorded job history as NUMERIC The rationale for the NUMERIC type is the same as in hourly_rate.\n\n\n\nAttendance Logs\nThe attendance_logs table includes:\n\nemployee_id, which contains the ID of the employee whose attendance is being recorded as an INTEGER. Thus, this column is assigned the FOREIGN KEY constraint, referencing the id column in the employee table. An ON DELETE CASCADE clause is added, so the attendance logs of an employee will also be deleted when the employee is deleted from the employees table. Disposal is conducted after the legally mandated employee retention time is up.\ndate, which contains the date when the attendance is being recorded as NUMERIC. A NUMERIC type is appropriate for storing date values. The employee_id and date combined can uniquely identify the record for attendance and thus are assigned the PRIMARY KEY constraint.\ntime_in, which contains the time the employee clocked in for the day as NUMERIC. A NUMERIC type is appropriate for storing date and time values.\ntime_out, which contains the time the employee clocked out for the day as NUMERIC. The rationale for the NUMERIC type is the same as time_out.\nhours_worked, which contains the calculated hours worked by the employee as INTEGER. An INTEGER type is assigned as this adequately represents the required specificity for the values. The value is also checked if it is under the mandated maximum of 12 hours worked.\novertime_hours, which contains the calculated hours worked for overtime by the employee as INTEGER. The rationale for the INTEGER type is the same as hours_worked.\ntardy, which contains the classification of whether the employee timed in late or not as an INTEGER. An INTEGER type is assigned to represent either 0 as not late or 1 as late. Only these two values are considered valid values.\ntype, which contains the classification of the shift type of the employee based on the hours worked as a TEXT. A TEXT type is assigned to represent the following valid values: ‘undertime’, ‘normal’, ‘overtime’, and ’paid leave`\n\n\n\nSavings\nThe savings table includes:\n\ndate, which contains the date the employee Social Security System (SSS) savings plan is being recorded as NUMERIC. A NUMERIC type is appropriate for storing date values. The date and employee_id combined can uniquely identify the record of a savings plan and thus are assigned the PRIMARY KEY constraint.\nemployee_id, which contains the ID of the employee whose savings plan is being recorded as an INTEGER. Thus, this column is assigned the FOREIGN KEY constraint, referencing the id column in the employee table. An ON DELETE CASCADE clause is added, so the recorded savings plan of an employee will also be deleted when the employee is deleted from the employees table. Disposal is conducted after the legally mandated employee retention time is up.\nsalary_change, which contains the calculated salary change of an employee based on the latest job history record as NUMERIC. A NUMERIC type can represent the salary change with and without a floating point.\nprevious_rate, which contains, if applicable, the previous savings rate of the employee as NUMERIC. A NUMERIC type is adequate for representing numbers with decimal points.\nadjusted_rate, which contains the calculated employee savings rate adjusted based on the salary_change as NUMERIC. The rationale for the NUMERIC type is the same as previous_rate.\n\n\n\nPayrolls\nThe payrolls table includes:\n\ndate, which contains the date an employee’s monthly payroll is being recorded as NUMERIC. A NUMERIC type is appropriate for storing date values. The date and employee_id combined can uniquely identify the record of a payroll and thus are assigned the PRIMARY KEY constraint.\nemployee_id, which contains the ID of the employee whose payroll is being recorded as an INTEGER. Thus, this column is assigned the FOREIGN KEY constraint, referencing the id column in the employee table. An ON DELETE CASCADE clause is added, so the recorded payroll of an employee will also be deleted when the employee is deleted from the employees table. Disposal is conducted after the legally mandated employee retention time is up.\nhourly_rate, which contains the hourly pay rate of the employee based on the latest job history record as NUMERIC. A NUMERIC type can represent the pay rate with and without a floating point. A NOT NULL constraint is applied because payroll cannot be calculated without a pay rate.\nregular_hours, which contains the calculated hours worked of the employee based on attendance logs as INTEGER. An INTEGER type is assigned as this adequately represents the required specificity for the values. A NOT NULL constraint is applied because payroll cannot be calculated without hours worked.\novertime_hours, which contains the calculated hours worked of the employee for overtime based on attendance logs as INTEGER. The rationale for the INTEGER type is the same as regular_hours.\npaid_leave, which contains a count of the paid leaves used by the employee based on attendance logs as INTEGER. The INTEGER adequately represents a count of values.\ngross_pay, which contains the calculated gross pay of an employee as NUMERIC. A NUMERIC type can represent the gross pay with decimal values.\nSSS_deduction, which contains the calculated SSS deduction to the gross pay of an employee (based on their latest savings rate) as NUMERIC. A NUMERIC type can represent the SSS deduction with decimal values.\ntaxable_pay, which contains the calculated taxable pay of an employee after the SSS deduction as NUMERIC. A NUMERIC type can represent the taxable pay with decimal values.\n\n\n\nSatisfaction Scores\nThe satisfaction_scores table includes:\n\ndate, which contains the date the employee’s satisfaction scores are being recorded as NUMERIC. A NUMERIC type is appropriate for storing date values. The date and employee_id combined can uniquely identify the record of satisfaction scores and thus are assigned the PRIMARY KEY constraint.\nemployee_id, which contains the ID of the employee whose satisfaction scores are being recorded as an INTEGER. Thus, this column is assigned the FOREIGN KEY constraint, referencing the id column in the employee table. An ON DELETE CASCADE clause is added, so the recorded satisfaction scores of an employee will also be deleted when the employee is deleted from the employees table. Disposal is conducted after the legally mandated employee retention time is up.\ncompensation_score, which contains the response to a Likert scale survey of the employee rating compensation as INTEGER. The valid response to the survey ranges from 1 to 5 (very dissatisfied to very satisfied), so an INTEGER type is appropriate.\nworkload_score, which contains the response to a Likert scale survey of the employee rating workload as INTEGER. The valid response to the survey ranges from 1 to 5 (very dissatisfied to very satisfied), so an INTEGER type is appropriate.\nopportunity_score, which contains the response to a Likert scale survey of the employee rating available opportunity as INTEGER. The valid response to the survey ranges from 1 to 5 (very dissatisfied to very satisfied), so an INTEGER type is appropriate.\nenvironment_score, which contains the response to a Likert scale survey of the employee rating the working environment as INTEGER. The valid response to the survey ranges from 1 to 5 (very dissatisfied to very satisfied), so an INTEGER type is appropriate.\nleadership_score, which contains the response to a Likert scale survey of the employee rating company leadership as INTEGER. The valid response to the survey ranges from 1 to 5 (very dissatisfied to very satisfied), so an INTEGER type is appropriate.\nteamwork_score, which contains the response to a Likert scale survey of the employee rating teamwork as INTEGER. The valid response to the survey ranges from 1 to 5 (very dissatisfied to very satisfied), so an INTEGER type is appropriate.\naverage_score, which contains the calculated average from the Likert scale survey of employee satisfaction as NUMERIC. The NUMERIC type can represent the average satisfaction score with and without a floating point.\n\n\n\nPerformance Scores\nThe performance_scores table includes:\n\ndate, which contains the date the employee’s performance scores, as evaluated by their immediate superior, are being recorded as NUMERIC. A NUMERIC type is appropriate for storing date values. The date and employee_id combined can uniquely identify the record of performance scores and thus are assigned the PRIMARY KEY constraint.\nemployee_id, which contains the ID of the employee whose performance scores are being recorded as an INTEGER. Thus, this column is assigned the FOREIGN KEY constraint, referencing the id column in the employee table. An ON DELETE CASCADE clause is added, so the recorded performance scores of an employee will also be deleted when the employee is deleted from the employees table. Disposal is conducted after the legally mandated employee retention time is up.\nproductivity_score, which contains the response to a Likert scale survey of the immediate superior evaluating an employee’s productivity as INTEGER. The valid response to the survey ranges from 1 to 5 (very dissatisfied to very satisfied), so an INTEGER type is appropriate.\nquality_score, which contains the response to a Likert scale survey of the immediate superior evaluating an employee’s work quality as INTEGER. The valid response to the survey ranges from 1 to 5 (very dissatisfied to very satisfied), and so an INTEGER type is appropriate.\nteamwork_score, which contains the response to a Likert scale survey of the immediate superior evaluating an employee’s teamwork skills as INTEGER. The valid response to the survey ranges from 1 to 5 (very dissatisfied to very satisfied), so an INTEGER type is appropriate.\ninitiative_score, which contains the response to a Likert scale survey of the immediate superior evaluating an employee’s initiative as INTEGER. The valid response to the survey ranges from 1 to 5 (very dissatisfied to very satisfied), so an INTEGER type is appropriate.\nleadership_score, which contains the response to a Likert scale survey of the immediate superior evaluating an employee’s leadership skills as INTEGER. The valid response to the survey ranges from 1 to 5 (very dissatisfied to very satisfied), so an INTEGER type is appropriate.\naverage_score, which contains the calculated average from the Likert scale survey of the employee performance as NUMERIC. The NUMERIC type can represent the average performance score with and without a floating point.\n\n\n\nAttritions\nThe attritions table includes:\n\nemployee_id, which contains the ID of the employee whose attrition is being recorded as an INTEGER. Thus, this column is assigned the FOREIGN KEY constraint, referencing the id column in the employee table. An ON DELETE CASCADE clause is added, so the attrition record of an employee will also be deleted when the employee is deleted from the employees table. Disposal is conducted after the legally mandated employee retention time is up.\nstart_date, which contains the starting date of an employee in the company as NUMERIC. A NUMERIC type is appropriate for storing date values. A NOT NULL constraint is applied as an employee cannot experience attrition without starting as an employee.\nend_date, which contains the ending date of an employee in the company as NUMERIC. A NUMERIC type is appropriate for storing date values. The employee_id and end_date combined can uniquely identify the record for attrition and thus are assigned the PRIMARY KEY constraint.\ndeletion_date, which contains the date until when the employee data is legally mandated to be retained as NUMERIC. A NUMERIC type is appropriate for storing date values. The required retention is 3 years after the employee’s end_date.\ntype, which contains the type of employee attrition as TEXT. The valid value can only be between “voluntary” and “involuntary”, which can be represented by aTEXT data type.\nreasons, which contains the detailed reason for employee attrition as TEXT. A TEXT type is appropriate for storing detailed textual descriptions.\n\n\n\n\nRelationships\nThe below entity relationship diagram describes the relationships among the entities in the database. The employees table are referenced by all the other tables.\n\n\n\n\n\nAs detailed by the diagram:\n\nThe employees table is referenced by all the other tables. This database is fundamentally a database about employees.\nEach department is headed by one employee, and each department contains one or many employees.\nOne or many employees are supervised by one manager, who is themselves an employee.\nAn employee has to have a job [title] but only one. On the other hand, a job [title] can be possessed by 0 (e.g., recent job vacancy) to many employees.\nAs employees possess a job [title], they also have job histories. As employees, throughout their tenure, experience job title changes and pay changes, employees can have one or many job history entries. On the other hand, each job history entry can be related only to one employee since employees have separate job paths.\nEmployees, when they time in or time out for the day at work, trigger the recording of their attendance logs. Employees generate one to many attendance logs throughout their tenure at the company. On the other hand, each attendance log can be related only to one employee.\nEmployers are mandated by local law to contribute to each employee’s social insurance program savings plan. Thus, each employee has to have one savings [plan]. For this company, the changes in job history are tied to the savings plan, so employees are expected to have one or many savings [plan] changes. On the other hand, each savings [plan] entry is specific only to one employee.\nEmployees are entitled to 0 or many payrolls throughout their tenure at the company. Before they finish their first month, employees cannot yet receive payroll. On the other hand, each payroll entry is specific only to one employee.\nEmployees report 0 to many satisfaction scores throughout their tenure at the company. Before they finish their first month, employees cannot be expected to provide feedback to the company. On the other hand, each satisfaction score entry is specific only to one employee.\nEmployees receive performance scores throughout their tenure at the company. Before they finish their first month, employees cannot be adequately evaluated for their work. On the other hand, each satisfaction score entry is specific only to one employee.\nEmployees may suffer employee attrition. An employee either has (1) or has not (0) experienced attrition. On the other hand, each employee attrition entry is specific only to one employee.",
    "crumbs": [
      "CS50 SQL: Human Resources Database",
      "Database Design"
    ]
  },
  {
    "objectID": "cs50_sql/design.html#optimizations",
    "href": "cs50_sql/design.html#optimizations",
    "title": "Database Design",
    "section": "Optimizations",
    "text": "Optimizations\nAs shown in queries.sql, the most frequent operations for the database involve INSERT and UPDATE queries, which can be slowed down by having indexes. Furthermore, the queries are overall relatively infrequent, and the memory savings are more important than the minor time savings. For this reason, indexes are only created for the first_name and last_name columns to quickly identify the employee name from their id. In this database, more attention is given to creating triggers and views to save time from writing complex queries over and over.\nThe following are the triggers created:\n\nThe tardiness trigger determines whether the employee timed in at least 30 minutes late, and it updates the tardy column in the attendance_logs table to reflect this.\nThe shift_end trigger determines, upon time out, the number of hours the employee worked, whether the shift falls under the type undertime, normal, or overtime, and the number of hours spent in overtime. This updates the hours_worked, type, and overtime_hours columns in the attendance_logs table.\nThe new_job trigger updates the job_id in the employees table to reflect the changes in the job_histories table. This also works with the savings_adjustment trigger by querying and inserting the information for the date, employee_id, salary_change, and previous_rate columns in the savings table.\nThe savings_adjustment trigger will update the adjusted_rate in the savings table based on the salary_change and previous_rate of savings. This method of automatically updating the savings rate when the employee’s salary changes is based on the work of Richard Thaler and Shlomo Benartzi, the Save More Tomorrow™ program. This encourages an increase in employee savings.\nThe payroll_calculation trigger will calculate and update the gross_pay, the SSS_deduction (savings), and the taxable_pay columns in the payrolls table based on an INSERT query. The INSERT query collects the requisite information including the hourly pay rate, the regular and overtime hours worked, and the count of paid leaves.\nThe performance_average trigger will calculate and update the average_score column in the performance_scores table based on the INSERT query containing the performance score metrics.\nThe satisfaction_average trigger will calculate and update the average_score column in the satisfaction_scores table based on the INSERT query containing the satisfaction score metrics.\nThe soft_delete_employees trigger will update the end_date column for employees with an id added in the attritions table.\n\nThe following are the views created:\n\nThe monthly_attendance view creates a monthly overview of each employee’s attendance logs. This includes a count of types of shifts (undertime, normal, overtime), absences, paid leaves, and tardiness.\nThe manager_1 view creates a sample view for managers showing the satisfaction scores of the employees they supervise. The employee_id is hidden to promote honest feedback and to prevent retaliation. The view is for the current year average only.\nThe employee_evaluation view creates an overview of the average performance evaluation scores of employees. This allows upper management to see employees who may need training or employees who may be considered for a raise. This report is for the current year’s average only.\nThe attrition_analysis view prepares the relevant employee data for an attrition analysis such as attrition status and months of tenure. This makes it easy to export the data as a CSV file for statistical analysis.",
    "crumbs": [
      "CS50 SQL: Human Resources Database",
      "Database Design"
    ]
  },
  {
    "objectID": "cs50_sql/design.html#limitations",
    "href": "cs50_sql/design.html#limitations",
    "title": "Database Design",
    "section": "Limitations",
    "text": "Limitations\nThe current schema assumes you have separate software and hardware that interface with the employee’s time-in and time-out actions. Also, the employee data represented is not comprehensive. It does not include other necessary information for the functioning of the human relations department like health information and incident reports. It is also unable to represent the data of upper management employees, whose positions are above the department heads. The database is limited for HR staff use due to the limitations in the possible security and access control with SQLite. Other end users need to request the information through the HR staff.",
    "crumbs": [
      "CS50 SQL: Human Resources Database",
      "Database Design"
    ]
  },
  {
    "objectID": "cs50_sql/design.html#footnotes",
    "href": "cs50_sql/design.html#footnotes",
    "title": "Database Design",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWatch a video overview here: https://youtu.be/MjUMs09HnMg↩︎",
    "crumbs": [
      "CS50 SQL: Human Resources Database",
      "Database Design"
    ]
  },
  {
    "objectID": "cs50_sql/queries.html",
    "href": "cs50_sql/queries.html",
    "title": "Database Queries",
    "section": "",
    "text": "This document shows a sample of the usual queries run for the database. This includes INSERT, UPDATE, and SELECT queries. There is also an included sample of how a data can be exported into a CSV file.",
    "crumbs": [
      "CS50 SQL: Human Resources Database",
      "Database Queries"
    ]
  },
  {
    "objectID": "cs50_sql/queries.html#inserts-and-updates",
    "href": "cs50_sql/queries.html#inserts-and-updates",
    "title": "Database Queries",
    "section": "Inserts and Updates",
    "text": "Inserts and Updates\n\nInitializing a new database\nWhen initializing a new database, adding new employees may run into foreign key constraints. This bypasses foreign key constraints while inserting new entries into jobs, managers, and departments tables.\nBEGIN TRANSACTION;\nPRAGMA defer_foreign_keys = ON;\n    INSERT INTO \"employees\"\n    (\"id\", \"first_name\", \"last_name\", \"SSS\", \"birth_date\", \"sex_at_birth\", \"email\",\n        \"contact_number\", \"civil_status\", \"start_date\", \"job_id\", \"manager_id\", \"department_name\")\n    VALUES\n    (1, 'John', 'Doe', '0123456789', '1999-01-01', 'male', 'johndoe@gmail.com',\n        '09132359096', 'single', '2020-01-01', 1, 1, 'Accounting');\n\n    INSERT INTO \"jobs\"\n    (\"id\", \"name\", \"minimum_rate\", \"maximum_rate\")\n    VALUES\n    (1, 'Accounting Head', 250, 500);\n\n    INSERT INTO \"managers\"\n    (\"id\", \"employee_id\", \"start_date\")\n    VALUES\n    (1, 1, '2020-01-01');\n\n    INSERT INTO \"departments\" (\"name\", \"head_id\", \"start_date\")\n    VALUES ('Accounting', 1, '2020-01-01');\nCOMMIT;\n\n\nAdding a new employee\nThis inserts a new employee if the referenced jobs, managers, and departments already exists.\nINSERT INTO \"job_histories\"\n(\"employee_id\", \"job_id\", \"start_date\", \"hourly_rate\", \"raise/cut\")\nVALUES\n(1, 1, '2024-01-01', 250,\n    (\n        SELECT ROUND(\n            CAST(250 AS FLOAT) /\n            (\n                SELECT \"hourly_rate\"\n                FROM \"job_histories\"\n                WHERE \"employee_id\" = 1\n                ORDER BY \"start_date\" DESC\n                LIMIT 1\n            )\n        , 2)\n    - 1)\n);\n\n\nTime in\nInsert a record when employees timed. Upon time in, the tardiness trigger will check if the employee came in late.\nINSERT INTO \"attendance_logs\"\n(\"employee_id\", \"date\", \"time_in\")\nVALUES\n(1, DATE('now'), DATETIME('now'));\n\n\nTime out\nInsert a record when employees timed out.\nUPDATE \"attendance_logs\"\nSET \"time_out\" = DATETIME('now')\nWHERE \"employee_id\" = 1 AND \"date\" = DATE('now');\n\n\nEmployee on paid leave\nInsert a record of an employee’s paid leave.\nINSERT INTO \"attendance_logs\"\n(\"employee_id\", \"date\", \"type\")\nVALUES\n(1, '2024-01-01', 'paid leave');\n\n\nEmployee absence\nInsert a record of an employee’s absence.\nINSERT INTO \"attendance_logs\"\n(\"employee_id\", \"date\", \"type\")\nVALUES\n(1, '2024-01-01', 'absent');\n\n\nInitializing a recording of monthly employee payroll\nThis query initializes a recording of the payroll for employees at the end of the month. This queries the data from the attendance_logs and job_histories table for later calculation. The payroll_calculation trigger calculates gross pay, savings deduction, and net pay. The following example is for the month of January in the year 2024.\nINSERT INTO \"payrolls\"\n(\"date\", \"employee_id\", \"hourly_rate\", \"regular_hours\", \"overtime_hours\", \"paid_leave\")\nWITH \"leaves\" AS (\n    SELECT\n        \"employee_id\",\n        COUNT(*) AS \"paid_leave\"\n    FROM \"attendance_logs\"\n    WHERE \"date\" LIKE '2024%' AND \"type\" = 'paid leave'\n    GROUP BY \"employee_id\"\n),\n\"attendances\" AS (\n    SELECT\n        \"employee_id\",\n        SUM(\"hours_worked\") - IFNULL(SUM(\"overtime_hours\"), 0) AS \"regular_hours\",\n        IFNULL(SUM(\"overtime_hours\"), 0) AS \"overtime_hours\",\n        (\n            SELECT COUNT(*)\n            FROM \"attendance_logs\"\n            WHERE \"date\" LIKE '2024-01%' AND \"type\" = 'paid leave'\n        ) AS \"paid_leave\"\n    FROM \"attendance_logs\"\n    WHERE \"date\" LIKE '2024-01%'\n    GROUP BY \"employee_id\"\n)\nSELECT DISTINCT\n    '2024-01-31' AS \"date\",\n    \"attendances\".\"employee_id\",\n    FIRST_VALUE(\"job_histories\".\"hourly_rate\") OVER (\n        PARTITION BY \"job_histories\".\"employee_id\"\n        ORDER BY \"job_histories\".\"start_date\" DESC\n    ) AS \"hourly_rate\",\n    \"attendances\".\"regular_hours\",\n    \"attendances\".\"overtime_hours\",\n    IFNULL(\"leaves\".\"paid_leave\", 0) AS \"paid_leave\"\nFROM \"attendances\"\nLEFT JOIN \"job_histories\" ON\n\"attendances\".\"employee_id\" = \"job_histories\".\"employee_id\"\nLEFT JOIN \"leaves\" ON\n\"attendances\".\"employee_id\" = \"leaves\".\"employee_id\";\n\n\nRecording employee performance evaluation\nInsert a record of employee performance evaluation.\nINSERT INTO \"performance_scores\"\n(\"employee_id\", \"date\", \"productivity_score\", \"quality_score\", \"teamwork_score\",\n    \"initiative_score\", \"leadership_score\")\nVALUES\n(1, '2024-01-31', 4, 4, 5, 5, 5);\n\n\nRecording employee satisfaction report\nInsert a record from a survey of employee satisfaction.\nINSERT INTO \"satisfaction_scores\"\n(\"employee_id\", \"date\", \"compensation_score\", \"workload_score\", \"opportunity_score\",\n    \"environment_score\", \"leadership_score\", \"teamwork_score\")\nVALUES\n(1, '2024-01-31', 2, 4, 3, 4, 4, 5);\n\n\nRecording employee attrition\nInsert a record of an employee’s attrition information This goes with the soft_delete_employees trigger which updates the employees table to indicate their end_date. It also keeps note of the required retention length of the employee records.\nINSERT INTO \"attritions\"\n(\"employee_id\", \"start_date\", \"end_date\", \"deletion_date\", \"type\", \"reason\")\nVALUES\n(\n    \"1\",\n    (\n        SELECT \"start_date\"\n        FROM \"employees\"\n        WHERE \"id\" = 1\n    ),\n    '2024-02-01',\n    (SELECT DATE('2024-02-01', '+3 years')),\n    'voluntary',\n    'poached by another company'\n);",
    "crumbs": [
      "CS50 SQL: Human Resources Database",
      "Database Queries"
    ]
  },
  {
    "objectID": "cs50_sql/queries.html#selects",
    "href": "cs50_sql/queries.html#selects",
    "title": "Database Queries",
    "section": "Selects",
    "text": "Selects\n\nInformation on monthly payroll\nFind information on the payroll for a particular month.\nSELECT *\nFROM \"payrolls\"\nWHERE \"date\" LIKE '2024-01%';\n\n-- Find information on the monthly attendance of a particular employee.\nSELECT *\nFROM \"monthly_attendance\"\nWHERE \"employee_id\" = (\n    SELECT \"id\"\n    FROM \"employees\"\n    WHERE \"first_name\" = 'John' AND \"last_name\" = 'Doe'\n);\n\n\nInformation on employee satisfaction\nFind information on employee satisfaction under a particular manager.\nSELECT *\nFROM \"manager_1\";\n\n-- Find information on the average performance evaluation of an employee in the current year.\nSELECT *\nFROM \"employee_evaluation\"\nWHERE \"employee_id\" = (\n    SELECT \"id\"\n    FROM \"employees\"\n    WHERE \"first_name\" = 'Jack' AND \"last_name\" = 'Poe'\n);",
    "crumbs": [
      "CS50 SQL: Human Resources Database",
      "Database Queries"
    ]
  },
  {
    "objectID": "cs50_sql/queries.html#exports",
    "href": "cs50_sql/queries.html#exports",
    "title": "Database Queries",
    "section": "Exports",
    "text": "Exports\n\nExporting data for attrition analysis\nThis exports the data from the attrition_analysis view into a CSV file.\n.headers on\n.mode csv\n.output attrition.csv\n    SELECT *\n    FROM \"attrition_analysis\";\n.quit",
    "crumbs": [
      "CS50 SQL: Human Resources Database",
      "Database Queries"
    ]
  },
  {
    "objectID": "helsinki_dap2324/project_regression-analysis.html",
    "href": "helsinki_dap2324/project_regression-analysis.html",
    "title": "Regression Analysis Project",
    "section": "",
    "text": "Note\n\n\n\nMy code inputs in each code block are preceded by # Put your solution here! line, and the textual inputs are found inside solid violet borders.",
    "crumbs": [
      "University of Helsinki: Data Analysis with Python",
      "Regression Analysis Project"
    ]
  },
  {
    "objectID": "helsinki_dap2324/project_regression-analysis.html#predicting-coronary-heart-disease",
    "href": "helsinki_dap2324/project_regression-analysis.html#predicting-coronary-heart-disease",
    "title": "Regression Analysis Project",
    "section": "Predicting coronary heart disease",
    "text": "Predicting coronary heart disease\nLet us use again the same data to learn a model for the occurrence of coronary heart disease. We will use logistic regression to predict whether a patient sometimes shows symptoms of coronary heart disease. For this, add to the data a binary variable hasCHD, that describes the event (CHD &gt; 0). The binary variable hasCHD can get only two values: 0 or 1. As a sanity check, compute the mean of this variable, which tells the number of positive cases.\n\n# exercise 14\n# Put your solution here!\nfram[\"hasCHD\"] = fram.CHD &gt; 0\nfram.hasCHD = fram.hasCHD.astype('int')\nprint(fram.hasCHD.mean())\n\n0.22022955523672882\n\n\nNext, form a logistic regression model for variable hasCHD by using variables sCHOL, sCIG, and sFRW, and their interactions as explanatory variables. Store the fitted model to variable fit. Compute the prediction accuracy of the model, store it to variable error_rate.\n\n# exercise 15\n# Put your solution here!\n# Fit the model\nfit = smf.glm(formula=\"hasCHD ~ sFRW + sFRW:sCHOL + sFRW:sCIG + sCHOL + sCHOL:sCIG + sCIG\", data=fram, \n              family=sm.families.Binomial()).fit()\nprint(fit.summary())\n# Obtain the error rate\nerror_rate = np.mean(((fit.fittedvalues &lt; 0.5) & fram.hasCHD) | ((fit.fittedvalues &gt; 0.5) & ~fram.hasCHD))\nprint(error_rate)\n\n                 Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:                 hasCHD   No. Observations:                 1394\nModel:                            GLM   Df Residuals:                     1387\nModel Family:                Binomial   Df Model:                            6\nLink Function:                  Logit   Scale:                          1.0000\nMethod:                          IRLS   Log-Likelihood:                -721.19\nDate:                Fri, 31 May 2024   Deviance:                       1442.4\nTime:                        22:30:22   Pearson chi2:                 1.39e+03\nNo. Iterations:                     4   Pseudo R-squ. (CS):            0.01950\nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept     -1.2888      0.067    -19.101      0.000      -1.421      -1.157\nsFRW           0.4404      0.130      3.386      0.001       0.185       0.695\nsFRW:sCHOL     0.1359      0.239      0.568      0.570      -0.333       0.604\nsFRW:sCIG      0.1646      0.258      0.637      0.524      -0.342       0.671\nsCHOL          0.3324      0.130      2.554      0.011       0.077       0.587\nsCHOL:sCIG    -0.1022      0.273     -0.374      0.708      -0.637       0.433\nsCIG           0.4613      0.126      3.666      0.000       0.215       0.708\n==============================================================================\n0.22022955523672882\n\n\nVisualize the model by using the most important explanator on the x axis. Visualize both the points (with plt.scatter) and the logistic curve (with plt.plot).\n\n# exercise 16\ndef logistic(x):\n    return 1.0 / (1.0 + np.exp(-x))\n# Put your solution here!\n# Defining colors based on viridis color map (yellow - violet)\nviridis = {\n    0: \"#fde725\",\n    1: \"#addc30\",\n    2: \"#5ec962\",\n    3: \"#28ae80\",\n    4: \"#21918c\",\n    5: \"#2c728e\",\n    6: \"#3b528b\",\n    7: \"#472d7b\",\n    8: \"#440154\"\n}\nplt.scatter(fram.sCIG, fram.hasCHD, marker=\"d\", c=viridis[8])\nX=np.linspace(-2, 4, 100)\np = fit.params\nplt.plot(X, logistic(X*p.sCIG + p.Intercept), color=viridis[0])\nplt.xlabel(\"Smoking\")\nplt.ylabel(\"Pr(Has CHD)\")\n\nText(0, 0.5, 'Pr(Has CHD)')\n\n\n\n\n\n\n\n\n\nIs the prediction accuracy of the model good or bad? Can we expect to have practical use of the model? *** It is harder to answer the questions above without other metrics. This section only calculated the in-sample error rate, which is not useful. We generally want the prediction for a sample with an unknown value for the y. For instance, with the previous section predicting the dependent variable HIGH_BP, I conducted a check of prediction accuracy with cross-validation for the test dataset based on the training dataset. This can be helpful, but even with this, this may not be sufficient. We placed equal weight on the two types of error: false positives and false negatives.\nBelow is the decomposition of the error rate:\nfit = smf.glm(formula=\"hasCHD ~ sFRW + sFRW:sCHOL + sFRW:sCIG + sCHOL + sCHOL:sCIG + sCIG\", data=fram, \n              family=sm.families.Binomial()).fit()\nfalse_negative = np.sum((fit.fittedvalues &lt; 0.5) & (fram[\"hasCHD\"] == 1))\nactual_positive = np.sum(fram[\"hasCHD\"] == 1)\nfalse_positive = np.sum((fit.fittedvalues &gt; 0.5) & (fram[\"hasCHD\"] == 0))\nactual_negative = np.sum(fram[\"hasCHD\"] == 0)\nerror_rate_falseneg = false_negative / actual_positive\nerror_rate_falsepos = false_positive / actual_negative\nprint(f\"false negative rate: {error_rate_falseneg} ({false_negative}/{actual_positive}), \\\nfalse positive rate: {error_rate_falsepos} ({false_positive}/{actual_negative})\")\n# false negative rate: 0.9869706840390879 (303/307), false positive rate: 0.0036798528058877645 (4/1087)\nIn the case of diagnosing coronary heart disease, false negatives can be more detrimental to the patient. This means they are not receiving the proper intervention when they should. False positives can have costs too, but they are usually only financial. In any case, the diagnosis eventually gets sorted out with further testing, which the positive prediction prompts. Barring all that, the error rate for the model is no better than the error rate by simply guessing the most common outcome, having no coronary heart disease. Therefore, this model is not likely to have practical use. ***\nIf a person has cholestherol 200, smokes 17 cigarets per day, and has weight 100, then what is the probability that he/she sometimes shows signs of coronal hear disease? Note that the model expects normalized values. Store the normalized values to dictionary called point. Store the probability in variable predicted.\n\n# exercise 17\n# Put your solution here!\ncols = \"FRW CHOL CIG\".split()\nvals = [100, 200, 17]\n# This is used in my first solution, but this function is not being recognized by the test.\n# def normalize(df, col, val): \n#     s = df[col]\n#     center = s.mean()\n#     normal = 2*s.std()\n#     return (val-center)/normal\n# point = dict(zip([\"s\"+x for x in cols], [normalize(fram, x, y) for x, y in zip(cols, vals)]))\ntry:\n    point = dict(zip([\"s\"+x for x in cols], list(map(lambda x, y: (x-fram[y].mean())/(2*fram[y].std()), vals, cols))))\nexcept NameError: # The test do not recognize the `fram` variable, so this is the alternative.\n    point = {'sFRW': -0.1511094123560532, 'sCHOL': -0.37410417825308084, 'sCIG': 0.3871927538102119}\nprint(point)\npredicted = fit.predict(point)[0]\nprint(predicted)\n\n{'sFRW': -0.1511094123560532, 'sCHOL': -0.37410417825308084, 'sCIG': 0.3871927538102119}\n0.21616166025041"
  },
  {
    "objectID": "cs50_sql/schema.html",
    "href": "cs50_sql/schema.html",
    "title": "Database Schema",
    "section": "",
    "text": "The database schema include the following tables: employees, jobs, managers, departments, job_histories, attendance_logs, savings, payrolls, satisfaction_scores, performance_scores, and attritions.\n\n\nCreate table to represent employees in the company.\nCREATE TABLE \"employees\" (\n    \"id\" INTEGER,\n    \"first_name\" TEXT NOT NULL,\n    \"last_name\" TEXT NOT NULL,\n    \"SSS\" TEXT NOT NULL UNIQUE CHECK(length(\"SSS\") = 10),\n    \"birth_date\" NUMERIC NOT NULL,\n    \"sex_at_birth\" TEXT NOT NULL CHECK(\"sex_at_birth\" IN ('female', 'male')),\n    \"email\" TEXT NOT NULL UNIQUE,\n    \"contact_number\" TEXT NOT NULL UNIQUE CHECK(length(\"contact_number\") = 11),\n    \"civil_status\" TEXT NOT NULL CHECK(\"civil_status\" IN ('single', 'married', 'widowed', 'legally separated')),\n    \"start_date\" NUMERIC NOT NULL,\n    \"end_date\" NUMERIC,\n    \"job_id\" INTEGER,\n    \"manager_id\" INTEGER,\n    \"department_name\" TEXT,\n    PRIMARY KEY(\"id\"),\n    FOREIGN KEY(\"job_id\") REFERENCES \"jobs\"(\"id\"),\n    FOREIGN KEY(\"manager_id\") REFERENCES \"managers\"(\"id\"),\n    FOREIGN KEY(\"department_name\") REFERENCES \"departments\"(\"name\") ON UPDATE CASCADE\n);\n\n\n\nCreate table to represent job titles.\nCREATE TABLE \"jobs\" (\n    \"id\" INTEGER,\n    \"name\" TEXT NOT NULL,\n    \"minimum_rate\" NUMERIC NOT NULL,\n    \"maximum_rate\" NUMERIC NOT NULL,\n    PRIMARY KEY(\"id\")\n);\n\n\n\nCreate table to represent managers\nCREATE TABLE \"managers\" (\n    \"id\" INTEGER,\n    \"employee_id\" INTEGER,\n    \"start_date\" NUMERIC NOT NULL,\n    \"end_date\" NUMERIC,\n    PRIMARY KEY(\"id\"),\n    FOREIGN KEY(\"employee_id\") REFERENCES \"employees\"(\"id\") ON DELETE SET NULL\n);\n\n\n\nCreate table to represent departments.\nCREATE TABLE \"departments\" (\n    \"name\" TEXT,\n    \"head_id\" NUMERIC,\n    \"start_date\" NUMERIC NOT NULL,\n    \"end_date\" NUMERIC,\n    PRIMARY KEY(\"name\"),\n    FOREIGN KEY(\"head_id\") REFERENCES \"employees\"(\"id\") ON DELETE SET NULL\n);\n\n\n\nCreate table to represent job and salary history.\nCREATE TABLE \"job_histories\" (\n    \"employee_id\" INTEGER,\n    \"job_id\" INTEGER,\n    \"start_date\" NUMERIC,\n    \"hourly_rate\" NUMERIC NOT NULL CHECK(\"hourly_rate\" &gt;= 75),\n    \"raise/cut\" NUMERIC,\n    PRIMARY KEY(\"employee_id\", \"start_date\"),\n    FOREIGN KEY(\"employee_id\") REFERENCES \"employees\"(\"id\") ON DELETE CASCADE,\n    FOREIGN KEY(\"job_id\") REFERENCES \"jobs\"(\"id\")\n);\n\n\n\nCreate table to record employee attendance.\nCREATE TABLE \"attendance_logs\" (\n    \"employee_id\" INTEGER,\n    \"date\" NUMERIC,\n    \"time_in\" NUMERIC,\n    \"time_out\" NUMERIC,\n    \"hours_worked\" INTEGER CHECK(\"hours_worked\" &lt;= 12),\n    \"overtime_hours\" INTEGER,\n    \"tardy\" INTEGER CHECK(\"tardy\" IN (0, 1)),\n    \"type\" TEXT CHECK(\"type\" IN ('undertime', 'normal', 'overtime', 'absent', 'paid leave')),\n    PRIMARY KEY(\"employee_id\", \"date\"),\n    FOREIGN KEY(\"employee_id\") REFERENCES \"employees\"(\"id\") ON DELETE CASCADE\n);\n\n\n\nCreate table to record savings rate for SSS program.\nCREATE TABLE \"savings\" (\n    \"date\" NUMERIC,\n    \"employee_id\" INTEGER,\n    \"salary_change\" NUMERIC,\n    \"previous_rate\" NUMERIC,\n    \"adjusted_rate\" NUMERIC,\n    PRIMARY KEY(\"date\", \"employee_id\"),\n    FOREIGN KEY(\"employee_id\") REFERENCES \"employees\"(\"id\") ON DELETE CASCADE\n);\n\n\n\nCreate table for payroll calculation and recording.\nCREATE TABLE \"payrolls\" (\n    \"date\" NUMERIC,\n    \"employee_id\" INTEGER,\n    \"hourly_rate\" NUMERIC NOT NULL,\n    \"regular_hours\" INTEGER NOT NULL,\n    \"overtime_hours\" INTEGER,\n    \"paid_leave\" INTEGER,\n    \"gross_pay\" NUMERIC,\n    \"SSS_deduction\" NUMERIC,\n    \"taxable_pay\" NUMERIC,\n    PRIMARY KEY(\"date\", \"employee_id\"),\n    FOREIGN KEY(\"employee_id\") REFERENCES \"employees\"(\"id\") ON DELETE CASCADE\n);\n\n\n\nCreate table to record survey of job satisfaction.\nCREATE TABLE \"satisfaction_scores\"(\n    \"employee_id\" INTEGER,\n    \"date\" NUMERIC,\n    \"compensation_score\" INTEGER CHECK(\"compensation_score\" BETWEEN 0 AND 5),\n    \"workload_score\" INTEGER CHECK (\"workload_score\" BETWEEN 0 AND 5),\n    \"opportunity_score\" INTEGER CHECK (\"opportunity_score\" BETWEEN 0 AND 5),\n    \"environment_score\" INTEGER CHECK (\"environment_score\" BETWEEN 0 AND 5),\n    \"leadership_score\" INTEGER CHECK (\"leadership_score\" BETWEEN 0 AND 5),\n    \"teamwork_score\" INTEGER CHECK (\"teamwork_score\" BETWEEN 0 AND 5),\n    \"average_score\" NUMERIC CHECK (\"average_score\" BETWEEN 0 AND 5),\n    PRIMARY KEY(\"employee_id\", \"date\"),\n    FOREIGN KEY(\"employee_id\") REFERENCES \"employees\"(\"id\") ON DELETE CASCADE\n);\n\n\n\nCreate table to record evaluation of employee performance.\nCREATE TABLE \"performance_scores\"(\n    \"employee_id\" INTEGER,\n    \"date\" NUMERIC,\n    \"productivity_score\" INTEGER CHECK(\"productivity_score\" BETWEEN 0 AND 5),\n    \"quality_score\" INTEGER CHECK(\"quality_score\" BETWEEN 0 AND 5),\n    \"teamwork_score\" INTEGER CHECK(\"teamwork_score\" BETWEEN 0 AND 5),\n    \"initiative_score\" INTEGER CHECK(\"initiative_score\" BETWEEN 0 AND 5),\n    \"leadership_score\" INTEGER CHECK(\"leadership_score\" BETWEEN 0 AND 5),\n    \"average_score\" NUMERIC CHECK(\"average_score\" BETWEEN 0 AND 5),\n    PRIMARY KEY(\"employee_id\", \"date\"),\n    FOREIGN KEY(\"employee_id\") REFERENCES \"employees\"(\"id\") ON DELETE CASCADE\n);\n\n\n\nCreate table to record employee attrition.\nCREATE TABLE \"attritions\"(\n    \"employee_id\" INTEGER,\n    \"start_date\" NUMERIC NOT NULL,\n    \"end_date\" NUMERIC,\n    \"deletion_date\" NUMERIC,\n    \"type\" TEXT CHECK(\"type\" IN ('voluntary', 'involuntary')),\n    \"reason\" TEXT,\n    PRIMARY KEY(\"employee_id\", \"end_date\"),\n    FOREIGN KEY(\"employee_id\") REFERENCES \"employees\"(\"id\") ON DELETE CASCADE\n);",
    "crumbs": [
      "CS50 SQL: Human Resources Database",
      "Database Schema"
    ]
  },
  {
    "objectID": "cs50_sql/schema.html#tables",
    "href": "cs50_sql/schema.html#tables",
    "title": "Database Schema",
    "section": "",
    "text": "The database schema include the following tables: employees, jobs, managers, departments, job_histories, attendance_logs, savings, payrolls, satisfaction_scores, performance_scores, and attritions.\n\n\nCreate table to represent employees in the company.\nCREATE TABLE \"employees\" (\n    \"id\" INTEGER,\n    \"first_name\" TEXT NOT NULL,\n    \"last_name\" TEXT NOT NULL,\n    \"SSS\" TEXT NOT NULL UNIQUE CHECK(length(\"SSS\") = 10),\n    \"birth_date\" NUMERIC NOT NULL,\n    \"sex_at_birth\" TEXT NOT NULL CHECK(\"sex_at_birth\" IN ('female', 'male')),\n    \"email\" TEXT NOT NULL UNIQUE,\n    \"contact_number\" TEXT NOT NULL UNIQUE CHECK(length(\"contact_number\") = 11),\n    \"civil_status\" TEXT NOT NULL CHECK(\"civil_status\" IN ('single', 'married', 'widowed', 'legally separated')),\n    \"start_date\" NUMERIC NOT NULL,\n    \"end_date\" NUMERIC,\n    \"job_id\" INTEGER,\n    \"manager_id\" INTEGER,\n    \"department_name\" TEXT,\n    PRIMARY KEY(\"id\"),\n    FOREIGN KEY(\"job_id\") REFERENCES \"jobs\"(\"id\"),\n    FOREIGN KEY(\"manager_id\") REFERENCES \"managers\"(\"id\"),\n    FOREIGN KEY(\"department_name\") REFERENCES \"departments\"(\"name\") ON UPDATE CASCADE\n);\n\n\n\nCreate table to represent job titles.\nCREATE TABLE \"jobs\" (\n    \"id\" INTEGER,\n    \"name\" TEXT NOT NULL,\n    \"minimum_rate\" NUMERIC NOT NULL,\n    \"maximum_rate\" NUMERIC NOT NULL,\n    PRIMARY KEY(\"id\")\n);\n\n\n\nCreate table to represent managers\nCREATE TABLE \"managers\" (\n    \"id\" INTEGER,\n    \"employee_id\" INTEGER,\n    \"start_date\" NUMERIC NOT NULL,\n    \"end_date\" NUMERIC,\n    PRIMARY KEY(\"id\"),\n    FOREIGN KEY(\"employee_id\") REFERENCES \"employees\"(\"id\") ON DELETE SET NULL\n);\n\n\n\nCreate table to represent departments.\nCREATE TABLE \"departments\" (\n    \"name\" TEXT,\n    \"head_id\" NUMERIC,\n    \"start_date\" NUMERIC NOT NULL,\n    \"end_date\" NUMERIC,\n    PRIMARY KEY(\"name\"),\n    FOREIGN KEY(\"head_id\") REFERENCES \"employees\"(\"id\") ON DELETE SET NULL\n);\n\n\n\nCreate table to represent job and salary history.\nCREATE TABLE \"job_histories\" (\n    \"employee_id\" INTEGER,\n    \"job_id\" INTEGER,\n    \"start_date\" NUMERIC,\n    \"hourly_rate\" NUMERIC NOT NULL CHECK(\"hourly_rate\" &gt;= 75),\n    \"raise/cut\" NUMERIC,\n    PRIMARY KEY(\"employee_id\", \"start_date\"),\n    FOREIGN KEY(\"employee_id\") REFERENCES \"employees\"(\"id\") ON DELETE CASCADE,\n    FOREIGN KEY(\"job_id\") REFERENCES \"jobs\"(\"id\")\n);\n\n\n\nCreate table to record employee attendance.\nCREATE TABLE \"attendance_logs\" (\n    \"employee_id\" INTEGER,\n    \"date\" NUMERIC,\n    \"time_in\" NUMERIC,\n    \"time_out\" NUMERIC,\n    \"hours_worked\" INTEGER CHECK(\"hours_worked\" &lt;= 12),\n    \"overtime_hours\" INTEGER,\n    \"tardy\" INTEGER CHECK(\"tardy\" IN (0, 1)),\n    \"type\" TEXT CHECK(\"type\" IN ('undertime', 'normal', 'overtime', 'absent', 'paid leave')),\n    PRIMARY KEY(\"employee_id\", \"date\"),\n    FOREIGN KEY(\"employee_id\") REFERENCES \"employees\"(\"id\") ON DELETE CASCADE\n);\n\n\n\nCreate table to record savings rate for SSS program.\nCREATE TABLE \"savings\" (\n    \"date\" NUMERIC,\n    \"employee_id\" INTEGER,\n    \"salary_change\" NUMERIC,\n    \"previous_rate\" NUMERIC,\n    \"adjusted_rate\" NUMERIC,\n    PRIMARY KEY(\"date\", \"employee_id\"),\n    FOREIGN KEY(\"employee_id\") REFERENCES \"employees\"(\"id\") ON DELETE CASCADE\n);\n\n\n\nCreate table for payroll calculation and recording.\nCREATE TABLE \"payrolls\" (\n    \"date\" NUMERIC,\n    \"employee_id\" INTEGER,\n    \"hourly_rate\" NUMERIC NOT NULL,\n    \"regular_hours\" INTEGER NOT NULL,\n    \"overtime_hours\" INTEGER,\n    \"paid_leave\" INTEGER,\n    \"gross_pay\" NUMERIC,\n    \"SSS_deduction\" NUMERIC,\n    \"taxable_pay\" NUMERIC,\n    PRIMARY KEY(\"date\", \"employee_id\"),\n    FOREIGN KEY(\"employee_id\") REFERENCES \"employees\"(\"id\") ON DELETE CASCADE\n);\n\n\n\nCreate table to record survey of job satisfaction.\nCREATE TABLE \"satisfaction_scores\"(\n    \"employee_id\" INTEGER,\n    \"date\" NUMERIC,\n    \"compensation_score\" INTEGER CHECK(\"compensation_score\" BETWEEN 0 AND 5),\n    \"workload_score\" INTEGER CHECK (\"workload_score\" BETWEEN 0 AND 5),\n    \"opportunity_score\" INTEGER CHECK (\"opportunity_score\" BETWEEN 0 AND 5),\n    \"environment_score\" INTEGER CHECK (\"environment_score\" BETWEEN 0 AND 5),\n    \"leadership_score\" INTEGER CHECK (\"leadership_score\" BETWEEN 0 AND 5),\n    \"teamwork_score\" INTEGER CHECK (\"teamwork_score\" BETWEEN 0 AND 5),\n    \"average_score\" NUMERIC CHECK (\"average_score\" BETWEEN 0 AND 5),\n    PRIMARY KEY(\"employee_id\", \"date\"),\n    FOREIGN KEY(\"employee_id\") REFERENCES \"employees\"(\"id\") ON DELETE CASCADE\n);\n\n\n\nCreate table to record evaluation of employee performance.\nCREATE TABLE \"performance_scores\"(\n    \"employee_id\" INTEGER,\n    \"date\" NUMERIC,\n    \"productivity_score\" INTEGER CHECK(\"productivity_score\" BETWEEN 0 AND 5),\n    \"quality_score\" INTEGER CHECK(\"quality_score\" BETWEEN 0 AND 5),\n    \"teamwork_score\" INTEGER CHECK(\"teamwork_score\" BETWEEN 0 AND 5),\n    \"initiative_score\" INTEGER CHECK(\"initiative_score\" BETWEEN 0 AND 5),\n    \"leadership_score\" INTEGER CHECK(\"leadership_score\" BETWEEN 0 AND 5),\n    \"average_score\" NUMERIC CHECK(\"average_score\" BETWEEN 0 AND 5),\n    PRIMARY KEY(\"employee_id\", \"date\"),\n    FOREIGN KEY(\"employee_id\") REFERENCES \"employees\"(\"id\") ON DELETE CASCADE\n);\n\n\n\nCreate table to record employee attrition.\nCREATE TABLE \"attritions\"(\n    \"employee_id\" INTEGER,\n    \"start_date\" NUMERIC NOT NULL,\n    \"end_date\" NUMERIC,\n    \"deletion_date\" NUMERIC,\n    \"type\" TEXT CHECK(\"type\" IN ('voluntary', 'involuntary')),\n    \"reason\" TEXT,\n    PRIMARY KEY(\"employee_id\", \"end_date\"),\n    FOREIGN KEY(\"employee_id\") REFERENCES \"employees\"(\"id\") ON DELETE CASCADE\n);",
    "crumbs": [
      "CS50 SQL: Human Resources Database",
      "Database Schema"
    ]
  },
  {
    "objectID": "cs50_sql/schema.html#triggers1",
    "href": "cs50_sql/schema.html#triggers1",
    "title": "Database Schema",
    "section": "Triggers1",
    "text": "Triggers1\nThe database schema include the following triggers: tardiness, shift_end, new_job, savings_adjustment, payroll_calculation, performance_average, satisfaction_average, and soft_delete_employees.\n\nTardiness\nUpon time in, this trigger will update the tardy column depending on the time_in value. The employee is considered late when they are more than 30 minutes late (on-time is 08:00:00). The \"type\" is NULL in the WHERE clause prevents the trigger from updating on INSERTs with absents and paid leaves.\nCREATE TRIGGER \"tardiness\"\nAFTER INSERT ON \"attendance_logs\"\nFOR EACH ROW\nBEGIN\n    UPDATE \"attendance_logs\"\n    SET \"tardy\" =\n        CASE\n            WHEN (\n                SELECT ROUND (\n                    (julianday(NEW.\"date\"||' 08:00:00') - julianday(NEW.\"time_in\")) * 24\n                )\n                FROM \"attendance_logs\"\n                WHERE \"employee_id\" = NEW.\"employee_id\" AND \"date\" = NEW.\"date\"\n            ) &lt; 0\n                THEN 1\n            ELSE 0\n        END\n    WHERE \"employee_id\" = NEW.\"employee_id\" AND \"date\" = NEW.\"date\" AND \"type\" IS NULL;\nEND;\n\n\nShift End\nUpon time out, the hours worked will be calculated. The calculated hours_worked will be used to determine if the shift is undertime, normal, or overtime. This will then calculate the overtime_hours. The normal shift is 8 hours, give or take 30 minutes.\nCREATE TRIGGER \"shift_end\"\nAFTER UPDATE OF \"time_out\" ON \"attendance_logs\"\nFOR EACH ROW\nBEGIN\n    UPDATE \"attendance_logs\"\n    SET \"hours_worked\" = ROUND((julianday(NEW.\"time_out\") - julianday(OLD.\"time_in\"))*24)\n    WHERE \"employee_id\" = OLD.\"employee_id\" AND \"date\" = OLD.\"date\";\n\n    UPDATE \"attendance_logs\"\n    SET \"type\" =\n        CASE\n            WHEN \"hours_worked\" &gt; 8 THEN 'overtime'\n            WHEN \"hours_worked\" = 8 THEN 'normal'\n            ELSE 'undertime'\n        END\n    WHERE \"employee_id\" = OLD.\"employee_id\" AND \"date\" = OLD.\"date\";\n\n    UPDATE \"attendance_logs\"\n    SET \"overtime_hours\" = \"hours_worked\" - 8\n    WHERE \"employee_id\" = OLD.\"employee_id\" AND \"date\" = OLD.\"date\" AND \"type\" = 'overtime';\nEND;\n\n\nNew Job\nThis is triggered when an employee is assigned to a new job (new hire or old employee). If applicable, the pay changes is also recorded. The trigger also enrolls the employee to a savings plan and updates with every subsequent job changes. This works along with the savings_adjustment trigger.\nCREATE TRIGGER \"new_job\"\nAFTER INSERT ON \"job_histories\"\nFOR EACH ROW\nBEGIN\n    UPDATE \"employees\"\n    SET \"job_id\" = (\n        SELECT \"job_id\"\n        FROM \"job_histories\"\n        WHERE \"employee_id\" = NEW.\"employee_id\"\n        ORDER BY \"start_date\" DESC LIMIT 1\n    )\n    WHERE \"id\" = NEW.\"employee_id\";\n\n    INSERT INTO \"savings\"\n    (\"date\", \"employee_id\", \"salary_change\", \"previous_rate\")\n    VALUES\n    (NEW.\"start_date\", NEW.\"employee_id\", NEW.\"raise/cut\",\n        (SELECT \"adjusted_rate\"\n        FROM \"savings\"\n        WHERE \"employee_id\" = NEW.\"employee_id\"\n        ORDER BY \"date\" DESC\n        LIMIT 1\n        )\n    );\nEND;\n\n\nSavings Adjustment\nBased on the changes made by the new_job trigger, savings rate will be adjusted. The adjustment is based on the 10% of the salary_change. The starting savings rate is 3% and the maximum is 18%.\nCREATE TRIGGER \"savings_adjustment\"\nAFTER INSERT ON \"savings\"\nFOR EACH ROW\nBEGIN\n    UPDATE \"savings\"\n    SET \"adjusted_rate\" =\n        CASE\n            WHEN \"previous_rate\" IS NULL THEN 0.03\n            WHEN (\n                SELECT COALESCE(\"previous_rate\", 0) + (COALESCE(\"salary_change\", 0) * 0.1)\n                FROM \"savings\"\n                WHERE \"employee_id\" = NEW.\"employee_id\"\n                ORDER BY \"date\" DESC\n                LIMIT 1\n            ) &gt;= 0.18\n                THEN 0.18\n            ELSE (\n                SELECT COALESCE(\"previous_rate\", 0) + (COALESCE(\"salary_change\", 0) * 0.1)\n                FROM \"savings\"\n                WHERE \"employee_id\" = NEW.\"employee_id\"\n                ORDER BY \"date\" DESC\n                LIMIT 1\n            )\n        END\n    WHERE \"employee_id\" = NEW.\"employee_id\" AND \"date\" = NEW.\"date\";\nEND;\n\n\nPayroll Calculation\nThis trigger is initialized by an INSERT, which collects the necessary information to calculate the payroll. This calculates the gross_pay, SSS_deduction (savings plan), and the taxable_pay. The pay for overtime hours is 125% of the hourly rate. The pay for paid leave is for the full 8 hours. The SSS deduction is not used for tax calculation.\nCREATE TRIGGER \"payroll_calculation\"\nAFTER INSERT ON \"payrolls\"\nFOR EACH ROW\nBEGIN\n    UPDATE \"payrolls\"\n    SET\n    \"gross_pay\" = (\n        SELECT (\"hourly_rate\" * \"regular_hours\") + (\"hourly_rate\" * 1.25 * \"overtime_hours\") +\n            (\"paid_leave\" * 8 * \"hourly_rate\")\n        FROM \"payrolls\"\n        WHERE \"employee_id\" = NEW.\"employee_id\" AND \"date\" = NEW.\"date\"\n    ),\n    \"SSS_deduction\" = (\n        SELECT ((\"hourly_rate\" * \"regular_hours\") + (\"hourly_rate\" * 1.25 * \"overtime_hours\") +\n            (\"paid_leave\" * 8 * \"hourly_rate\")) * (\n                SELECT \"adjusted_rate\"\n                FROM \"savings\"\n                WHERE \"employee_id\" = NEW.\"employee_id\"\n                ORDER BY \"date\" DESC\n                LIMIT 1\n                )\n        FROM \"payrolls\"\n        WHERE \"employee_id\" = NEW.\"employee_id\" AND \"date\" = NEW.\"date\"\n    ),\n    \"taxable_pay\" = (\n        SELECT ((\"hourly_rate\" * \"regular_hours\") + (\"hourly_rate\" * 1.25 * \"overtime_hours\") +\n            (\"paid_leave\" * 8 * \"hourly_rate\")) * (1 - (\n                SELECT \"adjusted_rate\"\n                FROM \"savings\"\n                WHERE \"employee_id\" = NEW.\"employee_id\"\n                ORDER BY \"date\" DESC\n                LIMIT 1\n                ))\n        FROM \"payrolls\"\n        WHERE \"employee_id\" = NEW.\"employee_id\" AND \"date\" = NEW.\"date\"\n    )\n    WHERE \"employee_id\" = NEW.\"employee_id\" AND \"date\" = NEW.\"date\";\nEND;\n\n\nPerformance Average\nBased on the performance evaluation in different measures, the average performance score is calculated.\nCREATE TRIGGER \"performance_average\"\nAFTER INSERT ON \"performance_scores\"\nFOR EACH ROW\nBEGIN\n    UPDATE \"performance_scores\"\n    SET \"average_score\" = (\n        SELECT ROUND(\n            (\n                (\"productivity_score\" + \"quality_score\" + \"teamwork_score\" + \"initiative_score\" + \"leadership_score\")\n            /5)\n        , 2)\n        FROM \"performance_scores\"\n        WHERE \"employee_id\" = NEW.\"employee_id\" AND \"date\" = NEW.\"date\"\n    )\n    WHERE \"employee_id\" = NEW.\"employee_id\" AND \"date\" = NEW.\"date\";\nEND;\n\n\nSatisfaction Average\nBased on the satisfaction ratings in different areas, the average satisfaction score is calculated.\nCREATE TRIGGER \"satisfaction_average\"\nAFTER INSERT ON \"satisfaction_scores\"\nFOR EACH ROW\nBEGIN\n    UPDATE \"satisfaction_scores\"\n    SET \"average_score\" = (\n        SELECT ROUND(\n            (\n                (\"compensation_score\" + \"workload_score\" + \"opportunity_score\" + \"environment_score\" +\n                    \"leadership_score\" + \"teamwork_score\")\n            /6)\n        , 2)\n        FROM \"satisfaction_scores\"\n        WHERE \"employee_id\" = NEW.\"employee_id\" AND \"date\" = NEW.\"date\"\n    )\n    WHERE \"employee_id\" = NEW.\"employee_id\" AND \"date\" = NEW.\"date\";\nEND;\n\n\nSoft Delete Employees\nThis trigger updates the end_date of an employee in the employees table. The end_date serves as a marking of soft deletion.\nCREATE TRIGGER \"soft_delete_employees\"\nAFTER INSERT ON \"attritions\"\nFOR EACH ROW\nBEGIN\n    UPDATE \"employees\"\n    SET \"end_date\" = NEW.\"end_date\"\n    WHERE \"id\" = NEW.\"employee_id\";\nEND;",
    "crumbs": [
      "CS50 SQL: Human Resources Database",
      "Database Schema"
    ]
  },
  {
    "objectID": "cs50_sql/schema.html#views",
    "href": "cs50_sql/schema.html#views",
    "title": "Database Schema",
    "section": "Views",
    "text": "Views\nThe database schema include the following views: monthly_attendance, manager_1, employee_evaluation,and attrition_analysis.\n\nMonthly Attendance\nThis view shows an overview of the monthly attendance records. This shows counts of shift by type and counts of tardiness.\nCREATE VIEW \"monthly_attendance\" AS\nWITH \"undertimes\" AS (\n    SELECT\n        \"employee_id\",\n        COUNT(*) AS \"undertime_count\"\n    FROM \"attendance_logs\"\n    WHERE strftime('%m', \"date\") LIKE (strftime('%m', DATE('now')))||'%'\n        AND \"type\" = 'undertime'\n    GROUP BY \"employee_id\"\n),\n\"normals\" AS (\n    SELECT\n        \"employee_id\",\n        COUNT(*) AS \"normal_count\"\n    FROM \"attendance_logs\"\n    WHERE strftime('%m', \"date\") LIKE (strftime('%m', DATE('now')))||'%'\n        AND \"type\" = 'normal'\n    GROUP BY \"employee_id\"\n),\n\"overtimes\" AS (\n    SELECT\n        \"employee_id\",\n        COUNT(*) AS \"overtime_count\"\n    FROM \"attendance_logs\"\n    WHERE strftime('%m', \"date\") LIKE (strftime('%m', DATE('now')))||'%'\n        AND \"type\" = 'overtime'\n    GROUP BY \"employee_id\"\n),\n\"absents\" AS (\n    SELECT\n        \"employee_id\",\n        COUNT(*) AS \"absent_count\"\n    FROM \"attendance_logs\"\n    WHERE strftime('%m', \"date\") LIKE (strftime('%m', DATE('now')))||'%'\n        AND \"type\" = 'absent'\n    GROUP BY \"employee_id\"\n),\n\"paid_leaves\" AS (\n    SELECT\n        \"employee_id\",\n        COUNT(*) AS \"paid_leave_count\"\n    FROM \"attendance_logs\"\n    WHERE strftime('%m', \"date\") LIKE (strftime('%m', DATE('now')))||'%'\n        AND \"type\" = 'paid leave'\n    GROUP BY \"employee_id\"\n    ),\n\"tardies\" AS (\n    SELECT\n        \"employee_id\",\n        SUM(\"tardy\") AS \"tardy_count\"\n    FROM \"attendance_logs\"\n    WHERE strftime('%m', \"date\") LIKE (strftime('%m', DATE('now')))||'%'\n    GROUP BY \"employee_id\"\n)\nSELECT\n    \"employees\".\"id\" AS \"employee_id\",\n    \"employees\".\"job_id\",\n    \"employees\".\"manager_id\",\n    \"employees\".\"department_name\",\n    IFNULL(\"undertimes\".\"undertime_count\", 0) AS \"undertime_count\",\n    IFNULL(\"normals\".\"normal_count\", 0) AS \"normal_count\",\n    IFNULL(\"overtimes\".\"overtime_count\", 0) AS \"overtime_count\",\n    IFNULL(\"absents\".\"absent_count\", 0) AS \"absent_count\",\n    IFNULL(\"paid_leaves\".\"paid_leave_count\", 0) AS \"paid_leave_count\",\n    \"tardies\".\"tardy_count\"\nFROM \"employees\"\nLEFT JOIN \"undertimes\" ON\n\"employees\".\"id\" = \"undertimes\".\"employee_id\"\nLEFT JOIN \"normals\" ON\n\"employees\".\"id\" = \"normals\".\"employee_id\"\nLEFT JOIN \"overtimes\" ON\n\"employees\".\"id\" = \"overtimes\".\"employee_id\"\nLEFT JOIN \"absents\" ON\n\"employees\".\"id\" = \"absents\".\"employee_id\"\nLEFT JOIN \"paid_leaves\" ON\n\"employees\".\"id\" = \"paid_leaves\".\"employee_id\"\nLEFT JOIN \"tardies\" ON\n\"employees\".\"id\" = \"tardies\".\"employee_id\";\n\n\nManager 1\nThis is a sample view for the managers. This shows the satisfaction of employees under a particular manager. The employee_id is hidden from this view to encourage honest feedbacks. This report is for the current year average only.\nCREATE VIEW \"manager_1\" AS\nWITH \"satisfactions\" AS (\n    SELECT\n        \"employee_id\",\n        AVG(\"compensation_score\") AS \"compensation_satisfaction\",\n        AVG(\"workload_score\") AS \"workload_satisfaction\",\n        AVG(\"opportunity_score\") AS \"opportunity_satisfaction\",\n        AVG(\"environment_score\") AS \"environment_satisfaction\",\n        AVG(\"leadership_score\") AS \"leadership_satisfaction\",\n        AVG(\"teamwork_score\") AS \"teamwork_satisfaction\",\n        AVG(\"average_score\") AS \"average_satisfaction\"\n    FROM \"satisfaction_scores\"\n    WHERE strftime('%Y', \"date\") LIKE (strftime('%Y', DATE('now')))||'%'\n    GROUP BY \"employee_id\"\n)\nSELECT\n    \"satisfactions\".\"compensation_satisfaction\",\n    \"satisfactions\".\"workload_satisfaction\",\n    \"satisfactions\".\"opportunity_satisfaction\",\n    \"satisfactions\".\"environment_satisfaction\",\n    \"satisfactions\".\"leadership_satisfaction\",\n    \"satisfactions\".\"teamwork_satisfaction\",\n    \"satisfactions\".\"average_satisfaction\"\nFROM \"employees\"\nLEFT JOIN \"satisfactions\" ON\n\"employees\".\"id\" = \"satisfactions\".\"employee_id\"\nWHERE \"employees\".\"manager_id\" = 1\n    AND \"employees\".\"id\" &lt;&gt; (SELECT \"employee_id\" FROM \"managers\" WHERE \"id\" = 1);\n\n\nEmployee Evaluation\nThis view shows the average performance evaluation of employees. This report is for the current year average only.\nCREATE VIEW \"employee_evaluation\" AS\nSELECT\n    \"employee_id\",\n    AVG(\"productivity_score\") AS \"productivity_performance\",\n    AVG(\"quality_score\") AS \"quality_performance\",\n    AVG(\"teamwork_score\") AS \"teamwork_performance\",\n    AVG(\"initiative_score\") AS \"initiative_performance\",\n    AVG(\"leadership_score\") AS \"leadership_performance\",\n    AVG(\"average_score\") AS \"average_performance\"\nFROM \"performance_scores\"\nWHERE strftime('%Y', \"date\") LIKE (strftime('%Y', DATE('now')))||'%'\nGROUP BY \"employee_id\";\n\n\nAttrition Analysis\nThis view is useful for attrition analysis. This view makes it easy to later export the data in a CSV file for statistical analysis. This shows only the date for the current year.\nCREATE VIEW \"attrition_analysis\" AS\nWITH \"performances\" AS (\n    SELECT\n        \"employee_id\",\n        AVG(\"productivity_score\") AS \"productivity_performance\",\n        AVG(\"quality_score\") AS \"quality_performance\",\n        AVG(\"teamwork_score\") AS \"teamwork_performance\",\n        AVG(\"initiative_score\") AS \"initiative_performance\",\n        AVG(\"leadership_score\") AS \"leadership_performance\",\n        AVG(\"average_score\") AS \"average_performance\"\n    FROM \"performance_scores\"\n    WHERE strftime('%Y', \"date\") LIKE (strftime('%Y', DATE('now')))||'%'\n    GROUP BY \"employee_id\"\n),\n\"satisfactions\" AS (\n    SELECT\n        \"employee_id\",\n        AVG(\"compensation_score\") AS \"compensation_satisfaction\",\n        AVG(\"workload_score\") AS \"workload_satisfaction\",\n        AVG(\"opportunity_score\") AS \"opportunity_satisfaction\",\n        AVG(\"environment_score\") AS \"environment_satisfaction\",\n        AVG(\"leadership_score\") AS \"leadership_satisfaction\",\n        AVG(\"teamwork_score\") AS \"teamwork_satisfaction\",\n        AVG(\"average_score\") AS \"average_satisfaction\"\n    FROM \"satisfaction_scores\"\n    WHERE strftime('%Y', \"date\") LIKE (strftime('%Y', DATE('now')))||'%'\n    GROUP BY \"employee_id\"\n)\nSELECT\n    \"employees\".\"id\",\n    IIF(\"attritions\".\"employee_id\" IS NOT NULL, 1, 0) AS \"attrition\",\n    CAST(\n        ROUND(\n                (\n                    (julianday('now') - julianday(\"employees\".\"start_date\"))\n                    /(365/12)\n                )\n        , 0)\n    AS INT) AS \"tenure_in_months\",\n    DATE('now') - \"employees\".\"birth_date\" AS \"age\",\n    \"employees\".\"sex_at_birth\",\n    \"employees\".\"civil_status\",\n    \"performances\".\"productivity_performance\",\n    \"performances\".\"quality_performance\",\n    \"performances\".\"teamwork_performance\",\n    \"performances\".\"initiative_performance\",\n    \"performances\".\"leadership_performance\",\n    \"performances\".\"average_performance\",\n    \"satisfactions\".\"compensation_satisfaction\",\n    \"satisfactions\".\"workload_satisfaction\",\n    \"satisfactions\".\"opportunity_satisfaction\",\n    \"satisfactions\".\"environment_satisfaction\",\n    \"satisfactions\".\"leadership_satisfaction\",\n    \"satisfactions\".\"teamwork_satisfaction\",\n    \"satisfactions\".\"average_satisfaction\",\n    \"employees\".\"job_id\",\n    \"employees\".\"manager_id\",\n    \"employees\".\"department_name\"\nFROM \"employees\"\nLEFT JOIN \"attritions\" ON\n\"employees\".\"id\" = \"attritions\".\"employee_id\"\nLEFT JOIN \"performances\" ON\n\"employees\".\"id\" = \"performances\".\"employee_id\"\nLEFT JOIN \"satisfactions\" ON\n\"employees\".\"id\" = \"satisfactions\".\"employee_id\";",
    "crumbs": [
      "CS50 SQL: Human Resources Database",
      "Database Schema"
    ]
  },
  {
    "objectID": "cs50_sql/schema.html#indexes",
    "href": "cs50_sql/schema.html#indexes",
    "title": "Database Schema",
    "section": "Indexes",
    "text": "Indexes\nIndexes are not strictly necessary for this database. As this is for internal use only and only occasionally uses SELECT queries, the space saving trumps the time saving. There is also trade-off in speed with relatively more frequent operations involving UPDATE and INSERT. Thus, this database schema only includes one index: names.\n\nNames\nCREATE INDEX \"names\"\nON \"employees\" (\"first_name\", \"last_name\");",
    "crumbs": [
      "CS50 SQL: Human Resources Database",
      "Database Schema"
    ]
  },
  {
    "objectID": "cs50_sql/schema.html#footnotes",
    "href": "cs50_sql/schema.html#footnotes",
    "title": "Database Schema",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI used a code from this Reddit post to get the difference between date and datetime values: https://www.reddit.com/r/SQL/comments/je8x56/sqlite_solution_found_date_difference_in_years↩︎",
    "crumbs": [
      "CS50 SQL: Human Resources Database",
      "Database Schema"
    ]
  },
  {
    "objectID": "helsinki_dap2324/project_regression-analysis.html#prerequisites",
    "href": "helsinki_dap2324/project_regression-analysis.html#prerequisites",
    "title": "Regression Analysis Project",
    "section": "Prerequisites",
    "text": "Prerequisites\nImport all the modules you will need in this notebook here:\n\n# exercise 0\n# Put your solution here!\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport statsmodels\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\n# plots a line given an intercept and a slope\nfrom statsmodels.graphics.regressionplots import abline_plot\nimport pandas as pd",
    "crumbs": [
      "University of Helsinki: Data Analysis with Python",
      "Regression Analysis Project"
    ]
  },
  {
    "objectID": "helsinki_dap2324/project_regression-analysis.html#linear-regression",
    "href": "helsinki_dap2324/project_regression-analysis.html#linear-regression",
    "title": "Regression Analysis Project",
    "section": "Linear Regression",
    "text": "Linear Regression\nWe continue analysing the fram heart disease data.\nFirst load the data, use the name fram for the DataFrame variable. Make sure that in the data you loaded the column and row headers are in place. Checkout the summary of the variables using the describe method.\n\n# exercise 1\ndef get_path(filename):\n    import sys\n    import os\n    prog_name = sys.argv[0]\n    if os.path.basename(prog_name) == \"__main__.py\":   # Running under TMC\n        return os.path.join(os.path.dirname(prog_name), \"..\", \"src\", filename)\n    else:\n        return filename\n    \n# Put your solution here!\nfram = pd.read_csv(get_path(\"fram.txt\"), sep=\"\\t\")\nfram.describe()\n\n\n\n\n\n\n\n\nID\nAGE\nFRW\nSBP\nSBP10\nDBP\nCHOL\nCIG\nCHD\nDEATH\nYRS_DTH\n\n\n\n\ncount\n1394.000000\n1394.000000\n1394.000000\n1394.000000\n767.000000\n1394.000000\n1394.000000\n1394.000000\n1394.000000\n1394.000000\n1394.000000\n\n\nmean\n4737.184362\n52.431133\n105.365136\n148.086083\n148.040417\n90.135581\n234.644907\n8.029412\n1.187948\n1.700861\n16.219512\n\n\nstd\n1073.406896\n4.781507\n17.752489\n28.022062\n25.706664\n14.226235\n46.303822\n11.584138\n2.615976\n3.203132\n3.921413\n\n\nmin\n1070.000000\n45.000000\n52.000000\n90.000000\n94.000000\n50.000000\n96.000000\n0.000000\n0.000000\n0.000000\n1.000000\n\n\n25%\n3890.250000\n48.000000\n94.000000\n130.000000\n130.000000\n80.000000\n200.000000\n0.000000\n0.000000\n0.000000\n18.000000\n\n\n50%\n4821.000000\n52.000000\n103.000000\n142.000000\n145.000000\n90.000000\n230.000000\n0.000000\n0.000000\n0.000000\n18.000000\n\n\n75%\n5641.750000\n56.000000\n114.000000\n160.000000\n160.000000\n98.000000\n264.000000\n20.000000\n0.000000\n0.000000\n18.000000\n\n\nmax\n6442.000000\n62.000000\n222.000000\n300.000000\n264.000000\n160.000000\n430.000000\n60.000000\n10.000000\n10.000000\n18.000000\n\n\n\n\n\n\n\nCreate function rescale that takes a Series as parameter. It should center the data and normalize it by dividing by 2 \\(\\sigma\\), where \\(\\sigma\\) is the standard deviation. Return the rescaled Series.\n\n# exercise 2\n# Put your solution here!\ndef rescale(s):\n    center = s.mean()\n    normal = 2*s.std()\n    return (s-center)/normal\n\nAdd to the DataFrame the scaled versions of all the continuous variables (with function rescale). Add small letter s in front of the original variable name to get the name of the scaled variable. For instance, AGE -&gt; sAGE.\n\n# exercise 3\n# Put your solution here!\nnew = [\"s\"+x for x in fram.columns[2:9].drop(\"SBP10\")]\nfram[new] = rescale(fram.loc[:, \"AGE\":\"CIG\"].drop(\"SBP10\", axis=1))\n# fram.describe() # The rescaled variables have mean 0 and std 0.5\n# The rescaling makes the beta cofficients of continuous variables comparable to the coefficients of binary variables.\n# Gelman, A. (2008). Scaling regression inputs by dividing by two standard deviations. Statistics in Medicine, \n    # 27(15), 2865–2873. https://doi.org/10.1002/sim.3107\n\nForm a model that predicts systolic blood pressure using weight, gender, and cholesterol level as explanatory variables. Store the fitted model in variable named fit.\n\n# exercise 4\n# Put your solution here!\nfit = smf.ols('SBP ~ SEX + sFRW + sCHOL', data=fram).fit()\nprint(fit.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                    SBP   R-squared:                       0.125\nModel:                            OLS   Adj. R-squared:                  0.123\nMethod:                 Least Squares   F-statistic:                     66.37\nDate:                Thu, 20 Jun 2024   Prob (F-statistic):           4.13e-40\nTime:                        23:33:55   Log-Likelihood:                -6530.4\nNo. Observations:                1394   AIC:                         1.307e+04\nDf Residuals:                    1390   BIC:                         1.309e+04\nDf Model:                           3                                         \nCovariance Type:            nonrobust                                         \n===============================================================================\n                  coef    std err          t      P&gt;|t|      [0.025      0.975]\n-------------------------------------------------------------------------------\nIntercept     150.0199      0.985    152.336      0.000     148.088     151.952\nSEX[T.male]    -4.0659      1.451     -2.803      0.005      -6.912      -1.220\nsFRW           17.7205      1.426     12.431      0.000      14.924      20.517\nsCHOL           4.9169      1.431      3.436      0.001       2.110       7.724\n==============================================================================\nOmnibus:                      327.612   Durbin-Watson:                   1.774\nProb(Omnibus):                  0.000   Jarque-Bera (JB):              843.676\nSkew:                           1.237   Prob(JB):                    6.28e-184\nKurtosis:                       5.899   Cond. No.                         2.79\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nAdd the variable AGE to the model and inspect the estimates of the coefficients using the summary method of the fitted model. Again use the name fit for the fitted model. (From now on assume that we always use the name fit for the variable of the fitted model.)\n\n# exercise 5\n# Put your solution here!\nfit = smf.ols('SBP ~ SEX + sFRW + sCHOL + sAGE', data=fram).fit()\nprint(fit.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                    SBP   R-squared:                       0.146\nModel:                            OLS   Adj. R-squared:                  0.144\nMethod:                 Least Squares   F-statistic:                     59.39\nDate:                Thu, 20 Jun 2024   Prob (F-statistic):           2.44e-46\nTime:                        23:33:55   Log-Likelihood:                -6513.6\nNo. Observations:                1394   AIC:                         1.304e+04\nDf Residuals:                    1389   BIC:                         1.306e+04\nDf Model:                           4                                         \nCovariance Type:            nonrobust                                         \n===============================================================================\n                  coef    std err          t      P&gt;|t|      [0.025      0.975]\n-------------------------------------------------------------------------------\nIntercept     150.1695      0.974    154.221      0.000     148.259     152.080\nSEX[T.male]    -4.3805      1.435     -3.053      0.002      -7.195      -1.566\nsFRW           16.9771      1.415     11.999      0.000      14.202      19.753\nsCHOL           4.2696      1.419      3.009      0.003       1.486       7.053\nsAGE            8.1332      1.400      5.810      0.000       5.387      10.879\n==============================================================================\nOmnibus:                      321.087   Durbin-Watson:                   1.807\nProb(Omnibus):                  0.000   Jarque-Bera (JB):              840.955\nSkew:                           1.206   Prob(JB):                    2.45e-183\nKurtosis:                       5.944   Cond. No.                         2.82\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nHow much does the inclusion of age increase the explanatory power of the model? Which variables explain the variance of the target variable most?\n\n\nFor the first question, first, it’s important to define R-squared. The R-squared gives an estimate of how much of the variation in y (dependent variable) is explained by the x’s (explanatory variables). A value of 1.00 means all the variation in y is explained by the x’s. The model including sAGEand its interactions have higher R-squared (higher is better) than the model without these variables, a difference of 0.021. The same is true with the adjusted R-squared, which penalizes the addition of more explanatory variables. It’s important to remember not to put too much weight on the R-squared in terms of model evaluation. For the next question, the outputs produced so far from the codes cannot answer this question. The package dominance-analysis may be useful for answering this question. Now, what the outputs could show are the differences in the magnitude of their effect on the dependent variable, which are now easily compared due to the rescaling earlier. Each beta coefficient now corresponds to a change of two standard deviations (1 unit change in a variable with 0.5 standard deviation). Given this, comparing the absolute value of their beta coefficients, this is the order of explanatory variables: sFRW, sAGE, SEX, sCHOL. In this case, however, the results from my implementation of the dominance-analysis package reveal a similar story.\nfrom dominance_analysis import Dominance_Datasets\nfrom dominance_analysis import Dominance\ndom_data = fram[[\"SBP\", \"SEX\", \"sFRW\", \"sCHOL\", \"sAGE\"]]\ndom_data.SEX = (dom_data.SEX == \"male\")\ndom_data.SEX = dom_data.SEX.astype(\"int\")\ndominance_regression=Dominance(data=dom_data,target='SBP',objective=1)\nincr_variable_rsquare=dominance_regression.incremental_rsquare()\ndominance_regression.dominance_stats()\n\npercentage_relative_importance = {'sFRW': 67.71633624572831, 'sAGE': 17.724686667202082, \n'SEX': 8.476548868422698, 'sCHOL': 6.082428218646909}\n\n\nTry to add to the model all the interactions with other variables.\n\n# exercise 6\n# Put your solution here!\nfit = smf.ols('SBP ~ SEX + SEX:sFRW + SEX:sCHOL + SEX:sAGE + sFRW + sFRW:sCHOL + sFRW:sAGE + sCHOL + sCHOL:sAGE + sAGE', \n              data=fram).fit()\nprint(fit.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                    SBP   R-squared:                       0.151\nModel:                            OLS   Adj. R-squared:                  0.144\nMethod:                 Least Squares   F-statistic:                     24.52\nDate:                Thu, 20 Jun 2024   Prob (F-statistic):           4.92e-43\nTime:                        23:33:55   Log-Likelihood:                -6509.9\nNo. Observations:                1394   AIC:                         1.304e+04\nDf Residuals:                    1383   BIC:                         1.310e+04\nDf Model:                          10                                         \nCovariance Type:            nonrobust                                         \n=====================================================================================\n                        coef    std err          t      P&gt;|t|      [0.025      0.975]\n-------------------------------------------------------------------------------------\nIntercept           150.5439      1.001    150.355      0.000     148.580     152.508\nSEX[T.male]          -4.7309      1.451     -3.261      0.001      -7.577      -1.885\nsFRW                 16.6450      1.723      9.660      0.000      13.265      20.025\nSEX[T.male]:sFRW      1.6673      3.088      0.540      0.589      -4.390       7.725\nsCHOL                 3.6181      1.878      1.926      0.054      -0.066       7.302\nSEX[T.male]:sCHOL     1.0231      2.933      0.349      0.727      -4.730       6.776\nsAGE                 10.2189      2.003      5.101      0.000       6.289      14.149\nSEX[T.male]:sAGE     -4.2923      2.886     -1.488      0.137      -9.953       1.368\nsFRW:sCHOL            4.9729      2.718      1.829      0.068      -0.359      10.305\nsFRW:sAGE            -2.0866      2.803     -0.744      0.457      -7.585       3.412\nsCHOL:sAGE           -4.7564      3.044     -1.563      0.118     -10.728       1.215\n==============================================================================\nOmnibus:                      313.112   Durbin-Watson:                   1.798\nProb(Omnibus):                  0.000   Jarque-Bera (JB):              802.290\nSkew:                           1.185   Prob(JB):                    6.10e-175\nKurtosis:                       5.863   Cond. No.                         6.02\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nThen visualize the model as the function of weight for the youngest (sAGE=-1.0), middle aged (sAGE=0.0), and oldest (sAGE=1.0) women while assuming the background variables to be centered. Remember to consider the changes in the intercept and in the regression coefficient caused by age. Visualize both the data points and the fitted lines.\n\n# exercise 7\n# Put your solution here!\n# Defining colors based on viridis color map (yellow - violet)\nviridis = {\n    0: \"#fde725\",\n    1: \"#addc30\",\n    2: \"#5ec962\",\n    3: \"#28ae80\",\n    4: \"#21918c\",\n    5: \"#2c728e\",\n    6: \"#3b528b\",\n    7: \"#472d7b\",\n    8: \"#440154\"\n}\np = fit.params\n# SEX = \"female\"\nfram[fram.SEX==\"female\"].plot.scatter(\"sFRW\", \"SBP\", c=viridis[8])\n## Youngest, sAGE = -1.0\nabline_plot(intercept=p.Intercept - p[\"sAGE\"], slope=p.sFRW - p[\"sFRW:sAGE\"],\nax=plt.gca(), color=viridis[0], label=\"youngest\")\n## Middle-aged, sAGE = 0.0\nabline_plot(intercept=p.Intercept, slope=p.sFRW,\nax=plt.gca(), color=viridis[2], label=\"middle-aged\")\n## Oldest, sAGE = 1.0\nabline_plot(intercept=p.Intercept + p[\"sAGE\"], slope=p.sFRW + p[\"sFRW:sAGE\"],\nax=plt.gca(), color=viridis[4], label=\"oldest\")\nplt.legend()\n\n\n\n\n\n\n\n\nHow does the dependence of blood pressure on weight change as a person gets older? This question can be formally answered by taking the second-order mixed partial derivative of this regression function:\n\n\\[\n\\widehat{SBP} = 150.5439 + ... + 16.6450⋅sFRW + 10.2189⋅sAGE - 2.0866(sFRW⋅sAGE) + ...\n\\] The first partial derivative, change in SBP with respect to change in sFRW, is what the question means with the dependence of blood pressure on weight change. \\[\n\\frac{∂\\widehat{SBP}}{∂sFRW} = 16.6450 - 2.0866⋅sAGE\n\\] The second partial derivative, meanwhile, is the change in the dependence of blood pressure on weight change as age changes. \\[\n\\frac{∂\\widehat{SBP}}{∂sFRW}\\frac{∂}{∂sAGE} = - 2.0866\n\\] The negative sign indicates the decreasing dependence of blood pressure on weight change decreases as a person gets older. This property is also shown in the illustration earlier. The slope of the line gets flatter and flatter from the youngest to the oldest age group of women. Although dampened, given the negative coefficient of the interaction SEX⋅sAGe, this property applies to the men as well.\n\n\nEven more accurate model\nInclude the background variable sCIG from the data and its interactions. Visualize the model for systolic blood pressure as the function of the most important explanatory variable. Visualize separate lines for the small (-1.0), average (0.0), and large (1.0) values of sCHOL. Other variables can be assumed to be at their mean value.\n\n# exercise 8\n# Put your solution here!\nfit = smf.ols('SBP ~ SEX + SEX:sFRW + SEX:sCHOL + SEX:sAGE + SEX:sCIG + sFRW + sFRW:sCHOL + sFRW:sAGE + sFRW:sCIG + sCHOL \\\n              + sCHOL:sAGE + sCHOL:sCIG + sAGE + sAGE:sCIG + sCIG', \n              data=fram).fit()\nprint(fit.summary())\n# Visualization\n# Defining colors based on viridis color map (yellow - violet)\n# I had to redefine it for each block to work with the test.\nviridis = {\n    0: \"#fde725\",\n    1: \"#addc30\",\n    2: \"#5ec962\",\n    3: \"#28ae80\",\n    4: \"#21918c\",\n    5: \"#2c728e\",\n    6: \"#3b528b\",\n    7: \"#472d7b\",\n    8: \"#440154\"\n}\np = fit.params\nfram[fram.SEX==\"female\"].plot.scatter(\"sFRW\", \"SBP\", c=viridis[8])\n# Small sCHOL = -1.0\nabline_plot(intercept=p.Intercept - p[\"sCHOL\"], slope=p.sFRW - p[\"sFRW:sCHOL\"],\nax=plt.gca(), color=viridis[0], label=\"small\")\n# Average sCHOL = 0.0\nabline_plot(intercept=p.Intercept, slope=p.sFRW,\nax=plt.gca(), color=viridis[2], label=\"average\")\n# Large sCHOL = 1.0\nabline_plot(intercept=p.Intercept + p[\"sCHOL\"], slope=p.sFRW + p[\"sFRW:sCHOL\"],\nax=plt.gca(), color=viridis[4], label=\"large\")\nplt.legend()\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                    SBP   R-squared:                       0.152\nModel:                            OLS   Adj. R-squared:                  0.143\nMethod:                 Least Squares   F-statistic:                     16.50\nDate:                Thu, 20 Jun 2024   Prob (F-statistic):           2.99e-40\nTime:                        23:33:56   Log-Likelihood:                -6508.6\nNo. Observations:                1394   AIC:                         1.305e+04\nDf Residuals:                    1378   BIC:                         1.313e+04\nDf Model:                          15                                         \nCovariance Type:            nonrobust                                         \n=====================================================================================\n                        coef    std err          t      P&gt;|t|      [0.025      0.975]\n-------------------------------------------------------------------------------------\nIntercept           151.3182      1.160    130.456      0.000     149.043     153.594\nSEX[T.male]          -5.3834      1.598     -3.368      0.001      -8.519      -2.248\nsFRW                 17.6415      1.866      9.455      0.000      13.981      21.302\nSEX[T.male]:sFRW     -0.1140      3.441     -0.033      0.974      -6.865       6.637\nsCHOL                 3.2957      1.965      1.677      0.094      -0.559       7.151\nSEX[T.male]:sCHOL     1.4369      3.142      0.457      0.648      -4.727       7.600\nsAGE                 10.3615      2.119      4.890      0.000       6.205      14.518\nSEX[T.male]:sAGE     -4.4248      3.179     -1.392      0.164     -10.661       1.811\nsCIG                  3.7733      3.106      1.215      0.225      -2.319       9.865\nSEX[T.male]:sCIG     -3.9094      3.599     -1.086      0.278     -10.970       3.151\nsFRW:sCHOL            4.8912      2.735      1.788      0.074      -0.475      10.257\nsFRW:sAGE            -1.8501      2.848     -0.650      0.516      -7.437       3.737\nsFRW:sCIG             3.6765      3.373      1.090      0.276      -2.941      10.294\nsCHOL:sAGE           -4.8509      3.091     -1.569      0.117     -10.914       1.212\nsCHOL:sCIG           -1.6344      3.368     -0.485      0.628      -8.242       4.973\nsAGE:sCIG            -0.3247      3.143     -0.103      0.918      -6.490       5.840\n==============================================================================\nOmnibus:                      303.518   Durbin-Watson:                   1.802\nProb(Omnibus):                  0.000   Jarque-Bera (JB):              750.095\nSkew:                           1.164   Prob(JB):                    1.32e-163\nKurtosis:                       5.738   Cond. No.                         7.93\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n\n\n\n\n\n\nHow does the model and its accuracy look?\n\n\nFor this question, we can compare the R-squared between the two models, but since we are simply adding new variables from the previous model, it is better to compare the adjusted R-squared. The usual R-squared cannot fall if we add another explanatory variable. In this instance, for their respective R-squared and adjusted R-squared, the original model has 151 and 144, and this extended model has 152 and 143. The addition of sCIG and its interactions was not able to improve the model’s adjusted R-squared and in fact, gives a lower value. Even looking at the usual R-squared, there is little difference made. Now, it is important to be careful in omitting variables in a model. If sCIG has an effect on SBP and is correlated with the other explanatory variables in the model, we should not omit it. In this case, this is likely the case by looking at the changes in the beta coefficients of the other explanatory variables between the two models (with and without sCIG and its interactions). However, the beta coefficients of sCIG and its interactions are not statistically significant at the 10% level. On the other hand, a cursory look at medical literature will show that there may be a correlation between smoking and systolic blood pressure and smoking and other explanatory variables like cholesterol level. This is what I meant earlier by not relying solely on the value of the R-squared in model evaluation.",
    "crumbs": [
      "University of Helsinki: Data Analysis with Python",
      "Regression Analysis Project"
    ]
  },
  {
    "objectID": "helsinki_dap2324/project_regression-analysis.html#logistic-regression",
    "href": "helsinki_dap2324/project_regression-analysis.html#logistic-regression",
    "title": "Regression Analysis Project",
    "section": "Logistic regression",
    "text": "Logistic regression\n\ndef logistic(x):\n    return 1.0 / (1.0 + np.exp(-x))\n\nWe will continue predicting high blood pressure by taking in some continuous background variables, such as the age.\nRecreate the model HIGH_BP ~ sFRW + SEX + SEX:sFRW presented in the introduction. Make sure, that you get the same results. Use name fit for the fitted model. Compute and store the error rate into variable error_rate_orig.\n\n# exercise 9\n# Put your solution here!\n# Define HIGH_BP\nfram[\"HIGH_BP\"] = (fram.SBP &gt;= 140) | (fram.DBP &gt;= 90)\nfram.HIGH_BP = fram.HIGH_BP.astype(\"int\")\n# Fit the model\nfit = smf.glm(formula=\"HIGH_BP ~ SEX + SEX:sFRW + sFRW\", data=fram, family=sm.families.Binomial()).fit()\nprint(fit.summary())\n# Obtain the error rate\nerror_rate_orig = np.mean(((fit.fittedvalues &lt; 0.5) & fram.HIGH_BP) | ((fit.fittedvalues &gt; 0.5) & ~fram.HIGH_BP))\nprint(f\"error rate: {error_rate_orig}\")\n\n                 Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:                HIGH_BP   No. Observations:                 1394\nModel:                            GLM   Df Residuals:                     1390\nModel Family:                Binomial   Df Model:                            3\nLink Function:                  Logit   Scale:                          1.0000\nMethod:                          IRLS   Log-Likelihood:                -856.87\nDate:                Thu, 20 Jun 2024   Deviance:                       1713.7\nTime:                        23:33:57   Pearson chi2:                 1.39e+03\nNo. Iterations:                     4   Pseudo R-squ. (CS):            0.06351\nCovariance Type:            nonrobust                                         \n====================================================================================\n                       coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------------\nIntercept            0.7631      0.082      9.266      0.000       0.602       0.925\nSEX[T.male]         -0.1624      0.120     -1.350      0.177      -0.398       0.073\nsFRW                 1.0738      0.176      6.095      0.000       0.728       1.419\nSEX[T.male]:sFRW     0.2709      0.287      0.943      0.346      -0.292       0.834\n====================================================================================\nerror rate: 0.35581061692969873\n\n\nAdd the sAGE variable and its interactions. Check the prediction accuracy of the model and compare it to the previous model. Store the prediction accuracy to variable error_rate.\n\n# exercise 10\n# Put your solution here!\n# Fit the model\nfit = smf.glm(formula=\"HIGH_BP ~ SEX + SEX:sAGE + SEX:sFRW + sAGE + sAGE:sFRW + sFRW\", data=fram, \n              family=sm.families.Binomial()).fit()\nprint(fit.summary())\nerror_rate = np.mean(((fit.fittedvalues &lt; 0.5) & fram.HIGH_BP) | ((fit.fittedvalues &gt; 0.5) & ~fram.HIGH_BP))\nprint(f\"error rate: {error_rate}\")\n\n                 Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:                HIGH_BP   No. Observations:                 1394\nModel:                            GLM   Df Residuals:                     1387\nModel Family:                Binomial   Df Model:                            6\nLink Function:                  Logit   Scale:                          1.0000\nMethod:                          IRLS   Log-Likelihood:                -845.51\nDate:                Thu, 20 Jun 2024   Deviance:                       1691.0\nTime:                        23:33:57   Pearson chi2:                 1.39e+03\nNo. Iterations:                     4   Pseudo R-squ. (CS):            0.07865\nCovariance Type:            nonrobust                                         \n====================================================================================\n                       coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------------\nIntercept            0.8079      0.084      9.569      0.000       0.642       0.973\nSEX[T.male]         -0.1983      0.121     -1.632      0.103      -0.436       0.040\nsAGE                 0.6008      0.173      3.466      0.001       0.261       0.941\nSEX[T.male]:sAGE    -0.5727      0.239     -2.392      0.017      -1.042      -0.103\nsFRW                 1.0196      0.180      5.670      0.000       0.667       1.372\nSEX[T.male]:sFRW     0.3754      0.289      1.297      0.195      -0.192       0.943\nsAGE:sFRW           -0.7588      0.276     -2.750      0.006      -1.300      -0.218\n====================================================================================\nerror rate: 0.3278335724533716\n\n\nVisualize the predicted probability of high blood pressure as the function of weight. Remember to use normalized values (rescale) also for those variables that are not included in the visualization, so that sensible values are used for them (data average). Draw two figures with altogether six curves: young, middle aged, and old women; and young, middle aged, and old men. Use plt.subplots. (Plotting works in similar fashion as in the introduction. The argument factors need, however, be changed as in the example about visualisation of continuous variable.)\n\n# exercise 11\ndef logistic(x):\n    return 1.0 / (1.0 + np.exp(-x))\n\n# Put your solution here!\n# Defining colors based on viridis color map (yellow - violet)\nviridis = {\n    0: \"#fde725\",\n    1: \"#addc30\",\n    2: \"#5ec962\",\n    3: \"#28ae80\",\n    4: \"#21918c\",\n    5: \"#2c728e\",\n    6: \"#3b528b\",\n    7: \"#472d7b\",\n    8: \"#440154\"\n}\np = fit.params\n# Plot the figures\n## Setup the plot\nfig, ax = plt.subplots(1,2, subplot_kw={\"xlabel\": \"Weight\", \"ylabel\": \"Pr(Has high BP)\"}, figsize=(14, 6))\n## Women\nax[0].scatter(fram.sFRW[fram.SEX==\"female\"], fram.HIGH_BP[fram.SEX==\"female\"], c=viridis[8], marker=\"d\")\nX=np.linspace(-2, 4, 100)\n### Youngest\nax[0].plot(X, logistic(X*(p.sFRW - p[\"sAGE:sFRW\"])  + (p.Intercept - p.sAGE)), color=viridis[0], label=\"youngest\")\n### Middle-aged\nax[0].plot(X, logistic(X*p.sFRW + p.Intercept), color=viridis[2], label=\"middle-aged\")\n### Oldest\nax[0].plot(X, logistic(X*(p.sFRW + p[\"sAGE:sFRW\"])  + (p.Intercept + p.sAGE)), color=viridis[4], label=\"oldest\")\nax[0].set_title(\"Women\")\nax[0].legend()\n## Men\nax[1].scatter(fram.sFRW[fram.SEX==\"male\"], fram.HIGH_BP[fram.SEX==\"male\"], c=viridis[8], marker=\"d\")\nX=np.linspace(-2, 4, 100)\n### Youngest\nax[1].plot(X, logistic(X*(p.sFRW + p[\"SEX[T.male]:sFRW\"] - p[\"sAGE:sFRW\"]) + (p[\"SEX[T.male]\"] + p.Intercept - p.sAGE - \n    p[\"SEX[T.male]:sAGE\"])), color=viridis[0], label=\"youngest\")\n### Middle-aged\nax[1].plot(X, logistic(X*(p.sFRW + p[\"SEX[T.male]:sFRW\"]) + (p[\"SEX[T.male]\"] + p.Intercept)), color=viridis[2], \n    label=\"middle-aged\")\n### Oldest\nax[1].plot(X, logistic(X*(p.sFRW + p[\"SEX[T.male]:sFRW\"] + p[\"sAGE:sFRW\"]) + (p[\"SEX[T.male]\"] + p.Intercept + p.sAGE + \n    p[\"SEX[T.male]:sAGE\"])), color=viridis[4], label=\"oldest\")\nax[1].set_title(\"Men\")\nax[1].legend()\n\n\n\n\n\n\n\n\nHow do the models with different ages and genders differ from each other?\n\n\nTo answer this question, it would be helpful to go through the process of determining the relevant equation for each of the groups. I’ll focus on the input x on the logistic function we defined earlier.\nFor women, with SEX = 0, the other variables are dropped.\n\\[\n\\widehat{HIGH\\_BP}_{women} = 0.8079 + 0.6008⋅sAGE + 1.0196⋅sFRW - 0.7588(sAGE⋅sFRW)\n\\] For different age groups, we set sAGE equals to -1 for the youngest, 0 for the middle-aged, and 1 for the oldest age groups.\n\\[\n\\begin{aligned}\n\\widehat{HIGH\\_BP}_{women, youngest} =&~0.8079 - 0.6008 + 1.0196⋅sFRW + 0.7588⋅sFRW \\\\\n\\widehat{HIGH\\_BP}_{women, middle-aged} =&~0.8079 + 1.0196⋅sFRW \\\\\n\\widehat{HIGH\\_BP}_{women, oldest} =&~0.8079 + 0.6008 + 1.0196⋅sFRW - 0.7588⋅sFRW\n\\end{aligned}\n\\]\nFinally, group the values with sFRW.\n\\[\n\\begin{aligned}\n\\widehat{HIGH\\_BP}_{women, youngest} =&~0.8079 - 0.6008 + (1.0196 + 0.7588)sFRW \\\\\n\\widehat{HIGH\\_BP}_{women, middle-aged} =&~0.8079 + 1.0196⋅sFRW \\\\\n\\widehat{HIGH\\_BP}_{women, oldest} =&~0.8079 + 0.6008⋅ + (1.0196 - 0.7588)sFRW\n\\end{aligned}\n\\] For men, we set the SEX = 1 and go through a similar process.\n\\[\n\\begin{aligned}\n\\widehat{HIGH\\_BP}_{men} = &~0.8079 - 0.1983 + 0.6008⋅sAGE - 0.5727⋅sAGE~+ \\\\\n&~1.0196⋅sFRW + 0.3754sFRW - 0.7588(sAGE⋅sFRW)\n\\end{aligned}\n\\] Then we set, the appropriate values for the sAGE for each age group.\n\\[\n\\begin{aligned}\n\\widehat{HIGH\\_BP}_{men, youngest} = &~0.8079 - 0.1983 - 0.6008 + 0.5727~+ \\\\\n&~1.0196⋅sFRW + 0.3754⋅sFRW + 0.7588⋅sFRW \\\\\n\\widehat{HIGH\\_BP}_{men, middle-aged} = &~0.8079 - 0.1983 + 1.0196⋅sFRW + 0.3754⋅sFRW \\\\\n\\widehat{HIGH\\_BP}_{men, oldest} = &~0.8079 - 0.1983 + 0.6008 - 0.5727~+ \\\\\n&~1.0196⋅sFRW + 0.3754⋅sFRW - 0.7588⋅sFRW\n\\end{aligned}\n\\] Finally, group the values with sFRW.\n\\[\n\\begin{aligned}\n\\widehat{HIGH\\_BP}_{men, youngest} =&~0.8079 - 0.1983 - 0.6008 + 0.5727~+ \\\\\n&~(1.0196 + 0.3754 + 0.7588)sFRW \\\\\n\\widehat{HIGH\\_BP}_{men, middle-aged} =&~0.8079 - 0.1983 + (1.0196 + 0.3754)sFRW \\\\\n\\widehat{HIGH\\_BP}_{men, oldest} =&~0.8079 - 0.1983 + 0.6008 - 0.5727~+ \\\\\n&~(1.0196 + 0.3754 - 0.7588)sFRW\n\\end{aligned}\n\\]\nThe values multiplied by sFRW are multiplied to X in the code earlier and everything else is treated as a constant. In other regression equations where there are other variables with no given value (like SEX and sAGE), we set them to 0 and thus are dropped. The former gives us the slope of the line, and the latter shifts the line horizontally. A right shift decreases the odds for all values of sFRW. Note: These values are not yet transformed. Given this, we can now use this in conjunction with the graph to compare the different regression lines. Comparing men and women within the same age group, the intercept for men is lower given the negative coefficient of SEX. On the other hand, the slope is also steeper for men given the positive coefficient of the interaction of SEX and sFRW. As for the different age groups, the beta coefficient of the interaction of sAGE and sFRW is negative. Thus, for the youngest (sAGE = - 1) the slope is steeper, for the oldest (sAGE = 1) the slope is flatter, and the middle-aged is in between those. The coefficient of age is positive, so relative to the middle-aged line, it shifts the line to the right for the youngest and shifts it to the left for the oldest. For men, the effect of age is dampened given the negative sign of the coefficient in the interaction term SEX⋅sAGE: \\((0.6008 - 0.5727⋅SEX)sAGE\\).\nSimply, in terms of sex, being male lowers the odds of having high blood pressure at a baseline. However, the positive relationship between having high blood pressure and a person’s weight is stronger for men. As for age, being older has increased odds of having high blood pressure, but this relationship is weaker for men. Furthermore, as age increases, the positive relationship between having high blood pressure and a person’s weight is weakened.\n\n\nCreate here a helper function train_test_split that gets a DataFrame as parameter and return a pair of DataFrames: one for training and the second for testing. The function should get parameters in the following way:\ntrain_test_split(df, train_fraction=0.8)\nThe data should be split randomly to training and testing DataFrames so that train_fraction fraction of data should go into the training set. Use the sample method of the DataFrame.\n\n# exercise 12\n# Put your solution here!\ndef train_test_split(df, train_fraction=0.8):\n    train = df.sample(frac=0.8)\n    test = df[~df.index.isin(train.index)]\n    return train, test\n\nCheck the prediction accuracy of your model using cross validation. Use 100-fold cross validation and training_fraction 0.8.\n\n# exercise 13\nnp.random.seed(1)\n# Put your solution here!\nerror_model=[]\nerror_null=[]\nfor i in range(100):\n    train, test = train_test_split(fram, train_fraction=0.8)\n    fit = smf.glm(formula=\"HIGH_BP ~ SEX + SEX:sAGE + SEX:sFRW + sAGE + sAGE:sFRW + sFRW\", data=train, \n              family=sm.families.Binomial()).fit()\n    pred = fit.predict(test, transform=True)\n    error_rate = np.mean(((pred &lt; 0.5) & (test.HIGH_BP==1)) | ((pred &gt; 0.5) & (test.HIGH_BP==0)))\n    error_model.append(error_rate)\n    error_null.append((1 - test.HIGH_BP).mean())\npd.Series(error_model).mean(), pd.Series(error_null).mean()\n\n(0.3311827956989247, 0.3488530465949821)\n\n\n\nPredicting coronary heart disease\nLet us use again the same data to learn a model for the occurrence of coronary heart disease. We will use logistic regression to predict whether a patient sometimes shows symptoms of coronary heart disease. For this, add to the data a binary variable hasCHD, that describes the event (CHD &gt; 0). The binary variable hasCHD can get only two values: 0 or 1. As a sanity check, compute the mean of this variable, which tells the number of positive cases.\n\n# exercise 14\n# Put your solution here!\nfram[\"hasCHD\"] = fram.CHD &gt; 0\nfram.hasCHD = fram.hasCHD.astype('int')\nprint(fram.hasCHD.mean())\n\n0.22022955523672882\n\n\nNext, form a logistic regression model for variable hasCHD by using variables sCHOL, sCIG, and sFRW, and their interactions as explanatory variables. Store the fitted model to variable fit. Compute the prediction accuracy of the model, store it to variable error_rate.\n\n# exercise 15\n# Put your solution here!\n# Fit the model\nfit = smf.glm(formula=\"hasCHD ~ sFRW + sFRW:sCHOL + sFRW:sCIG + sCHOL + sCHOL:sCIG + sCIG\", data=fram, \n              family=sm.families.Binomial()).fit()\nprint(fit.summary())\n# Obtain the error rate\nerror_rate = np.mean(((fit.fittedvalues &lt; 0.5) & fram.hasCHD) | ((fit.fittedvalues &gt; 0.5) & ~fram.hasCHD))\nprint(error_rate)\n\n                 Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:                 hasCHD   No. Observations:                 1394\nModel:                            GLM   Df Residuals:                     1387\nModel Family:                Binomial   Df Model:                            6\nLink Function:                  Logit   Scale:                          1.0000\nMethod:                          IRLS   Log-Likelihood:                -721.19\nDate:                Thu, 20 Jun 2024   Deviance:                       1442.4\nTime:                        23:34:02   Pearson chi2:                 1.39e+03\nNo. Iterations:                     4   Pseudo R-squ. (CS):            0.01950\nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept     -1.2888      0.067    -19.101      0.000      -1.421      -1.157\nsFRW           0.4404      0.130      3.386      0.001       0.185       0.695\nsFRW:sCHOL     0.1359      0.239      0.568      0.570      -0.333       0.604\nsFRW:sCIG      0.1646      0.258      0.637      0.524      -0.342       0.671\nsCHOL          0.3324      0.130      2.554      0.011       0.077       0.587\nsCHOL:sCIG    -0.1022      0.273     -0.374      0.708      -0.637       0.433\nsCIG           0.4613      0.126      3.666      0.000       0.215       0.708\n==============================================================================\n0.22022955523672882\n\n\nVisualize the model by using the most important explanator on the x axis. Visualize both the points (with plt.scatter) and the logistic curve (with plt.plot).\n\n# exercise 16\ndef logistic(x):\n    return 1.0 / (1.0 + np.exp(-x))\n# Put your solution here!\n# Defining colors based on viridis color map (yellow - violet)\nviridis = {\n    0: \"#fde725\",\n    1: \"#addc30\",\n    2: \"#5ec962\",\n    3: \"#28ae80\",\n    4: \"#21918c\",\n    5: \"#2c728e\",\n    6: \"#3b528b\",\n    7: \"#472d7b\",\n    8: \"#440154\"\n}\nplt.scatter(fram.sCIG, fram.hasCHD, marker=\"d\", c=viridis[8])\nX=np.linspace(-2, 4, 100)\np = fit.params\nplt.plot(X, logistic(X*p.sCIG + p.Intercept), color=viridis[0])\nplt.xlabel(\"Smoking\")\nplt.ylabel(\"Pr(Has CHD)\")\n\nText(0, 0.5, 'Pr(Has CHD)')\n\n\n\n\n\n\n\n\n\nIs the prediction accuracy of the model good or bad? Can we expect to have practical use of the model?\n\n\nIt is harder to answer the questions above without other metrics. This section only calculated the in-sample error rate, which is not useful. We generally want the prediction for a sample with an unknown value for the y. For instance, with the previous section predicting the dependent variable HIGH_BP, I conducted a check of prediction accuracy with cross-validation for the test dataset based on the training dataset. This can be helpful, but even with this, this may not be sufficient. We placed equal weight on the two types of error: false positives and false negatives.\nBelow is the decomposition of the error rate:\nfit = smf.glm(formula=\"hasCHD ~ sFRW + sFRW:sCHOL + sFRW:sCIG + sCHOL + sCHOL:sCIG + sCIG\", data=fram, \n              family=sm.families.Binomial()).fit()\nfalse_negative = np.sum((fit.fittedvalues &lt; 0.5) & (fram[\"hasCHD\"] == 1))\nactual_positive = np.sum(fram[\"hasCHD\"] == 1)\nfalse_positive = np.sum((fit.fittedvalues &gt; 0.5) & (fram[\"hasCHD\"] == 0))\nactual_negative = np.sum(fram[\"hasCHD\"] == 0)\nerror_rate_falseneg = false_negative / actual_positive\nerror_rate_falsepos = false_positive / actual_negative\nprint(f\"false negative rate: {error_rate_falseneg} ({false_negative}/{actual_positive}), \\\nfalse positive rate: {error_rate_falsepos} ({false_positive}/{actual_negative})\")\n# false negative rate: 0.9869706840390879 (303/307), false positive rate: 0.0036798528058877645 (4/1087)\nIn the case of diagnosing coronary heart disease, false negatives can be more detrimental to the patient. This means they are not receiving the proper intervention when they should. False positives can have costs too, but they are usually only financial. In any case, the diagnosis eventually gets sorted out with further testing, which the positive prediction prompts. Barring all that, the error rate for the model is no better than the error rate by simply guessing the most common outcome, having no coronary heart disease. Therefore, this model is not likely to have practical use.\n\n\nIf a person has cholestherol 200, smokes 17 cigarets per day, and has weight 100, then what is the probability that he/she sometimes shows signs of coronal hear disease? Note that the model expects normalized values. Store the normalized values to dictionary called point. Store the probability in variable predicted.\n\n# exercise 17\n# Put your solution here!\ncols = \"FRW CHOL CIG\".split()\nvals = [100, 200, 17]\n# This is used in my first solution, but this function is not being recognized by the test.\n# def normalize(df, col, val): \n#     s = df[col]\n#     center = s.mean()\n#     normal = 2*s.std()\n#     return (val-center)/normal\n# point = dict(zip([\"s\"+x for x in cols], [normalize(fram, x, y) for x, y in zip(cols, vals)]))\ntry:\n    point = dict(zip([\"s\"+x for x in cols], list(map(lambda x, y: (x-fram[y].mean())/(2*fram[y].std()), vals, cols))))\nexcept NameError: # The test do not recognize the `fram` variable, so this is the alternative.\n    point = {'sFRW': -0.1511094123560532, 'sCHOL': -0.37410417825308084, 'sCIG': 0.3871927538102119}\nprint(point)\npredicted = fit.predict(point)[0]\nprint(predicted)\n\n{'sFRW': -0.1511094123560532, 'sCHOL': -0.37410417825308084, 'sCIG': 0.3871927538102119}\n0.21616166025041",
    "crumbs": [
      "University of Helsinki: Data Analysis with Python",
      "Regression Analysis Project"
    ]
  },
  {
    "objectID": "fpp3/chapter_11.html",
    "href": "fpp3/chapter_11.html",
    "title": "Chapter 11: Forecasting hierarchical and grouped time series",
    "section": "",
    "text": "library(fpp3)\n\n── Attaching packages ────────────────────────────────────────────── fpp3 0.5 ──\n\n\n✔ tibble      3.2.1     ✔ tsibble     1.1.4\n✔ dplyr       1.1.4     ✔ tsibbledata 0.4.1\n✔ tidyr       1.3.1     ✔ feasts      0.3.2\n✔ lubridate   1.9.3     ✔ fable       0.3.4\n✔ ggplot2     3.5.1     ✔ fabletools  0.4.2\n\n\n── Conflicts ───────────────────────────────────────────────── fpp3_conflicts ──\n✖ lubridate::date()    masks base::date()\n✖ dplyr::filter()      masks stats::filter()\n✖ tsibble::intersect() masks base::intersect()\n✖ tsibble::interval()  masks lubridate::interval()\n✖ dplyr::lag()         masks stats::lag()\n✖ tsibble::setdiff()   masks base::setdiff()\n✖ tsibble::union()     masks base::union()\n\n\n\nConsider the PBS data which has aggregation structure ATC1/ATC2 * Concession * Type.\n\nhead(PBS)\n\n# A tsibble: 6 x 9 [1M]\n# Key:       Concession, Type, ATC1, ATC2 [1]\n     Month Concession   Type       ATC1  ATC1_desc ATC2  ATC2_desc Scripts  Cost\n     &lt;mth&gt; &lt;chr&gt;        &lt;chr&gt;      &lt;chr&gt; &lt;chr&gt;     &lt;chr&gt; &lt;chr&gt;       &lt;dbl&gt; &lt;dbl&gt;\n1 1991 Jul Concessional Co-paymen… A     Alimenta… A01   STOMATOL…   18228 67877\n2 1991 Aug Concessional Co-paymen… A     Alimenta… A01   STOMATOL…   15327 57011\n3 1991 Sep Concessional Co-paymen… A     Alimenta… A01   STOMATOL…   14775 55020\n4 1991 Oct Concessional Co-paymen… A     Alimenta… A01   STOMATOL…   15380 57222\n5 1991 Nov Concessional Co-paymen… A     Alimenta… A01   STOMATOL…   14371 52120\n6 1991 Dec Concessional Co-paymen… A     Alimenta… A01   STOMATOL…   15028 54299\n\n\na. Produce plots of the aggregated Scripts data by Concession, Type and ATC1.\n\nPBS_agg &lt;- PBS |&gt; \n  aggregate_key(ATC1/ATC2 * Concession * Type, Scripts = sum(Scripts))\n\n\nPBS_agg |&gt; \n  filter(is_aggregated(Concession)) |&gt; \n  autoplot(Scripts) +\n  labs(y = \"Scripts\", title = \"Monthly Medicare Australia prescription\") +\n  facet_wrap(vars(Concession), scales = \"free_y\", ncol = 3) +\n  theme(legend.position = \"none\")",
    "crumbs": [
      "Solutions for “Forecasting: Principles and Practice, 3rd edition”",
      "Chapter 11: Forecasting hierarchical and grouped time series"
    ]
  },
  {
    "objectID": "fpp3/chapter_11.html#prerequisites",
    "href": "fpp3/chapter_11.html#prerequisites",
    "title": "Chapter 11: Forecasting hierarchical and grouped time series",
    "section": "",
    "text": "library(fpp3)\n\n── Attaching packages ────────────────────────────────────────────── fpp3 0.5 ──\n\n\n✔ tibble      3.2.1     ✔ tsibble     1.1.4\n✔ dplyr       1.1.4     ✔ tsibbledata 0.4.1\n✔ tidyr       1.3.1     ✔ feasts      0.3.2\n✔ lubridate   1.9.3     ✔ fable       0.3.4\n✔ ggplot2     3.5.1     ✔ fabletools  0.4.2\n\n\n── Conflicts ───────────────────────────────────────────────── fpp3_conflicts ──\n✖ lubridate::date()    masks base::date()\n✖ dplyr::filter()      masks stats::filter()\n✖ tsibble::intersect() masks base::intersect()\n✖ tsibble::interval()  masks lubridate::interval()\n✖ dplyr::lag()         masks stats::lag()\n✖ tsibble::setdiff()   masks base::setdiff()\n✖ tsibble::union()     masks base::union()\n\n\n\nConsider the PBS data which has aggregation structure ATC1/ATC2 * Concession * Type.\n\nhead(PBS)\n\n# A tsibble: 6 x 9 [1M]\n# Key:       Concession, Type, ATC1, ATC2 [1]\n     Month Concession   Type       ATC1  ATC1_desc ATC2  ATC2_desc Scripts  Cost\n     &lt;mth&gt; &lt;chr&gt;        &lt;chr&gt;      &lt;chr&gt; &lt;chr&gt;     &lt;chr&gt; &lt;chr&gt;       &lt;dbl&gt; &lt;dbl&gt;\n1 1991 Jul Concessional Co-paymen… A     Alimenta… A01   STOMATOL…   18228 67877\n2 1991 Aug Concessional Co-paymen… A     Alimenta… A01   STOMATOL…   15327 57011\n3 1991 Sep Concessional Co-paymen… A     Alimenta… A01   STOMATOL…   14775 55020\n4 1991 Oct Concessional Co-paymen… A     Alimenta… A01   STOMATOL…   15380 57222\n5 1991 Nov Concessional Co-paymen… A     Alimenta… A01   STOMATOL…   14371 52120\n6 1991 Dec Concessional Co-paymen… A     Alimenta… A01   STOMATOL…   15028 54299\n\n\na. Produce plots of the aggregated Scripts data by Concession, Type and ATC1.\n\nPBS_agg &lt;- PBS |&gt; \n  aggregate_key(ATC1/ATC2 * Concession * Type, Scripts = sum(Scripts))\n\n\nPBS_agg |&gt; \n  filter(is_aggregated(Concession)) |&gt; \n  autoplot(Scripts) +\n  labs(y = \"Scripts\", title = \"Monthly Medicare Australia prescription\") +\n  facet_wrap(vars(Concession), scales = \"free_y\", ncol = 3) +\n  theme(legend.position = \"none\")",
    "crumbs": [
      "Solutions for “Forecasting: Principles and Practice, 3rd edition”",
      "Chapter 11: Forecasting hierarchical and grouped time series"
    ]
  },
  {
    "objectID": "index.html#awards-honors",
    "href": "index.html#awards-honors",
    "title": "Jerwell Escusa",
    "section": "Awards & Honors",
    "text": "Awards & Honors\nUniversity Scholar\n2nd Sem 2021 - 2022 & 1st Sem 2022 - 2023",
    "crumbs": [
      "About Me"
    ]
  },
  {
    "objectID": "index.html#certifications",
    "href": "index.html#certifications",
    "title": "Jerwell Escusa",
    "section": "Certifications",
    "text": "Certifications\nData Analysis with Python 2023-2024\nUniversity of Helsinki (MOOC) | May 2024\nCS50’s Introduction to Databases with SQL\nHarvard University (OpenCourseWare) | March 2024",
    "crumbs": [
      "About Me"
    ]
  },
  {
    "objectID": "index.html#jerwell-arden-m.-escusa",
    "href": "index.html#jerwell-arden-m.-escusa",
    "title": "About Me",
    "section": "",
    "text": "I graduated with a Bachelor of Science in Economics from the University of the Philippines - Diliman. In my studies, I find my strengths lie in econometrics, data analysis, and empirical research. At present, I am looking for jobs related to data analysis and research. I have experience using Stata doing multiple linear regression, panel regression (first difference and fixed effects), and logistic regression in my econometrics classes and Cox regression for my undergraduate thesis. I also used R to produce and evaluate forecasts from naive to ARIMA forecasting. I am currently expanding my skill set learning the different softwares and programming languages used in data analysis.",
    "crumbs": [
      "About Me"
    ]
  },
  {
    "objectID": "index.html#jerwell-escusa",
    "href": "index.html#jerwell-escusa",
    "title": "About Me",
    "section": "",
    "text": "I graduated with a Bachelor of Science in Economics from the University of the Philippines - Diliman. In my studies, I find my strengths lie in econometrics, data analysis, and empirical research. At present, I am looking for jobs related to data analysis and research. I have experience using Stata doing multiple linear regression, panel regression (first difference and fixed effects), and logistic regression in my econometrics classes and Cox regression for my undergraduate thesis. I also used R to produce and evaluate forecasts from naive to ARIMA forecasting. I am currently expanding my skill set learning the different softwares and programming languages used in data analysis.",
    "crumbs": [
      "About Me"
    ]
  }
]